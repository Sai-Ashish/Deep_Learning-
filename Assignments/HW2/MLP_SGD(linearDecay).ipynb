{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Note:run in python2######\n",
    "# same initialization of weights because same random seed used\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#MLP\n",
    "\n",
    "# initialisation of the weights he_normal\n",
    "def weights(noHiddenLayers,sizeOfLayers):\n",
    "\n",
    "    W=[]\n",
    "    b=[]\n",
    "\n",
    "    for i in range(0,noHiddenLayers+1):\n",
    "        \n",
    "        W.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+sizeOfLayers[i])),\n",
    "                                   (sizeOfLayers[i+1],sizeOfLayers[i])) )\n",
    "        b.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+1)),\n",
    "                                   (sizeOfLayers[i+1],1)) )\n",
    "\n",
    "    W=np.array(W)\n",
    "    b=np.array(b)\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "#mlp forward pass\n",
    "#layer\n",
    "def layer(w,x,b):\n",
    "    out = np.matmul(w,x)+b\n",
    "    return out\n",
    "\n",
    "def apply_activationMLP(Activation_function,inp):\n",
    "    \n",
    "    #activation functions\n",
    "    if Activation_function == 'relu':\n",
    "        return np.where(inp<0,0,inp)\n",
    "    elif Activation_function == \"tanh\":\n",
    "        return np.tanh(inp)\n",
    "    elif Activation_function == \"sigmoid\":\n",
    "        return 1.0/(1+np.exp(-1.0*inp))\n",
    "    elif Activation_function == \"softmax\":\n",
    "        return (1.0/(np.sum(np.exp(inp),axis=0)))*(np.exp(inp))\n",
    "\n",
    "#forward path\n",
    "def forward_path(noHiddenLayers,X,W,b,Actfnvect):\n",
    "\n",
    "    out=[]\n",
    "    \n",
    "    z=apply_activationMLP(Actfnvect[0],np.array(layer(W[0],X,b[0])))\n",
    "    out.append(np.array(z))\n",
    "\n",
    "    for i in range(1,noHiddenLayers):\n",
    "        z=apply_activationMLP(Actfnvect[i],np.array(layer(W[i],out[i-1],b[i])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    if noHiddenLayers > 0:\n",
    "        z=apply_activationMLP(Actfnvect[-1],np.array(layer(W[-1],out[-1],b[-1])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    y_pred = out[-1]\n",
    "\n",
    "    return np.array(out),np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "#only to import data\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    " \n",
    "    \n",
    "################################\n",
    "\n",
    "# #import data\n",
    "iris_data = load_iris() # load the iris dataset\n",
    "\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "# Split the data for training and testing\n",
    "x_train , x_test, y_train , y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(3, 120)\n",
      "(4, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "# run MLP algorithm\n",
    "\n",
    "x_train = np.moveaxis(x_train,0,-1)\n",
    "y_train = np.moveaxis(y_train,0,-1)\n",
    "x_test = np.moveaxis(x_test,0,-1)\n",
    "y_test = np.moveaxis(y_test,0,-1)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_relu(x):\n",
    "    return np.where(x == 0,0,1)\n",
    "\n",
    "# backpropation\n",
    "def backprop(y,y_true,z,T,u,M,x):\n",
    "    \n",
    "    dy = (y-y_true)\n",
    "    \n",
    "    # layer 3\n",
    "    dT  = np.matmul(dy,z.T)\n",
    "    db3 = np.sum(dy,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 2\n",
    "    s   = np.matmul(T.T,dy)\n",
    "    s   = s*der_relu(z)\n",
    "    dM  = np.matmul(s,u.T)\n",
    "    db2 = np.sum(s,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 1\n",
    "    sm  = np.matmul(M.T,s)\n",
    "    sm  = sm*der_relu(u)\n",
    "    dW  = np.matmul(sm,x.T)\n",
    "    db1 = np.sum(sm,axis=1).reshape(-1,1)\n",
    "    \n",
    "    return dW,dM,dT,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Training parameters\n",
    "num_classes = 3\n",
    "epochs = 600\n",
    "learning_rate_t=1e-4\n",
    "learning_rate_0=5e-4\n",
    "batch_size = 10\n",
    "\n",
    "################################\n",
    "#MLP PARAMETERS\n",
    "noHiddenLayers=2\n",
    "\n",
    "#also includes the input vector dimension and output vector dimension\n",
    "sizeOfLayers=[x_train.shape[0],10,10,num_classes]\n",
    "\n",
    "sizeofOutput=num_classes\n",
    "\n",
    "Actfnvect = ['relu','relu','softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = weights(noHiddenLayers,sizeOfLayers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0         Loss:256.0180606197502\n",
      "Epoch:1         Loss:244.5094803739828\n",
      "Epoch:2         Loss:235.3456093907099\n",
      "Epoch:3         Loss:227.79758307801575\n",
      "Epoch:4         Loss:221.49130513743228\n",
      "Epoch:5         Loss:216.25172767069634\n",
      "Epoch:6         Loss:211.8703426177371\n",
      "Epoch:7         Loss:208.20393411689798\n",
      "Epoch:8         Loss:205.1620016660833\n",
      "Epoch:9         Loss:202.65543963815816\n",
      "Epoch:10         Loss:200.61408235065323\n",
      "Epoch:11         Loss:198.99297008988898\n",
      "Epoch:12         Loss:197.7507784905046\n",
      "Epoch:13         Loss:196.6930988957051\n",
      "Epoch:14         Loss:195.6637412462342\n",
      "Epoch:15         Loss:194.65610249957598\n",
      "Epoch:16         Loss:193.66800617494374\n",
      "Epoch:17         Loss:192.70037749079341\n",
      "Epoch:18         Loss:191.75257229565685\n",
      "Epoch:19         Loss:190.8228518363041\n",
      "Epoch:20         Loss:189.91108132588417\n",
      "Epoch:21         Loss:189.01617298987983\n",
      "Epoch:22         Loss:188.1376001877022\n",
      "Epoch:23         Loss:187.27489383002967\n",
      "Epoch:24         Loss:186.42971733755115\n",
      "Epoch:25         Loss:185.60109165574255\n",
      "Epoch:26         Loss:184.79195713363305\n",
      "Epoch:27         Loss:184.00124456330965\n",
      "Epoch:28         Loss:183.22576267809595\n",
      "Epoch:29         Loss:182.46303368475182\n",
      "Epoch:30         Loss:181.7142938533717\n",
      "Epoch:31         Loss:180.97959221358036\n",
      "Epoch:32         Loss:180.2580895871731\n",
      "Epoch:33         Loss:179.54874891713254\n",
      "Epoch:34         Loss:178.85124493975584\n",
      "Epoch:35         Loss:178.16461995204267\n",
      "Epoch:36         Loss:177.48897876133748\n",
      "Epoch:37         Loss:176.82455000901092\n",
      "Epoch:38         Loss:176.17062078825958\n",
      "Epoch:39         Loss:175.52637399088866\n",
      "Epoch:40         Loss:174.89137336653607\n",
      "Epoch:41         Loss:174.2659651515912\n",
      "Epoch:42         Loss:173.6494942215351\n",
      "Epoch:43         Loss:173.0422165657343\n",
      "Epoch:44         Loss:172.44416853927828\n",
      "Epoch:45         Loss:171.85416274086552\n",
      "Epoch:46         Loss:171.2726549318397\n",
      "Epoch:47         Loss:170.6990522461031\n",
      "Epoch:48         Loss:170.1338608467949\n",
      "Epoch:49         Loss:169.5765637040509\n",
      "Epoch:50         Loss:169.02618272689008\n",
      "Epoch:51         Loss:168.48244840880622\n",
      "Epoch:52         Loss:167.94467424722646\n",
      "Epoch:53         Loss:167.4132971566724\n",
      "Epoch:54         Loss:166.8880203985304\n",
      "Epoch:55         Loss:166.3680049669969\n",
      "Epoch:56         Loss:165.8535890733761\n",
      "Epoch:57         Loss:165.34549489098492\n",
      "Epoch:58         Loss:164.84400511623747\n",
      "Epoch:59         Loss:164.34862046599832\n",
      "Epoch:60         Loss:163.85876417373302\n",
      "Epoch:61         Loss:163.37369459925483\n",
      "Epoch:62         Loss:162.89442311044158\n",
      "Epoch:63         Loss:162.4202434231323\n",
      "Epoch:64         Loss:161.95091517913778\n",
      "Epoch:65         Loss:161.48640046924635\n",
      "Epoch:66         Loss:161.027083963942\n",
      "Epoch:67         Loss:160.57306103977172\n",
      "Epoch:68         Loss:160.12357242357538\n",
      "Epoch:69         Loss:159.67854021657132\n",
      "Epoch:70         Loss:159.23709259994428\n",
      "Epoch:71         Loss:158.79916550114154\n",
      "Epoch:72         Loss:158.36485681480085\n",
      "Epoch:73         Loss:157.93464351170437\n",
      "Epoch:74         Loss:157.50838351611952\n",
      "Epoch:75         Loss:157.0869162900493\n",
      "Epoch:76         Loss:156.67069512412323\n",
      "Epoch:77         Loss:156.2570935809541\n",
      "Epoch:78         Loss:155.8461126569777\n",
      "Epoch:79         Loss:155.43921815366969\n",
      "Epoch:80         Loss:155.03510534953148\n",
      "Epoch:81         Loss:154.6331352335147\n",
      "Epoch:82         Loss:154.2342499336662\n",
      "Epoch:83         Loss:153.83743364438814\n",
      "Epoch:84         Loss:153.44611230733315\n",
      "Epoch:85         Loss:153.06179719008284\n",
      "Epoch:86         Loss:152.6785697197834\n",
      "Epoch:87         Loss:152.29988604342552\n",
      "Epoch:88         Loss:151.9242340792956\n",
      "Epoch:89         Loss:151.5521120519055\n",
      "Epoch:90         Loss:151.18414409872557\n",
      "Epoch:91         Loss:150.8205848117586\n",
      "Epoch:92         Loss:150.4614156891123\n",
      "Epoch:93         Loss:150.11372173402935\n",
      "Epoch:94         Loss:149.76163457618173\n",
      "Epoch:95         Loss:149.40622328274526\n",
      "Epoch:96         Loss:149.05393292259762\n",
      "Epoch:97         Loss:148.70554201084218\n",
      "Epoch:98         Loss:148.35381858516155\n",
      "Epoch:99         Loss:147.98988901303315\n",
      "Epoch:100         Loss:147.62681883145834\n",
      "Epoch:101         Loss:147.2653251694695\n",
      "Epoch:102         Loss:146.89005151031304\n",
      "Epoch:103         Loss:146.49619944206475\n",
      "Epoch:104         Loss:146.0843647353526\n",
      "Epoch:105         Loss:145.67517741990739\n",
      "Epoch:106         Loss:145.27884166991353\n",
      "Epoch:107         Loss:144.90048530635454\n",
      "Epoch:108         Loss:144.5413132641372\n",
      "Epoch:109         Loss:144.19227297987317\n",
      "Epoch:110         Loss:143.84899830139403\n",
      "Epoch:111         Loss:143.51050936482707\n",
      "Epoch:112         Loss:143.17785725206062\n",
      "Epoch:113         Loss:142.85526283199795\n",
      "Epoch:114         Loss:142.544472051188\n",
      "Epoch:115         Loss:142.23980868177506\n",
      "Epoch:116         Loss:141.94163780680773\n",
      "Epoch:117         Loss:141.6490704607487\n",
      "Epoch:118         Loss:141.3616102356111\n",
      "Epoch:119         Loss:141.07817227845814\n",
      "Epoch:120         Loss:140.79687497541866\n",
      "Epoch:121         Loss:140.5192665243968\n",
      "Epoch:122         Loss:140.24571623541894\n",
      "Epoch:123         Loss:139.97442364755094\n",
      "Epoch:124         Loss:139.70545645102476\n",
      "Epoch:125         Loss:139.4386643223547\n",
      "Epoch:126         Loss:139.17427805544924\n",
      "Epoch:127         Loss:138.91241737346738\n",
      "Epoch:128         Loss:138.6538622060336\n",
      "Epoch:129         Loss:138.39885529978739\n",
      "Epoch:130         Loss:138.146173163672\n",
      "Epoch:131         Loss:137.8948272309856\n",
      "Epoch:132         Loss:137.64482783959895\n",
      "Epoch:133         Loss:137.39589374696874\n",
      "Epoch:134         Loss:137.14775532973906\n",
      "Epoch:135         Loss:136.90069341046623\n",
      "Epoch:136         Loss:136.65460686568355\n",
      "Epoch:137         Loss:136.40935733072482\n",
      "Epoch:138         Loss:136.1650612471339\n",
      "Epoch:139         Loss:135.921740643542\n",
      "Epoch:140         Loss:135.6791370371731\n",
      "Epoch:141         Loss:135.43739770474258\n",
      "Epoch:142         Loss:135.1966079119475\n",
      "Epoch:143         Loss:134.95675903330456\n",
      "Epoch:144         Loss:134.71829623739214\n",
      "Epoch:145         Loss:134.48074648479295\n",
      "Epoch:146         Loss:134.2438434183184\n",
      "Epoch:147         Loss:134.00770219816297\n",
      "Epoch:148         Loss:133.77210129239248\n",
      "Epoch:149         Loss:133.53731705713247\n",
      "Epoch:150         Loss:133.303175366246\n",
      "Epoch:151         Loss:133.06960557194802\n",
      "Epoch:152         Loss:132.83678283212373\n",
      "Epoch:153         Loss:132.60468973027963\n",
      "Epoch:154         Loss:132.37298291106384\n",
      "Epoch:155         Loss:132.1419352463042\n",
      "Epoch:156         Loss:131.91165708706743\n",
      "Epoch:157         Loss:131.68162747796393\n",
      "Epoch:158         Loss:131.45171870652763\n",
      "Epoch:159         Loss:131.22183472764488\n",
      "Epoch:160         Loss:130.99277873707973\n",
      "Epoch:161         Loss:130.76395803164561\n",
      "Epoch:162         Loss:130.53519911584414\n",
      "Epoch:163         Loss:130.3067125211858\n",
      "Epoch:164         Loss:130.0776897312068\n",
      "Epoch:165         Loss:129.84799489020642\n",
      "Epoch:166         Loss:129.61896580729723\n",
      "Epoch:167         Loss:129.39080200308288\n",
      "Epoch:168         Loss:129.1633772773557\n",
      "Epoch:169         Loss:128.93650879221494\n",
      "Epoch:170         Loss:128.70870864280005\n",
      "Epoch:171         Loss:128.47969117566925\n",
      "Epoch:172         Loss:128.2511345899507\n",
      "Epoch:173         Loss:128.0240084199926\n",
      "Epoch:174         Loss:127.79856991157611\n",
      "Epoch:175         Loss:127.57225069744732\n",
      "Epoch:176         Loss:127.34501366538258\n",
      "Epoch:177         Loss:127.11818538891198\n",
      "Epoch:178         Loss:126.8917245920166\n",
      "Epoch:179         Loss:126.66581447483391\n",
      "Epoch:180         Loss:126.44093749305814\n",
      "Epoch:181         Loss:126.21699113683924\n",
      "Epoch:182         Loss:125.99319392527691\n",
      "Epoch:183         Loss:125.7696124871254\n",
      "Epoch:184         Loss:125.54525147545057\n",
      "Epoch:185         Loss:125.32164954610461\n",
      "Epoch:186         Loss:125.09838868958091\n",
      "Epoch:187         Loss:124.87546849802665\n",
      "Epoch:188         Loss:124.65314180129913\n",
      "Epoch:189         Loss:124.43058944865922\n",
      "Epoch:190         Loss:124.20608904427442\n",
      "Epoch:191         Loss:123.98192145659289\n",
      "Epoch:192         Loss:123.75878656777334\n",
      "Epoch:193         Loss:123.53611384625907\n",
      "Epoch:194         Loss:123.31293225638528\n",
      "Epoch:195         Loss:123.0868678997125\n",
      "Epoch:196         Loss:122.85929625525858\n",
      "Epoch:197         Loss:122.63198593235036\n",
      "Epoch:198         Loss:122.40494045243238\n",
      "Epoch:199         Loss:122.17814993401328\n",
      "Epoch:200         Loss:121.9471414287475\n",
      "Epoch:201         Loss:121.70980355523649\n",
      "Epoch:202         Loss:121.46977322421269\n",
      "Epoch:203         Loss:121.22409584472716\n",
      "Epoch:204         Loss:120.97559411728986\n",
      "Epoch:205         Loss:120.72392108630872\n",
      "Epoch:206         Loss:120.46648738779412\n",
      "Epoch:207         Loss:120.19986636735628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:208         Loss:119.92548333023744\n",
      "Epoch:209         Loss:119.64727968103611\n",
      "Epoch:210         Loss:119.35970825281373\n",
      "Epoch:211         Loss:119.05521933858945\n",
      "Epoch:212         Loss:118.73871581052384\n",
      "Epoch:213         Loss:118.41253854769631\n",
      "Epoch:214         Loss:118.08218801825403\n",
      "Epoch:215         Loss:117.75225510802912\n",
      "Epoch:216         Loss:117.416451461138\n",
      "Epoch:217         Loss:117.0743604207305\n",
      "Epoch:218         Loss:116.7267297297642\n",
      "Epoch:219         Loss:116.3748227980384\n",
      "Epoch:220         Loss:116.02633804033151\n",
      "Epoch:221         Loss:115.6761619281833\n",
      "Epoch:222         Loss:115.323970298766\n",
      "Epoch:223         Loss:114.97244729535929\n",
      "Epoch:224         Loss:114.61906417852069\n",
      "Epoch:225         Loss:114.263408331263\n",
      "Epoch:226         Loss:113.9094706789291\n",
      "Epoch:227         Loss:113.55901630654647\n",
      "Epoch:228         Loss:113.21026110415757\n",
      "Epoch:229         Loss:112.86260986688269\n",
      "Epoch:230         Loss:112.51599366859034\n",
      "Epoch:231         Loss:112.17062978684427\n",
      "Epoch:232         Loss:111.82931305225375\n",
      "Epoch:233         Loss:111.49202668065654\n",
      "Epoch:234         Loss:111.15611021507367\n",
      "Epoch:235         Loss:110.82126488599357\n",
      "Epoch:236         Loss:110.48971283473186\n",
      "Epoch:237         Loss:110.16043564644451\n",
      "Epoch:238         Loss:109.83213646993951\n",
      "Epoch:239         Loss:109.50571103265558\n",
      "Epoch:240         Loss:109.18158592225733\n",
      "Epoch:241         Loss:108.85843036827951\n",
      "Epoch:242         Loss:108.53592545018076\n",
      "Epoch:243         Loss:108.21652929813511\n",
      "Epoch:244         Loss:107.89864173215736\n",
      "Epoch:245         Loss:107.58191373072793\n",
      "Epoch:246         Loss:107.26617620746752\n",
      "Epoch:247         Loss:106.95257268420397\n",
      "Epoch:248         Loss:106.64416537691406\n",
      "Epoch:249         Loss:106.33693084915431\n",
      "Epoch:250         Loss:106.02963611357892\n",
      "Epoch:251         Loss:105.72440269709462\n",
      "Epoch:252         Loss:105.42199581443019\n",
      "Epoch:253         Loss:105.12221510712212\n",
      "Epoch:254         Loss:104.82411045110328\n",
      "Epoch:255         Loss:104.52783865235693\n",
      "Epoch:256         Loss:104.2352632044671\n",
      "Epoch:257         Loss:103.94658314238649\n",
      "Epoch:258         Loss:103.6587625017071\n",
      "Epoch:259         Loss:103.37451280878425\n",
      "Epoch:260         Loss:103.10146473754122\n",
      "Epoch:261         Loss:102.83386900459672\n",
      "Epoch:262         Loss:102.57019160623095\n",
      "Epoch:263         Loss:102.30744007852422\n",
      "Epoch:264         Loss:102.05037501073922\n",
      "Epoch:265         Loss:101.79719570570042\n",
      "Epoch:266         Loss:101.54768883084446\n",
      "Epoch:267         Loss:101.30430477007168\n",
      "Epoch:268         Loss:101.06748583872391\n",
      "Epoch:269         Loss:100.83558201822959\n",
      "Epoch:270         Loss:100.60782479219313\n",
      "Epoch:271         Loss:100.38268894503975\n",
      "Epoch:272         Loss:100.16131389587082\n",
      "Epoch:273         Loss:99.94583942462305\n",
      "Epoch:274         Loss:99.73279216693712\n",
      "Epoch:275         Loss:99.52106845989029\n",
      "Epoch:276         Loss:99.31065926496659\n",
      "Epoch:277         Loss:99.1008768583922\n",
      "Epoch:278         Loss:98.89635666367633\n",
      "Epoch:279         Loss:98.69733395037721\n",
      "Epoch:280         Loss:98.49896868880602\n",
      "Epoch:281         Loss:98.30198411070168\n",
      "Epoch:282         Loss:98.10629136423834\n",
      "Epoch:283         Loss:97.91187088412946\n",
      "Epoch:284         Loss:97.71870847896663\n",
      "Epoch:285         Loss:97.52805660471162\n",
      "Epoch:286         Loss:97.3392521834701\n",
      "Epoch:287         Loss:97.15200000296687\n",
      "Epoch:288         Loss:96.96649577338164\n",
      "Epoch:289         Loss:96.78226260264051\n",
      "Epoch:290         Loss:96.5992768098128\n",
      "Epoch:291         Loss:96.41710098716604\n",
      "Epoch:292         Loss:96.2358486204301\n",
      "Epoch:293         Loss:96.05584124817834\n",
      "Epoch:294         Loss:95.8770406939218\n",
      "Epoch:295         Loss:95.69962770263186\n",
      "Epoch:296         Loss:95.52448305182749\n",
      "Epoch:297         Loss:95.35074607351615\n",
      "Epoch:298         Loss:95.17780514803405\n",
      "Epoch:299         Loss:95.00602370454816\n",
      "Epoch:300         Loss:94.83501529821889\n",
      "Epoch:301         Loss:94.66462172772958\n",
      "Epoch:302         Loss:94.4940658018899\n",
      "Epoch:303         Loss:94.32475828660218\n",
      "Epoch:304         Loss:94.15649109862933\n",
      "Epoch:305         Loss:93.98896315136399\n",
      "Epoch:306         Loss:93.82319117596703\n",
      "Epoch:307         Loss:93.65782237278506\n",
      "Epoch:308         Loss:93.49383315530046\n",
      "Epoch:309         Loss:93.330889212482\n",
      "Epoch:310         Loss:93.16897159423597\n",
      "Epoch:311         Loss:93.00806182066009\n",
      "Epoch:312         Loss:92.84814503290563\n",
      "Epoch:313         Loss:92.68924468349199\n",
      "Epoch:314         Loss:92.5310465805511\n",
      "Epoch:315         Loss:92.37372315958723\n",
      "Epoch:316         Loss:92.21692631888465\n",
      "Epoch:317         Loss:92.06113896518247\n",
      "Epoch:318         Loss:91.90631645540194\n",
      "Epoch:319         Loss:91.75241215995965\n",
      "Epoch:320         Loss:91.59966188651528\n",
      "Epoch:321         Loss:91.449012254515\n",
      "Epoch:322         Loss:91.2991063377136\n",
      "Epoch:323         Loss:91.14943698028331\n",
      "Epoch:324         Loss:91.00078155610076\n",
      "Epoch:325         Loss:90.8521611622117\n",
      "Epoch:326         Loss:90.70434394316032\n",
      "Epoch:327         Loss:90.55692517105149\n",
      "Epoch:328         Loss:90.41078445600954\n",
      "Epoch:329         Loss:90.26568396281823\n",
      "Epoch:330         Loss:90.12100647397882\n",
      "Epoch:331         Loss:89.97724036755552\n",
      "Epoch:332         Loss:89.83410696671469\n",
      "Epoch:333         Loss:89.69180612723966\n",
      "Epoch:334         Loss:89.55039609981841\n",
      "Epoch:335         Loss:89.40869868077364\n",
      "Epoch:336         Loss:89.26768996053674\n",
      "Epoch:337         Loss:89.12762428042312\n",
      "Epoch:338         Loss:88.98825735287303\n",
      "Epoch:339         Loss:88.85015414845789\n",
      "Epoch:340         Loss:88.71393469568513\n",
      "Epoch:341         Loss:88.57936247344566\n",
      "Epoch:342         Loss:88.44577499481066\n",
      "Epoch:343         Loss:88.31314463139472\n",
      "Epoch:344         Loss:88.18073538093923\n",
      "Epoch:345         Loss:88.0487333302018\n",
      "Epoch:346         Loss:87.91885336868816\n",
      "Epoch:347         Loss:87.7892185742346\n",
      "Epoch:348         Loss:87.66042762894969\n",
      "Epoch:349         Loss:87.53097313588897\n",
      "Epoch:350         Loss:87.40265250557505\n",
      "Epoch:351         Loss:87.27554343966874\n",
      "Epoch:352         Loss:87.14874415034795\n",
      "Epoch:353         Loss:87.02277063123205\n",
      "Epoch:354         Loss:86.89735170928691\n",
      "Epoch:355         Loss:86.77248825970558\n",
      "Epoch:356         Loss:86.64846179042786\n",
      "Epoch:357         Loss:86.52598842247015\n",
      "Epoch:358         Loss:86.40430148029817\n",
      "Epoch:359         Loss:86.28364999435863\n",
      "Epoch:360         Loss:86.16412480477459\n",
      "Epoch:361         Loss:86.04585404153308\n",
      "Epoch:362         Loss:85.92864160466529\n",
      "Epoch:363         Loss:85.81201034567569\n",
      "Epoch:364         Loss:85.69639123242774\n",
      "Epoch:365         Loss:85.58258283973063\n",
      "Epoch:366         Loss:85.46980419517388\n",
      "Epoch:367         Loss:85.35773946970008\n",
      "Epoch:368         Loss:85.24636584207242\n",
      "Epoch:369         Loss:85.13611501063858\n",
      "Epoch:370         Loss:85.02627664137184\n",
      "Epoch:371         Loss:84.91708168428522\n",
      "Epoch:372         Loss:84.80850966660736\n",
      "Epoch:373         Loss:84.70054106891214\n",
      "Epoch:374         Loss:84.59315727782514\n",
      "Epoch:375         Loss:84.48634054109428\n",
      "Epoch:376         Loss:84.37998819989824\n",
      "Epoch:377         Loss:84.27439586161186\n",
      "Epoch:378         Loss:84.16895225496401\n",
      "Epoch:379         Loss:84.06420131238545\n",
      "Epoch:380         Loss:83.95986235396461\n",
      "Epoch:381         Loss:83.85593679491\n",
      "Epoch:382         Loss:83.7527939789884\n",
      "Epoch:383         Loss:83.65025896738499\n",
      "Epoch:384         Loss:83.54957898072956\n",
      "Epoch:385         Loss:83.4494235417663\n",
      "Epoch:386         Loss:83.3497765550289\n",
      "Epoch:387         Loss:83.25096996814779\n",
      "Epoch:388         Loss:83.15365152586062\n",
      "Epoch:389         Loss:83.05793811435719\n",
      "Epoch:390         Loss:82.96291403694032\n",
      "Epoch:391         Loss:82.86842345622838\n",
      "Epoch:392         Loss:82.7744485493462\n",
      "Epoch:393         Loss:82.68097235863634\n",
      "Epoch:394         Loss:82.58772796867795\n",
      "Epoch:395         Loss:82.49520200492911\n",
      "Epoch:396         Loss:82.40384340388096\n",
      "Epoch:397         Loss:82.31302595853128\n",
      "Epoch:398         Loss:82.2227121106766\n",
      "Epoch:399         Loss:82.13288483051973\n",
      "Epoch:400         Loss:82.0435279242342\n",
      "Epoch:401         Loss:81.95470315679692\n",
      "Epoch:402         Loss:81.86618372526209\n",
      "Epoch:403         Loss:81.77815110379154\n",
      "Epoch:404         Loss:81.69060855140182\n",
      "Epoch:405         Loss:81.60391845023453\n",
      "Epoch:406         Loss:81.51817387924639\n",
      "Epoch:407         Loss:81.43284612133027\n",
      "Epoch:408         Loss:81.34792180591758\n",
      "Epoch:409         Loss:81.26348078659753\n",
      "Epoch:410         Loss:81.17909686266628\n",
      "Epoch:411         Loss:81.09432125067337\n",
      "Epoch:412         Loss:81.00966301445023\n",
      "Epoch:413         Loss:80.92545383422673\n",
      "Epoch:414         Loss:80.84199312547148\n",
      "Epoch:415         Loss:80.7597653367545\n",
      "Epoch:416         Loss:80.678779100409\n",
      "Epoch:417         Loss:80.59800015198766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:418         Loss:80.51851278057349\n",
      "Epoch:419         Loss:80.43938602139445\n",
      "Epoch:420         Loss:80.36060780842838\n",
      "Epoch:421         Loss:80.28223530825737\n",
      "Epoch:422         Loss:80.20406444369841\n",
      "Epoch:423         Loss:80.12626660411543\n",
      "Epoch:424         Loss:80.04877463715766\n",
      "Epoch:425         Loss:79.97181575629331\n",
      "Epoch:426         Loss:79.8961132039735\n",
      "Epoch:427         Loss:79.82120799268932\n",
      "Epoch:428         Loss:79.74665652972932\n",
      "Epoch:429         Loss:79.6724416310807\n",
      "Epoch:430         Loss:79.59886563369666\n",
      "Epoch:431         Loss:79.52584392128004\n",
      "Epoch:432         Loss:79.45308357387017\n",
      "Epoch:433         Loss:79.3811698962838\n",
      "Epoch:434         Loss:79.30962566541773\n",
      "Epoch:435         Loss:79.23843536058452\n",
      "Epoch:436         Loss:79.16758437276937\n",
      "Epoch:437         Loss:79.097058948487\n",
      "Epoch:438         Loss:79.02684613708008\n",
      "Epoch:439         Loss:78.95693960545422\n",
      "Epoch:440         Loss:78.8872644046857\n",
      "Epoch:441         Loss:78.81786261436888\n",
      "Epoch:442         Loss:78.7486668721991\n",
      "Epoch:443         Loss:78.67972327183804\n",
      "Epoch:444         Loss:78.61101744208771\n",
      "Epoch:445         Loss:78.54249333691574\n",
      "Epoch:446         Loss:78.47419472495262\n",
      "Epoch:447         Loss:78.40611457692877\n",
      "Epoch:448         Loss:78.33824624866976\n",
      "Epoch:449         Loss:78.27058345774344\n",
      "Epoch:450         Loss:78.20310849998378\n",
      "Epoch:451         Loss:78.13575469165329\n",
      "Epoch:452         Loss:78.06858778878158\n",
      "Epoch:453         Loss:78.00160287465519\n",
      "Epoch:454         Loss:77.93473906334607\n",
      "Epoch:455         Loss:77.86797423024008\n",
      "Epoch:456         Loss:77.80134289057574\n",
      "Epoch:457         Loss:77.73487461881855\n",
      "Epoch:458         Loss:77.66856579739839\n",
      "Epoch:459         Loss:77.60241298667532\n",
      "Epoch:460         Loss:77.53641291459138\n",
      "Epoch:461         Loss:77.47056246696135\n",
      "Epoch:462         Loss:77.40508483196382\n",
      "Epoch:463         Loss:77.3403077991745\n",
      "Epoch:464         Loss:77.27568445760234\n",
      "Epoch:465         Loss:77.21113433455896\n",
      "Epoch:466         Loss:77.14673793094903\n",
      "Epoch:467         Loss:77.08247222625592\n",
      "Epoch:468         Loss:77.01831438676219\n",
      "Epoch:469         Loss:76.95429941639549\n",
      "Epoch:470         Loss:76.89042435576857\n",
      "Epoch:471         Loss:76.82668639248236\n",
      "Epoch:472         Loss:76.76316980931459\n",
      "Epoch:473         Loss:76.70026942012117\n",
      "Epoch:474         Loss:76.63751017266813\n",
      "Epoch:475         Loss:76.57488880140181\n",
      "Epoch:476         Loss:76.51240222242818\n",
      "Epoch:477         Loss:76.45004752184487\n",
      "Epoch:478         Loss:76.38797007439138\n",
      "Epoch:479         Loss:76.32612743027248\n",
      "Epoch:480         Loss:76.2643475728787\n",
      "Epoch:481         Loss:76.20275017937693\n",
      "Epoch:482         Loss:76.14127868398415\n",
      "Epoch:483         Loss:76.0799307609185\n",
      "Epoch:484         Loss:76.01870419975022\n",
      "Epoch:485         Loss:75.95754847985575\n",
      "Epoch:486         Loss:75.89646434564175\n",
      "Epoch:487         Loss:75.8356607114182\n",
      "Epoch:488         Loss:75.77517954603917\n",
      "Epoch:489         Loss:75.7150914696968\n",
      "Epoch:490         Loss:75.65548568766546\n",
      "Epoch:491         Loss:75.59596971766747\n",
      "Epoch:492         Loss:75.53651363057287\n",
      "Epoch:493         Loss:75.47717969613993\n",
      "Epoch:494         Loss:75.41812230794426\n",
      "Epoch:495         Loss:75.35966468986851\n",
      "Epoch:496         Loss:75.30134041920138\n",
      "Epoch:497         Loss:75.24314578567115\n",
      "Epoch:498         Loss:75.18507730694722\n",
      "Epoch:499         Loss:75.12713171300193\n",
      "Epoch:500         Loss:75.06930593157252\n",
      "Epoch:501         Loss:75.01150030317442\n",
      "Epoch:502         Loss:74.95379435999261\n",
      "Epoch:503         Loss:74.89662848843754\n",
      "Epoch:504         Loss:74.83958948677423\n",
      "Epoch:505         Loss:74.78266254646074\n",
      "Epoch:506         Loss:74.72612750425687\n",
      "Epoch:507         Loss:74.66972331141724\n",
      "Epoch:508         Loss:74.61344588379988\n",
      "Epoch:509         Loss:74.55729140776987\n",
      "Epoch:510         Loss:74.501256320698\n",
      "Epoch:511         Loss:74.44568975345149\n",
      "Epoch:512         Loss:74.3903324845933\n",
      "Epoch:513         Loss:74.33495310256826\n",
      "Epoch:514         Loss:74.27997454468544\n",
      "Epoch:515         Loss:74.22513971946638\n",
      "Epoch:516         Loss:74.17036341094226\n",
      "Epoch:517         Loss:74.1156578844048\n",
      "Epoch:518         Loss:74.06108129309634\n",
      "Epoch:519         Loss:74.00662925564575\n",
      "Epoch:520         Loss:73.9522976983163\n",
      "Epoch:521         Loss:73.89808283191527\n",
      "Epoch:522         Loss:73.84398113044941\n",
      "Epoch:523         Loss:73.78998644294927\n",
      "Epoch:524         Loss:73.73647554069325\n",
      "Epoch:525         Loss:73.68308941091665\n",
      "Epoch:526         Loss:73.62982373114846\n",
      "Epoch:527         Loss:73.57667448624674\n",
      "Epoch:528         Loss:73.52366857873362\n",
      "Epoch:529         Loss:73.47085691946023\n",
      "Epoch:530         Loss:73.41846078725894\n",
      "Epoch:531         Loss:73.36608079964547\n",
      "Epoch:532         Loss:73.31395355916055\n",
      "Epoch:533         Loss:73.26183658543172\n",
      "Epoch:534         Loss:73.21002779246601\n",
      "Epoch:535         Loss:73.15864727214786\n",
      "Epoch:536         Loss:73.10726600895897\n",
      "Epoch:537         Loss:73.05605505881441\n",
      "Epoch:538         Loss:73.0049593307072\n",
      "Epoch:539         Loss:72.95397501919737\n",
      "Epoch:540         Loss:72.90311336688401\n",
      "Epoch:541         Loss:72.85238608405902\n",
      "Epoch:542         Loss:72.80163784921375\n",
      "Epoch:543         Loss:72.75142819276239\n",
      "Epoch:544         Loss:72.701315795645\n",
      "Epoch:545         Loss:72.65129819339609\n",
      "Epoch:546         Loss:72.60137307821738\n",
      "Epoch:547         Loss:72.5516954177423\n",
      "Epoch:548         Loss:72.50234029796749\n",
      "Epoch:549         Loss:72.45307745374893\n",
      "Epoch:550         Loss:72.40381459214619\n",
      "Epoch:551         Loss:72.35479368839276\n",
      "Epoch:552         Loss:72.3059410480937\n",
      "Epoch:553         Loss:72.25717563585597\n",
      "Epoch:554         Loss:72.2084957404017\n",
      "Epoch:555         Loss:72.15989974087738\n",
      "Epoch:556         Loss:72.11137726217927\n",
      "Epoch:557         Loss:72.06299157420908\n",
      "Epoch:558         Loss:72.01468745565131\n",
      "Epoch:559         Loss:71.96646339826076\n",
      "Epoch:560         Loss:71.9183179739436\n",
      "Epoch:561         Loss:71.87024982926886\n",
      "Epoch:562         Loss:71.82222859476158\n",
      "Epoch:563         Loss:71.77422557518003\n",
      "Epoch:564         Loss:71.72629596547024\n",
      "Epoch:565         Loss:71.67848407647062\n",
      "Epoch:566         Loss:71.63097817701241\n",
      "Epoch:567         Loss:71.58366353434934\n",
      "Epoch:568         Loss:71.53636277243348\n",
      "Epoch:569         Loss:71.48908557529009\n",
      "Epoch:570         Loss:71.44152328579021\n",
      "Epoch:571         Loss:71.39470052484556\n",
      "Epoch:572         Loss:71.34793988605846\n",
      "Epoch:573         Loss:71.30134428685608\n",
      "Epoch:574         Loss:71.25474620512112\n",
      "Epoch:575         Loss:71.20831180827658\n",
      "Epoch:576         Loss:71.16164852550784\n",
      "Epoch:577         Loss:71.11509040630375\n",
      "Epoch:578         Loss:71.0685113109543\n",
      "Epoch:579         Loss:71.02198582951651\n",
      "Epoch:580         Loss:70.97537520395802\n",
      "Epoch:581         Loss:70.92893725894436\n",
      "Epoch:582         Loss:70.88246869204525\n",
      "Epoch:583         Loss:70.83604531038273\n",
      "Epoch:584         Loss:70.78950074294536\n",
      "Epoch:585         Loss:70.74312211521276\n",
      "Epoch:586         Loss:70.69671401040893\n",
      "Epoch:587         Loss:70.65046464965161\n",
      "Epoch:588         Loss:70.60380525879644\n",
      "Epoch:589         Loss:70.55684866312878\n",
      "Epoch:590         Loss:70.51013674899762\n",
      "Epoch:591         Loss:70.4635206278885\n",
      "Epoch:592         Loss:70.41704115160641\n",
      "Epoch:593         Loss:70.37052759915065\n",
      "Epoch:594         Loss:70.32417126936423\n",
      "Epoch:595         Loss:70.27780738249132\n",
      "Epoch:596         Loss:70.23159221510835\n",
      "Epoch:597         Loss:70.18546155801063\n",
      "Epoch:598         Loss:70.1394685734151\n",
      "Epoch:599         Loss:70.09335208000853\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# stochastic gradient descent\n",
    "\n",
    "# updating the weights\n",
    "\n",
    "t=0\n",
    "thresh=150\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss=0\n",
    "    for j in np.arange(0,x_train.shape[1],batch_size):       \n",
    "        dW = np.zeros(W[0].shape)\n",
    "        dM = np.zeros(W[1].shape)\n",
    "        dT = np.zeros(W[2].shape)\n",
    "        db1 = np.zeros(b[0].shape)\n",
    "        db2 = np.zeros(b[1].shape)\n",
    "        db3 = np.zeros(b[2].shape)\n",
    "        for k in range(0,batch_size):\n",
    "            # forward pass\n",
    "            x = x_train[:,j+k].reshape(-1,1)\n",
    "            y = y_train[:,j+k].reshape(-1,1)\n",
    "            out,y_pred=forward_path(noHiddenLayers,x,W,b,Actfnvect)\n",
    "            # backpropagation\n",
    "            dWtemp,dMtemp,dTtemp,db1temp,db2temp,db3temp=backprop(y_pred,y,out[1],W[-1],out[0],W[1],x)\n",
    "            \n",
    "            dW=dW+dWtemp\n",
    "            db1=db1+db1temp\n",
    "            \n",
    "            dM=dM+dMtemp\n",
    "            db2=db2+db2temp\n",
    "            \n",
    "            dT=dT+dTtemp\n",
    "            db3=db3+db3temp\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = loss + (-1.0*np.sum(y*np.log(y_pred)))\n",
    "         \n",
    "        if t<thresh:\n",
    "            alpha=(t*1.0/thresh)\n",
    "            learning_rate=(1-alpha)*learning_rate_0+alpha*learning_rate_t\n",
    "        \n",
    "        t=t+1\n",
    "        # Updating the weights SGD\n",
    "        b[0]=b[0]-learning_rate*db1*(1.0/batch_size)\n",
    "        W[0]=W[0]-learning_rate*dW*(1.0/batch_size)\n",
    "\n",
    "        b[1]=b[1]-learning_rate*db2*(1.0/batch_size)\n",
    "        W[1]=W[1]-learning_rate*dM*(1.0/batch_size)\n",
    "\n",
    "        b[2]=b[2]-learning_rate*db3*(1.0/batch_size)\n",
    "        W[2]=W[2]-learning_rate*dT*(1.0/batch_size)\n",
    "    \n",
    "    #print the loss in each epoch\n",
    "    print('Epoch:'+str(i)+'         Loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,y_pred=forward_path(noHiddenLayers,x_test,W,b,Actfnvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(y):\n",
    "    return np.argmax(y)\n",
    "\n",
    "yvect=[]\n",
    "y_trurevect=[]\n",
    "\n",
    "for i in range(0,x_test.shape[1]):\n",
    "    yvect.append(predict(y_pred[:,i]))\n",
    "    y_trurevect.append(predict(y_test[:,i]))\n",
    "\n",
    "# find accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "#predicting test accuracy\n",
    "print(accuracy_score(y_trurevect, yvect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# to see the output vs true values\n",
    "\n",
    "print y_trurevect\n",
    "print yvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print learning_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
