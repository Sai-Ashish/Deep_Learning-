{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Note:run in python2######\n",
    "# same initialization of weights because same random seed used\n",
    "\n",
    "# OBSERVATIONS\n",
    "# The RMSProp modiﬁes AdaGrad to perform better in the nonconvex setting by \n",
    "# changing the gradient accumulation into an exponentially weighted moving average.\n",
    "\n",
    "# RMSProp uses an exponentially decaying average to discard history from the extreme past so that\n",
    "# it can converge rapidly after ﬁnding a convex bowl, as if it were an instance of the\n",
    "# AdaGrad algorithm initialized within that bowl\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#MLP\n",
    "  \n",
    "# initialisation of the weights he_normal\n",
    "def weights(noHiddenLayers,sizeOfLayers):\n",
    "\n",
    "    W=[]\n",
    "    b=[]\n",
    "\n",
    "    for i in range(0,noHiddenLayers+1):\n",
    "        \n",
    "        W.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+sizeOfLayers[i])),\n",
    "                                   (sizeOfLayers[i+1],sizeOfLayers[i])) )\n",
    "        b.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+1)),\n",
    "                                   (sizeOfLayers[i+1],1)) )\n",
    "\n",
    "    W=np.array(W)\n",
    "    b=np.array(b)\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "#mlp forward pass\n",
    "#layer\n",
    "def layer(w,x,b):\n",
    "    out = np.matmul(w,x)+b\n",
    "    return out\n",
    "\n",
    "def apply_activationMLP(Activation_function,inp):\n",
    "    \n",
    "    #activation functions\n",
    "    if Activation_function == 'relu':\n",
    "        return np.where(inp<0,0,inp)\n",
    "    elif Activation_function == \"tanh\":\n",
    "        return np.tanh(inp)\n",
    "    elif Activation_function == \"sigmoid\":\n",
    "        return 1.0/(1+np.exp(-1.0*inp))\n",
    "    elif Activation_function == \"softmax\":\n",
    "        return (1.0/(np.sum(np.exp(inp),axis=0)))*(np.exp(inp))\n",
    "\n",
    "#forward path\n",
    "def forward_path(noHiddenLayers,X,W,b,Actfnvect):\n",
    "\n",
    "    out=[]\n",
    "    \n",
    "    z=apply_activationMLP(Actfnvect[0],np.array(layer(W[0],X,b[0])))\n",
    "    out.append(np.array(z))\n",
    "\n",
    "    for i in range(1,noHiddenLayers):\n",
    "        z=apply_activationMLP(Actfnvect[i],np.array(layer(W[i],out[i-1],b[i])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    if noHiddenLayers > 0:\n",
    "        z=apply_activationMLP(Actfnvect[-1],np.array(layer(W[-1],out[-1],b[-1])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    y_pred = out[-1]\n",
    "\n",
    "    return np.array(out),np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "#only to import data\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# #import data\n",
    "iris_data = load_iris() # load the iris dataset\n",
    "\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "# Split the data for training and testing\n",
    "x_train , x_test, y_train , y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(3, 120)\n",
      "(4, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "# run MLP algorithm\n",
    "\n",
    "x_train = np.moveaxis(x_train,0,-1)\n",
    "y_train = np.moveaxis(y_train,0,-1)\n",
    "x_test = np.moveaxis(x_test,0,-1)\n",
    "y_test = np.moveaxis(y_test,0,-1)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_relu(x):\n",
    "    return np.where(x == 0,0,1)\n",
    "\n",
    "# backpropation\n",
    "def backprop(y,y_true,z,T,u,M,x):\n",
    "    \n",
    "    dy = (y-y_true)\n",
    "    \n",
    "    # layer 3\n",
    "    dT  = np.matmul(dy,z.T)\n",
    "    db3 = np.sum(dy,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 2\n",
    "    s   = np.matmul(T.T,dy)\n",
    "    s   = s*der_relu(z)\n",
    "    dM  = np.matmul(s,u.T)\n",
    "    db2 = np.sum(s,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 1\n",
    "    sm  = np.matmul(M.T,s)\n",
    "    sm  = sm*der_relu(u)\n",
    "    dW  = np.matmul(sm,x.T)\n",
    "    db1 = np.sum(sm,axis=1).reshape(-1,1)\n",
    "    \n",
    "    return dW,dM,dT,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "################################\n",
    "# Training parameters\n",
    "num_classes = 3\n",
    "epochs = 200\n",
    "rho=0.1\n",
    "#1-rho=0.9\n",
    "batch_size = 10\n",
    "learning_rate = 1e-3\n",
    "delta=1e-10\n",
    "################################\n",
    "\n",
    "#MLP PARAMETERS\n",
    "noHiddenLayers=2\n",
    "\n",
    "#also includes the input vector dimension and output vector dimension\n",
    "sizeOfLayers=[x_train.shape[0],10,10,num_classes]\n",
    "\n",
    "sizeofOutput=num_classes\n",
    "\n",
    "Actfnvect = ['relu','relu','softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = weights(noHiddenLayers,sizeOfLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0         Loss:236.62075380767385\n",
      "Epoch:1         Loss:204.62371116976232\n",
      "Epoch:2         Loss:175.47946893658693\n",
      "Epoch:3         Loss:152.2019874835539\n",
      "Epoch:4         Loss:131.74629910481846\n",
      "Epoch:5         Loss:113.81205241642373\n",
      "Epoch:6         Loss:98.0435995188555\n",
      "Epoch:7         Loss:83.94687062149323\n",
      "Epoch:8         Loss:73.15912127422013\n",
      "Epoch:9         Loss:65.89934130657187\n",
      "Epoch:10         Loss:60.58958473880675\n",
      "Epoch:11         Loss:56.44375284185741\n",
      "Epoch:12         Loss:53.07295430513891\n",
      "Epoch:13         Loss:50.422070184741415\n",
      "Epoch:14         Loss:48.17675758498454\n",
      "Epoch:15         Loss:46.037493089718716\n",
      "Epoch:16         Loss:43.94097598028094\n",
      "Epoch:17         Loss:41.85621976732602\n",
      "Epoch:18         Loss:40.00344528167049\n",
      "Epoch:19         Loss:38.36914281584682\n",
      "Epoch:20         Loss:36.855283664999554\n",
      "Epoch:21         Loss:35.43997973941497\n",
      "Epoch:22         Loss:34.11236248128502\n",
      "Epoch:23         Loss:32.86003859108475\n",
      "Epoch:24         Loss:31.656601556454703\n",
      "Epoch:25         Loss:30.51115589083216\n",
      "Epoch:26         Loss:29.43048676707761\n",
      "Epoch:27         Loss:28.39817290600904\n",
      "Epoch:28         Loss:27.408832976331922\n",
      "Epoch:29         Loss:26.46346492503893\n",
      "Epoch:30         Loss:25.57029950324556\n",
      "Epoch:31         Loss:24.712272873286864\n",
      "Epoch:32         Loss:23.898155290214028\n",
      "Epoch:33         Loss:23.124875344341405\n",
      "Epoch:34         Loss:22.389421996835896\n",
      "Epoch:35         Loss:21.69143180067968\n",
      "Epoch:36         Loss:21.027912571229805\n",
      "Epoch:37         Loss:20.4001145543818\n",
      "Epoch:38         Loss:19.804051064970164\n",
      "Epoch:39         Loss:19.224421527546827\n",
      "Epoch:40         Loss:18.625585341361052\n",
      "Epoch:41         Loss:17.990900937230222\n",
      "Epoch:42         Loss:17.18872370040637\n",
      "Epoch:43         Loss:16.479912211028196\n",
      "Epoch:44         Loss:15.94814829251214\n",
      "Epoch:45         Loss:15.460932303190003\n",
      "Epoch:46         Loss:15.007646122676402\n",
      "Epoch:47         Loss:14.626331149211984\n",
      "Epoch:48         Loss:14.289493409871957\n",
      "Epoch:49         Loss:13.998634159731342\n",
      "Epoch:50         Loss:13.731979745281198\n",
      "Epoch:51         Loss:13.495573776377425\n",
      "Epoch:52         Loss:13.284041266975303\n",
      "Epoch:53         Loss:13.095986746656893\n",
      "Epoch:54         Loss:12.926774095407445\n",
      "Epoch:55         Loss:12.771292822644966\n",
      "Epoch:56         Loss:12.626949030052483\n",
      "Epoch:57         Loss:12.492639231536979\n",
      "Epoch:58         Loss:12.3686167042866\n",
      "Epoch:59         Loss:12.25185022282549\n",
      "Epoch:60         Loss:12.142701069779525\n",
      "Epoch:61         Loss:12.04374112661108\n",
      "Epoch:62         Loss:11.943674552226371\n",
      "Epoch:63         Loss:11.851607372650523\n",
      "Epoch:64         Loss:11.763642022664994\n",
      "Epoch:65         Loss:11.680682935937924\n",
      "Epoch:66         Loss:11.60019799914264\n",
      "Epoch:67         Loss:11.52310073868793\n",
      "Epoch:68         Loss:11.45003867452106\n",
      "Epoch:69         Loss:11.380553274830529\n",
      "Epoch:70         Loss:11.312759588608222\n",
      "Epoch:71         Loss:11.248660853876798\n",
      "Epoch:72         Loss:11.185859159777749\n",
      "Epoch:73         Loss:11.12527551623633\n",
      "Epoch:74         Loss:11.06670763485593\n",
      "Epoch:75         Loss:11.010474601802592\n",
      "Epoch:76         Loss:10.955190480518787\n",
      "Epoch:77         Loss:10.901400475521907\n",
      "Epoch:78         Loss:10.84912095226816\n",
      "Epoch:79         Loss:10.798557791734035\n",
      "Epoch:80         Loss:10.74900880511133\n",
      "Epoch:81         Loss:10.701107799590723\n",
      "Epoch:82         Loss:10.653910256469837\n",
      "Epoch:83         Loss:10.60758294505855\n",
      "Epoch:84         Loss:10.562862003304666\n",
      "Epoch:85         Loss:10.519002077560327\n",
      "Epoch:86         Loss:10.476214373142225\n",
      "Epoch:87         Loss:10.43394734187747\n",
      "Epoch:88         Loss:10.393042894387172\n",
      "Epoch:89         Loss:10.351539508161867\n",
      "Epoch:90         Loss:10.312265396415123\n",
      "Epoch:91         Loss:10.274618416187103\n",
      "Epoch:92         Loss:10.235040490862046\n",
      "Epoch:93         Loss:10.197745904441902\n",
      "Epoch:94         Loss:10.164027014057053\n",
      "Epoch:95         Loss:10.12875971321053\n",
      "Epoch:96         Loss:10.09712224203501\n",
      "Epoch:97         Loss:10.064958085217361\n",
      "Epoch:98         Loss:10.032576114056106\n",
      "Epoch:99         Loss:10.001126838084131\n",
      "Epoch:100         Loss:9.96823768612966\n",
      "Epoch:101         Loss:9.939042175343562\n",
      "Epoch:102         Loss:9.907046757119167\n",
      "Epoch:103         Loss:9.878666831912977\n",
      "Epoch:104         Loss:9.848018410885965\n",
      "Epoch:105         Loss:9.819077533338609\n",
      "Epoch:106         Loss:9.789942654899411\n",
      "Epoch:107         Loss:9.761795398248124\n",
      "Epoch:108         Loss:9.733688463326542\n",
      "Epoch:109         Loss:9.705954524257061\n",
      "Epoch:110         Loss:9.67857989890697\n",
      "Epoch:111         Loss:9.651389048725054\n",
      "Epoch:112         Loss:9.62458539739737\n",
      "Epoch:113         Loss:9.598071841164435\n",
      "Epoch:114         Loss:9.571885152658796\n",
      "Epoch:115         Loss:9.54584305200254\n",
      "Epoch:116         Loss:9.521550868336329\n",
      "Epoch:117         Loss:9.495674656003091\n",
      "Epoch:118         Loss:9.470533738323239\n",
      "Epoch:119         Loss:9.445702588447249\n",
      "Epoch:120         Loss:9.421109434591294\n",
      "Epoch:121         Loss:9.39674903257788\n",
      "Epoch:122         Loss:9.37261587714193\n",
      "Epoch:123         Loss:9.34870500162303\n",
      "Epoch:124         Loss:9.325011836633687\n",
      "Epoch:125         Loss:9.301532012482165\n",
      "Epoch:126         Loss:9.278261359898737\n",
      "Epoch:127         Loss:9.255195896608054\n",
      "Epoch:128         Loss:9.254536732724121\n",
      "Epoch:129         Loss:9.226851979610244\n",
      "Epoch:130         Loss:9.205803046341327\n",
      "Epoch:131         Loss:9.185017097225925\n",
      "Epoch:132         Loss:9.164186432996988\n",
      "Epoch:133         Loss:9.143380048241864\n",
      "Epoch:134         Loss:9.123201824761034\n",
      "Epoch:135         Loss:9.103243630168379\n",
      "Epoch:136         Loss:9.083599112006414\n",
      "Epoch:137         Loss:9.065209887893667\n",
      "Epoch:138         Loss:9.046319550022941\n",
      "Epoch:139         Loss:9.027466502357916\n",
      "Epoch:140         Loss:9.008902943919448\n",
      "Epoch:141         Loss:8.990595717433584\n",
      "Epoch:142         Loss:8.972532779164798\n",
      "Epoch:143         Loss:8.954707800716392\n",
      "Epoch:144         Loss:8.936961581178211\n",
      "Epoch:145         Loss:8.92101017908641\n",
      "Epoch:146         Loss:8.904318319420987\n",
      "Epoch:147         Loss:8.885876345202181\n",
      "Epoch:148         Loss:8.875301564038825\n",
      "Epoch:149         Loss:8.857982478345708\n",
      "Epoch:150         Loss:8.841683991027958\n",
      "Epoch:151         Loss:8.82508650412496\n",
      "Epoch:152         Loss:8.810486074903784\n",
      "Epoch:153         Loss:8.79542804734608\n",
      "Epoch:154         Loss:8.780518066874412\n",
      "Epoch:155         Loss:8.765804531406424\n",
      "Epoch:156         Loss:8.751285608167157\n",
      "Epoch:157         Loss:8.736951760488957\n",
      "Epoch:158         Loss:8.723571770235681\n",
      "Epoch:159         Loss:8.709508658734855\n",
      "Epoch:160         Loss:8.695654508992126\n",
      "Epoch:161         Loss:8.68169918445245\n",
      "Epoch:162         Loss:8.668172718377626\n",
      "Epoch:163         Loss:8.655057884105615\n",
      "Epoch:164         Loss:8.641544131240003\n",
      "Epoch:165         Loss:8.628547995620274\n",
      "Epoch:166         Loss:8.615854714906213\n",
      "Epoch:167         Loss:8.60291953172503\n",
      "Epoch:168         Loss:8.590275400158259\n",
      "Epoch:169         Loss:8.577711010507322\n",
      "Epoch:170         Loss:8.565534330997524\n",
      "Epoch:171         Loss:8.552961506771503\n",
      "Epoch:172         Loss:8.540782747004318\n",
      "Epoch:173         Loss:8.528679323761372\n",
      "Epoch:174         Loss:8.5167146541941\n",
      "Epoch:175         Loss:8.504759095838672\n",
      "Epoch:176         Loss:8.4930181607823\n",
      "Epoch:177         Loss:8.481418539392484\n",
      "Epoch:178         Loss:8.470004235577504\n",
      "Epoch:179         Loss:8.458649423710005\n",
      "Epoch:180         Loss:8.4473942333784\n",
      "Epoch:181         Loss:8.436320008075548\n",
      "Epoch:182         Loss:8.42530363053377\n",
      "Epoch:183         Loss:8.414382053925843\n",
      "Epoch:184         Loss:8.403636616855712\n",
      "Epoch:185         Loss:8.39294688687181\n",
      "Epoch:186         Loss:8.382387468958836\n",
      "Epoch:187         Loss:8.371908254487495\n",
      "Epoch:188         Loss:8.361517160586102\n",
      "Epoch:189         Loss:8.351294483177245\n",
      "Epoch:190         Loss:8.341124131094137\n",
      "Epoch:191         Loss:8.331457717254294\n",
      "Epoch:192         Loss:8.321973095560814\n",
      "Epoch:193         Loss:8.312045084152183\n",
      "Epoch:194         Loss:8.30242335829642\n",
      "Epoch:195         Loss:8.292749546242351\n",
      "Epoch:196         Loss:8.28315720110171\n",
      "Epoch:197         Loss:8.273685852385695\n",
      "Epoch:198         Loss:8.264343371004484\n",
      "Epoch:199         Loss:8.255045959836915\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# RMSPROP descent\n",
    "\n",
    "# updating the weights\n",
    "\n",
    "\n",
    "r = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss=0\n",
    "    for j in np.arange(0,x_train.shape[1],batch_size):       \n",
    "        dW = np.zeros(W[0].shape)\n",
    "        dM = np.zeros(W[1].shape)\n",
    "        dT = np.zeros(W[2].shape)\n",
    "        db1 = np.zeros(b[0].shape)\n",
    "        db2 = np.zeros(b[1].shape)\n",
    "        db3 = np.zeros(b[2].shape)\n",
    "        for k in range(0,batch_size):\n",
    "            # forward pass\n",
    "            x = x_train[:,j+k].reshape(-1,1)\n",
    "            y = y_train[:,j+k].reshape(-1,1)\n",
    "            out,y_pred=forward_path(noHiddenLayers,x,W,b,Actfnvect)\n",
    "            # backpropagation\n",
    "            dWtemp,dMtemp,dTtemp,db1temp,db2temp,db3temp=backprop(y_pred,y,out[1],W[-1],out[0],W[1],x)\n",
    "            \n",
    "            dW=dW+dWtemp\n",
    "            db1=db1+db1temp\n",
    "            \n",
    "            dM=dM+dMtemp\n",
    "            db2=db2+db2temp\n",
    "            \n",
    "            dT=dT+dTtemp\n",
    "            db3=db3+db3temp\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = loss + (-1.0*np.sum(y*np.log(y_pred)))\n",
    "            \n",
    "        # Updating the weights using RMSPROP Approach\n",
    "        \n",
    "        dW=dW*(1.0/batch_size)\n",
    "        dM=dM*(1.0/batch_size)\n",
    "        dT=dT*(1.0/batch_size)\n",
    "        db1=db1*(1.0/batch_size)\n",
    "        db2=db2*(1.0/batch_size)\n",
    "        db3=db3*(1.0/batch_size)\n",
    "        \n",
    "        r[\"W[0]\"] = (1-rho)*r[\"W[0]\"] + (rho*(dW*dW))\n",
    "        r[\"W[1]\"] = (1-rho)*r[\"W[1]\"] + (rho*(dM*dM))\n",
    "        r[\"W[2]\"] = (1-rho)*r[\"W[2]\"] + (rho*(dT*dT))\n",
    "        r[\"b[0]\"] = (1-rho)*r[\"b[0]\"] + (rho*(db1*db1))\n",
    "        r[\"b[1]\"] = (1-rho)*r[\"b[1]\"] + (rho*(db2*db2))\n",
    "        r[\"b[2]\"] = (1-rho)*r[\"b[2]\"] + (rho*(db3*db3))\n",
    "        \n",
    "        W[0] = W[0] - ((learning_rate*dW)/np.sqrt(delta +r[\"W[0]\"]))\n",
    "        W[1] = W[1] - ((learning_rate*dM)/np.sqrt(delta +r[\"W[1]\"]))\n",
    "        W[2] = W[2] - ((learning_rate*dT)/np.sqrt(delta +r[\"W[2]\"]))\n",
    "        b[0] = b[0] - ((learning_rate*db1)/np.sqrt(delta +r[\"b[0]\"]))\n",
    "        b[1] = b[1] - ((learning_rate*db2)/np.sqrt(delta +r[\"b[1]\"]))\n",
    "        b[2] = b[2] - ((learning_rate*db3)/np.sqrt(delta +r[\"b[2]\"]))\n",
    "    \n",
    "    #print the loss in each epoch\n",
    "    print('Epoch:'+str(i)+'         Loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,y_pred=forward_path(noHiddenLayers,x_test,W,b,Actfnvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(y):\n",
    "    return np.argmax(y)\n",
    "\n",
    "yvect=[]\n",
    "y_trurevect=[]\n",
    "\n",
    "for i in range(0,x_test.shape[1]):\n",
    "    yvect.append(predict(y_pred[:,i]))\n",
    "    y_trurevect.append(predict(y_test[:,i]))\n",
    "\n",
    "# find accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "#predicting test accuracy\n",
    "print(accuracy_score(y_trurevect, yvect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# to see the output vs true values\n",
    "\n",
    "print y_trurevect\n",
    "print yvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
