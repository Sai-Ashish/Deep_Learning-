{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Note:run in python2######\n",
    "# same initialization of weights because same random seed used\n",
    "\n",
    "#SGD has\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#MLP\n",
    "\n",
    "# initialisation of the weights he_normal\n",
    "def weights(noHiddenLayers,sizeOfLayers):\n",
    "\n",
    "    W=[]\n",
    "    b=[]\n",
    "\n",
    "    for i in range(0,noHiddenLayers+1):\n",
    "        \n",
    "        W.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+sizeOfLayers[i])),\n",
    "                                   (sizeOfLayers[i+1],sizeOfLayers[i])) )\n",
    "        b.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+1)),\n",
    "                                   (sizeOfLayers[i+1],1)) )\n",
    "\n",
    "    W=np.array(W)\n",
    "    b=np.array(b)\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "\n",
    "#mlp forward pass\n",
    "#layer\n",
    "def layer(w,x,b):\n",
    "    out = np.matmul(w,x)+b\n",
    "    return out\n",
    "\n",
    "def apply_activationMLP(Activation_function,inp):\n",
    "    \n",
    "    #activation functions\n",
    "    if Activation_function == 'relu':\n",
    "        return np.where(inp<0,0,inp)\n",
    "    elif Activation_function == \"tanh\":\n",
    "        return np.tanh(inp)\n",
    "    elif Activation_function == \"sigmoid\":\n",
    "        return 1.0/(1+np.exp(-1.0*inp))\n",
    "    elif Activation_function == \"softmax\":\n",
    "        return (1.0/(np.sum(np.exp(inp),axis=0)))*(np.exp(inp))\n",
    "\n",
    "#forward path\n",
    "def forward_path(noHiddenLayers,X,W,b,Actfnvect):\n",
    "\n",
    "    out=[]\n",
    "    \n",
    "    z=apply_activationMLP(Actfnvect[0],np.array(layer(W[0],X,b[0])))\n",
    "    out.append(np.array(z))\n",
    "\n",
    "    for i in range(1,noHiddenLayers):\n",
    "        z=apply_activationMLP(Actfnvect[i],np.array(layer(W[i],out[i-1],b[i])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    if noHiddenLayers > 0:\n",
    "        z=apply_activationMLP(Actfnvect[-1],np.array(layer(W[-1],out[-1],b[-1])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    y_pred = out[-1]\n",
    "\n",
    "    return np.array(out),np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "#only to import data\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# #import data\n",
    "iris_data = load_iris() # load the iris dataset\n",
    "\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "# Split the data for training and testing\n",
    "x_train , x_test, y_train , y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(3, 120)\n",
      "(4, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "# run MLP algorithm\n",
    "\n",
    "x_train = np.moveaxis(x_train,0,-1)\n",
    "y_train = np.moveaxis(y_train,0,-1)\n",
    "x_test = np.moveaxis(x_test,0,-1)\n",
    "y_test = np.moveaxis(y_test,0,-1)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_relu(x):\n",
    "    return np.where(x == 0,0,1)\n",
    "\n",
    "# backpropation\n",
    "def backprop(y,y_true,z,T,u,M,x):\n",
    "    \n",
    "    dy = (y-y_true)\n",
    "    \n",
    "    # layer 3\n",
    "    dT  = np.matmul(dy,z.T)\n",
    "    db3 = np.sum(dy,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 2\n",
    "    s   = np.matmul(T.T,dy)\n",
    "    s   = s*der_relu(z)\n",
    "    dM  = np.matmul(s,u.T)\n",
    "    db2 = np.sum(s,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 1\n",
    "    sm  = np.matmul(M.T,s)\n",
    "    sm  = sm*der_relu(u)\n",
    "    dW  = np.matmul(sm,x.T)\n",
    "    db1 = np.sum(sm,axis=1).reshape(-1,1)\n",
    "    \n",
    "    return dW,dM,dT,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "################################\n",
    "# Training parameters\n",
    "num_classes = 3\n",
    "epochs = 600\n",
    "learning_rate=0.0001\n",
    "batch_size = 10\n",
    "alpha = 0.9\n",
    "################################\n",
    "\n",
    "#MLP PARAMETERS\n",
    "noHiddenLayers=2\n",
    "\n",
    "#also includes the input vector dimension and output vector dimension\n",
    "sizeOfLayers=[x_train.shape[0],10,10,num_classes]\n",
    "\n",
    "sizeofOutput=num_classes\n",
    "\n",
    "Actfnvect = ['relu','relu','softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = weights(noHiddenLayers,sizeOfLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0         Loss:254.1806143958583\n",
      "Epoch:1         Loss:225.60720200334669\n",
      "Epoch:2         Loss:199.1778017341242\n",
      "Epoch:3         Loss:180.99145385678588\n",
      "Epoch:4         Loss:168.60863538208898\n",
      "Epoch:5         Loss:159.43546697158013\n",
      "Epoch:6         Loss:152.32405815518612\n",
      "Epoch:7         Loss:146.70684061898706\n",
      "Epoch:8         Loss:141.56895226362371\n",
      "Epoch:9         Loss:136.35049181781608\n",
      "Epoch:10         Loss:131.8917995004891\n",
      "Epoch:11         Loss:127.91283473370167\n",
      "Epoch:12         Loss:124.07604829384536\n",
      "Epoch:13         Loss:120.24037258727454\n",
      "Epoch:14         Loss:116.20574004561178\n",
      "Epoch:15         Loss:111.05236627869064\n",
      "Epoch:16         Loss:105.23640602480168\n",
      "Epoch:17         Loss:100.03327486605606\n",
      "Epoch:18         Loss:96.0908893238185\n",
      "Epoch:19         Loss:92.97600313926928\n",
      "Epoch:20         Loss:90.26667663391463\n",
      "Epoch:21         Loss:87.87737643916007\n",
      "Epoch:22         Loss:85.73456170370008\n",
      "Epoch:23         Loss:83.7621546165463\n",
      "Epoch:24         Loss:81.9365807314465\n",
      "Epoch:25         Loss:80.3100138197242\n",
      "Epoch:26         Loss:78.8645864325746\n",
      "Epoch:27         Loss:77.575824863318\n",
      "Epoch:28         Loss:76.38231795699657\n",
      "Epoch:29         Loss:75.26162105590709\n",
      "Epoch:30         Loss:74.1923819442868\n",
      "Epoch:31         Loss:73.20276850958615\n",
      "Epoch:32         Loss:72.27205813207902\n",
      "Epoch:33         Loss:71.37561244941452\n",
      "Epoch:34         Loss:70.49089430541343\n",
      "Epoch:35         Loss:69.62124074518266\n",
      "Epoch:36         Loss:68.76039582056893\n",
      "Epoch:37         Loss:67.90746258257089\n",
      "Epoch:38         Loss:67.04006518416884\n",
      "Epoch:39         Loss:66.17096869464903\n",
      "Epoch:40         Loss:65.31172986280144\n",
      "Epoch:41         Loss:64.45678231015063\n",
      "Epoch:42         Loss:63.61390291677769\n",
      "Epoch:43         Loss:62.80261848049733\n",
      "Epoch:44         Loss:62.02032277311303\n",
      "Epoch:45         Loss:61.266055173797376\n",
      "Epoch:46         Loss:60.53657879903467\n",
      "Epoch:47         Loss:59.805406017176885\n",
      "Epoch:48         Loss:59.09801239747202\n",
      "Epoch:49         Loss:58.413079539523714\n",
      "Epoch:50         Loss:57.7654074691914\n",
      "Epoch:51         Loss:57.1437622162485\n",
      "Epoch:52         Loss:56.53816103926768\n",
      "Epoch:53         Loss:55.94732701694985\n",
      "Epoch:54         Loss:55.37442194663726\n",
      "Epoch:55         Loss:54.80844694138088\n",
      "Epoch:56         Loss:54.255191756367125\n",
      "Epoch:57         Loss:53.70192148335508\n",
      "Epoch:58         Loss:53.154839592059815\n",
      "Epoch:59         Loss:52.61493976931288\n",
      "Epoch:60         Loss:52.083957665934165\n",
      "Epoch:61         Loss:51.56055640590608\n",
      "Epoch:62         Loss:51.043372452532815\n",
      "Epoch:63         Loss:50.53746701470488\n",
      "Epoch:64         Loss:50.04005480036615\n",
      "Epoch:65         Loss:49.54853704352785\n",
      "Epoch:66         Loss:49.062162451172476\n",
      "Epoch:67         Loss:48.58361783255963\n",
      "Epoch:68         Loss:48.123264264982616\n",
      "Epoch:69         Loss:47.67990319893636\n",
      "Epoch:70         Loss:47.24348907784724\n",
      "Epoch:71         Loss:46.81358065564202\n",
      "Epoch:72         Loss:46.3926717891868\n",
      "Epoch:73         Loss:45.98459592202002\n",
      "Epoch:74         Loss:45.58623401283034\n",
      "Epoch:75         Loss:45.197035243420316\n",
      "Epoch:76         Loss:44.814416251419516\n",
      "Epoch:77         Loss:44.44284408921832\n",
      "Epoch:78         Loss:44.075664233721234\n",
      "Epoch:79         Loss:43.71114925822931\n",
      "Epoch:80         Loss:43.354876435361305\n",
      "Epoch:81         Loss:43.004095608059814\n",
      "Epoch:82         Loss:42.65882290202662\n",
      "Epoch:83         Loss:42.31816096660767\n",
      "Epoch:84         Loss:41.980451463445235\n",
      "Epoch:85         Loss:41.64843550349465\n",
      "Epoch:86         Loss:41.32153518926073\n",
      "Epoch:87         Loss:40.99822812803928\n",
      "Epoch:88         Loss:40.67841330612495\n",
      "Epoch:89         Loss:40.36253406116959\n",
      "Epoch:90         Loss:40.05034443084494\n",
      "Epoch:91         Loss:39.74235876568628\n",
      "Epoch:92         Loss:39.43854377061612\n",
      "Epoch:93         Loss:39.13755257508797\n",
      "Epoch:94         Loss:38.839404652549476\n",
      "Epoch:95         Loss:38.54257412536417\n",
      "Epoch:96         Loss:38.252328011919865\n",
      "Epoch:97         Loss:37.96517242068548\n",
      "Epoch:98         Loss:37.67994394149626\n",
      "Epoch:99         Loss:37.39708078595104\n",
      "Epoch:100         Loss:37.11830795504315\n",
      "Epoch:101         Loss:36.84181679504466\n",
      "Epoch:102         Loss:36.56842376715523\n",
      "Epoch:103         Loss:36.29820233863136\n",
      "Epoch:104         Loss:36.0303582062587\n",
      "Epoch:105         Loss:35.76590727327829\n",
      "Epoch:106         Loss:35.50291714505779\n",
      "Epoch:107         Loss:35.24345404352268\n",
      "Epoch:108         Loss:34.98731998884184\n",
      "Epoch:109         Loss:34.73214347569915\n",
      "Epoch:110         Loss:34.48089078503357\n",
      "Epoch:111         Loss:34.23075373983541\n",
      "Epoch:112         Loss:33.98394646067874\n",
      "Epoch:113         Loss:33.741607604866196\n",
      "Epoch:114         Loss:33.50072663106772\n",
      "Epoch:115         Loss:33.262056029691216\n",
      "Epoch:116         Loss:33.02567209249475\n",
      "Epoch:117         Loss:32.79153046807936\n",
      "Epoch:118         Loss:32.559666647302805\n",
      "Epoch:119         Loss:32.33022478523342\n",
      "Epoch:120         Loss:32.10248807598275\n",
      "Epoch:121         Loss:31.878087484779734\n",
      "Epoch:122         Loss:31.655320694423818\n",
      "Epoch:123         Loss:31.434603967598044\n",
      "Epoch:124         Loss:31.21698986246555\n",
      "Epoch:125         Loss:31.00050791382553\n",
      "Epoch:126         Loss:30.78574442064141\n",
      "Epoch:127         Loss:30.575164612846983\n",
      "Epoch:128         Loss:30.364521094961397\n",
      "Epoch:129         Loss:30.156917612143914\n",
      "Epoch:130         Loss:29.95129015687049\n",
      "Epoch:131         Loss:29.74694184001584\n",
      "Epoch:132         Loss:29.545623595378654\n",
      "Epoch:133         Loss:29.344660172908302\n",
      "Epoch:134         Loss:29.147211710917755\n",
      "Epoch:135         Loss:28.950307059118785\n",
      "Epoch:136         Loss:28.756705515753588\n",
      "Epoch:137         Loss:28.56333360647058\n",
      "Epoch:138         Loss:28.373465117034357\n",
      "Epoch:139         Loss:28.1843015309926\n",
      "Epoch:140         Loss:27.996356413805177\n",
      "Epoch:141         Loss:27.81306512521366\n",
      "Epoch:142         Loss:27.62838919966229\n",
      "Epoch:143         Loss:27.44605939552607\n",
      "Epoch:144         Loss:27.266372061108505\n",
      "Epoch:145         Loss:27.088082634344104\n",
      "Epoch:146         Loss:26.911248978390834\n",
      "Epoch:147         Loss:26.735556193472924\n",
      "Epoch:148         Loss:26.563120080904543\n",
      "Epoch:149         Loss:26.38933368338237\n",
      "Epoch:150         Loss:26.219750338467385\n",
      "Epoch:151         Loss:26.050516452256144\n",
      "Epoch:152         Loss:25.883272649336345\n",
      "Epoch:153         Loss:25.71662732815942\n",
      "Epoch:154         Loss:25.55304715868942\n",
      "Epoch:155         Loss:25.390009615223683\n",
      "Epoch:156         Loss:25.227993512769352\n",
      "Epoch:157         Loss:25.068502820757793\n",
      "Epoch:158         Loss:24.909959837601495\n",
      "Epoch:159         Loss:24.752689476288385\n",
      "Epoch:160         Loss:24.597412221712297\n",
      "Epoch:161         Loss:24.443392945586808\n",
      "Epoch:162         Loss:24.29074478040546\n",
      "Epoch:163         Loss:24.140260369141465\n",
      "Epoch:164         Loss:23.99080383540535\n",
      "Epoch:165         Loss:23.84232342529887\n",
      "Epoch:166         Loss:23.696517948796625\n",
      "Epoch:167         Loss:23.55178058039413\n",
      "Epoch:168         Loss:23.407680288838332\n",
      "Epoch:169         Loss:23.26678395192558\n",
      "Epoch:170         Loss:23.127158835476198\n",
      "Epoch:171         Loss:22.988541735278027\n",
      "Epoch:172         Loss:22.852032781649026\n",
      "Epoch:173         Loss:22.717062561086465\n",
      "Epoch:174         Loss:22.583444710119615\n",
      "Epoch:175         Loss:22.452710878189837\n",
      "Epoch:176         Loss:22.32202148615137\n",
      "Epoch:177         Loss:22.193009501800603\n",
      "Epoch:178         Loss:22.06610416055285\n",
      "Epoch:179         Loss:21.940474739298562\n",
      "Epoch:180         Loss:21.816158142162664\n",
      "Epoch:181         Loss:21.693267681594385\n",
      "Epoch:182         Loss:21.571700255692985\n",
      "Epoch:183         Loss:21.45163036505223\n",
      "Epoch:184         Loss:21.3326919819625\n",
      "Epoch:185         Loss:21.215224057750962\n",
      "Epoch:186         Loss:21.098936640456845\n",
      "Epoch:187         Loss:20.984061156267625\n",
      "Epoch:188         Loss:20.871407150641655\n",
      "Epoch:189         Loss:20.758627045809785\n",
      "Epoch:190         Loss:20.64749710320663\n",
      "Epoch:191         Loss:20.53805028107157\n",
      "Epoch:192         Loss:20.42898750669361\n",
      "Epoch:193         Loss:20.32264560137236\n",
      "Epoch:194         Loss:20.217065785330615\n",
      "Epoch:195         Loss:20.112334106828623\n",
      "Epoch:196         Loss:20.008706214320846\n",
      "Epoch:197         Loss:19.90652729486725\n",
      "Epoch:198         Loss:19.805562892553425\n",
      "Epoch:199         Loss:19.70529626167721\n",
      "Epoch:200         Loss:19.606371609217135\n",
      "Epoch:201         Loss:19.508563972008282\n",
      "Epoch:202         Loss:19.411969134766395\n",
      "Epoch:203         Loss:19.316242681226388\n",
      "Epoch:204         Loss:19.22060934972915\n",
      "Epoch:205         Loss:19.1288142561366\n",
      "Epoch:206         Loss:19.037930443634313\n",
      "Epoch:207         Loss:18.947960931870536\n",
      "Epoch:208         Loss:18.858772415481702\n",
      "Epoch:209         Loss:18.770380917963866\n",
      "Epoch:210         Loss:18.683093197017524\n",
      "Epoch:211         Loss:18.59660758108175\n",
      "Epoch:212         Loss:18.51137808445016\n",
      "Epoch:213         Loss:18.426800613027215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:214         Loss:18.343230433424065\n",
      "Epoch:215         Loss:18.26090228682715\n",
      "Epoch:216         Loss:18.179255344920332\n",
      "Epoch:217         Loss:18.098427481761878\n",
      "Epoch:218         Loss:18.017967893726087\n",
      "Epoch:219         Loss:17.939147137497905\n",
      "Epoch:220         Loss:17.860963166132017\n",
      "Epoch:221         Loss:17.78353930271416\n",
      "Epoch:222         Loss:17.706954782223477\n",
      "Epoch:223         Loss:17.63080522456825\n",
      "Epoch:224         Loss:17.556054701437727\n",
      "Epoch:225         Loss:17.482029546071473\n",
      "Epoch:226         Loss:17.408237238986693\n",
      "Epoch:227         Loss:17.335791207528192\n",
      "Epoch:228         Loss:17.264074332903167\n",
      "Epoch:229         Loss:17.192168767231717\n",
      "Epoch:230         Loss:17.12278540582002\n",
      "Epoch:231         Loss:17.0541390601464\n",
      "Epoch:232         Loss:16.98539066734905\n",
      "Epoch:233         Loss:16.917756178583844\n",
      "Epoch:234         Loss:16.851279043029137\n",
      "Epoch:235         Loss:16.784764098926196\n",
      "Epoch:236         Loss:16.719074382552076\n",
      "Epoch:237         Loss:16.654132375941032\n",
      "Epoch:238         Loss:16.590037037223354\n",
      "Epoch:239         Loss:16.52632807907328\n",
      "Epoch:240         Loss:16.463347058423473\n",
      "Epoch:241         Loss:16.40098369618311\n",
      "Epoch:242         Loss:16.33938158210091\n",
      "Epoch:243         Loss:16.27824548132667\n",
      "Epoch:244         Loss:16.21784495857286\n",
      "Epoch:245         Loss:16.157914350213268\n",
      "Epoch:246         Loss:16.09872728847772\n",
      "Epoch:247         Loss:16.03984558281155\n",
      "Epoch:248         Loss:15.98200585268656\n",
      "Epoch:249         Loss:15.924325901798825\n",
      "Epoch:250         Loss:15.86726902091988\n",
      "Epoch:251         Loss:15.811117530398244\n",
      "Epoch:252         Loss:15.755313717905942\n",
      "Epoch:253         Loss:15.700087974383985\n",
      "Epoch:254         Loss:15.645488915726016\n",
      "Epoch:255         Loss:15.591320912726806\n",
      "Epoch:256         Loss:15.537617668599582\n",
      "Epoch:257         Loss:15.484828844517654\n",
      "Epoch:258         Loss:15.432255544273355\n",
      "Epoch:259         Loss:15.379930759751181\n",
      "Epoch:260         Loss:15.328321572635607\n",
      "Epoch:261         Loss:15.277319396676027\n",
      "Epoch:262         Loss:15.226714570850948\n",
      "Epoch:263         Loss:15.176905070381558\n",
      "Epoch:264         Loss:15.127268414676337\n",
      "Epoch:265         Loss:15.077892239915977\n",
      "Epoch:266         Loss:15.029191935525395\n",
      "Epoch:267         Loss:14.981049346205518\n",
      "Epoch:268         Loss:14.933266746927357\n",
      "Epoch:269         Loss:14.886248679887423\n",
      "Epoch:270         Loss:14.839359744059985\n",
      "Epoch:271         Loss:14.7927235657423\n",
      "Epoch:272         Loss:14.746723068262686\n",
      "Epoch:273         Loss:14.701238850372173\n",
      "Epoch:274         Loss:14.656137292071199\n",
      "Epoch:275         Loss:14.611425928770952\n",
      "Epoch:276         Loss:14.56713722274551\n",
      "Epoch:277         Loss:14.52326279291232\n",
      "Epoch:278         Loss:14.479827584147586\n",
      "Epoch:279         Loss:14.436785162748443\n",
      "Epoch:280         Loss:14.394074047465779\n",
      "Epoch:281         Loss:14.352528933097725\n",
      "Epoch:282         Loss:14.310307146548203\n",
      "Epoch:283         Loss:14.268785306567741\n",
      "Epoch:284         Loss:14.227909273856786\n",
      "Epoch:285         Loss:14.187238020276494\n",
      "Epoch:286         Loss:14.14683250847155\n",
      "Epoch:287         Loss:14.106822697774408\n",
      "Epoch:288         Loss:14.06720609760743\n",
      "Epoch:289         Loss:14.027941180617734\n",
      "Epoch:290         Loss:13.989017584428563\n",
      "Epoch:291         Loss:13.950440824131004\n",
      "Epoch:292         Loss:13.912198497321297\n",
      "Epoch:293         Loss:13.874320569123585\n",
      "Epoch:294         Loss:13.83677331829245\n",
      "Epoch:295         Loss:13.79955427146691\n",
      "Epoch:296         Loss:13.762663992945617\n",
      "Epoch:297         Loss:13.72609879418093\n",
      "Epoch:298         Loss:13.689853886849304\n",
      "Epoch:299         Loss:13.653936454086358\n",
      "Epoch:300         Loss:13.618331549630378\n",
      "Epoch:301         Loss:13.583031041546935\n",
      "Epoch:302         Loss:13.548036170979529\n",
      "Epoch:303         Loss:13.513345127143076\n",
      "Epoch:304         Loss:13.478953312768256\n",
      "Epoch:305         Loss:13.444856690878812\n",
      "Epoch:306         Loss:13.41105216749086\n",
      "Epoch:307         Loss:13.377536691041445\n",
      "Epoch:308         Loss:13.344307889182193\n",
      "Epoch:309         Loss:13.31135974798194\n",
      "Epoch:310         Loss:13.278691306446017\n",
      "Epoch:311         Loss:13.24629949232276\n",
      "Epoch:312         Loss:13.214180888709729\n",
      "Epoch:313         Loss:13.182371608366248\n",
      "Epoch:314         Loss:13.150554200317604\n",
      "Epoch:315         Loss:13.119315390815554\n",
      "Epoch:316         Loss:13.08841395433416\n",
      "Epoch:317         Loss:13.05761752492383\n",
      "Epoch:318         Loss:13.027030374856853\n",
      "Epoch:319         Loss:12.996764216138438\n",
      "Epoch:320         Loss:12.966510325303327\n",
      "Epoch:321         Loss:12.936808851656782\n",
      "Epoch:322         Loss:12.907176965130093\n",
      "Epoch:323         Loss:12.878012229334892\n",
      "Epoch:324         Loss:12.849765309071765\n",
      "Epoch:325         Loss:12.821470954409358\n",
      "Epoch:326         Loss:12.793355418726264\n",
      "Epoch:327         Loss:12.765484669089599\n",
      "Epoch:328         Loss:12.737854827541053\n",
      "Epoch:329         Loss:12.71044464073383\n",
      "Epoch:330         Loss:12.683256824109002\n",
      "Epoch:331         Loss:12.656287991132938\n",
      "Epoch:332         Loss:12.62952325836158\n",
      "Epoch:333         Loss:12.602964645403768\n",
      "Epoch:334         Loss:12.576614697369127\n",
      "Epoch:335         Loss:12.550471130508397\n",
      "Epoch:336         Loss:12.524538997423793\n",
      "Epoch:337         Loss:12.498805411180163\n",
      "Epoch:338         Loss:12.473266457220541\n",
      "Epoch:339         Loss:12.447924362491705\n",
      "Epoch:340         Loss:12.42276767309518\n",
      "Epoch:341         Loss:12.397825101251351\n",
      "Epoch:342         Loss:12.37303204213259\n",
      "Epoch:343         Loss:12.348486408772075\n",
      "Epoch:344         Loss:12.324112269057702\n",
      "Epoch:345         Loss:12.299914470683499\n",
      "Epoch:346         Loss:12.27590168391289\n",
      "Epoch:347         Loss:12.252072271356663\n",
      "Epoch:348         Loss:12.228421601075812\n",
      "Epoch:349         Loss:12.204947406801201\n",
      "Epoch:350         Loss:12.18164091921801\n",
      "Epoch:351         Loss:12.158525814755528\n",
      "Epoch:352         Loss:12.135577441309275\n",
      "Epoch:353         Loss:12.112797354095305\n",
      "Epoch:354         Loss:12.09018707144466\n",
      "Epoch:355         Loss:12.067744367113912\n",
      "Epoch:356         Loss:12.045466553158422\n",
      "Epoch:357         Loss:12.023351975219214\n",
      "Epoch:358         Loss:12.001399302338154\n",
      "Epoch:359         Loss:11.979606986961647\n",
      "Epoch:360         Loss:11.957973378305883\n",
      "Epoch:361         Loss:11.936496891935695\n",
      "Epoch:362         Loss:11.915176004133903\n",
      "Epoch:363         Loss:11.89400920281791\n",
      "Epoch:364         Loss:11.872994981075967\n",
      "Epoch:365         Loss:11.852131849719665\n",
      "Epoch:366         Loss:11.831418340820084\n",
      "Epoch:367         Loss:11.810853004453394\n",
      "Epoch:368         Loss:11.790434406921149\n",
      "Epoch:369         Loss:11.770161131124805\n",
      "Epoch:370         Loss:11.750031776869415\n",
      "Epoch:371         Loss:11.730044960551503\n",
      "Epoch:372         Loss:11.710199314773213\n",
      "Epoch:373         Loss:11.690493488122229\n",
      "Epoch:374         Loss:11.670926145002337\n",
      "Epoch:375         Loss:11.651495965429095\n",
      "Epoch:376         Loss:11.63220164480884\n",
      "Epoch:377         Loss:11.613041893728639\n",
      "Epoch:378         Loss:11.594015437755854\n",
      "Epoch:379         Loss:11.575121017239764\n",
      "Epoch:380         Loss:11.556357387113868\n",
      "Epoch:381         Loss:11.537723316701157\n",
      "Epoch:382         Loss:11.519217589522727\n",
      "Epoch:383         Loss:11.500898188055237\n",
      "Epoch:384         Loss:11.48270267593666\n",
      "Epoch:385         Loss:11.464579339897561\n",
      "Epoch:386         Loss:11.44657606542608\n",
      "Epoch:387         Loss:11.428709439826806\n",
      "Epoch:388         Loss:11.410967308401876\n",
      "Epoch:389         Loss:11.39334177519584\n",
      "Epoch:390         Loss:11.375833752726678\n",
      "Epoch:391         Loss:11.358444380057174\n",
      "Epoch:392         Loss:11.341172357369134\n",
      "Epoch:393         Loss:11.32401594337771\n",
      "Epoch:394         Loss:11.306974028617196\n",
      "Epoch:395         Loss:11.290045743585537\n",
      "Epoch:396         Loss:11.273230092361114\n",
      "Epoch:397         Loss:11.25652600419743\n",
      "Epoch:398         Loss:11.239932445146001\n",
      "Epoch:399         Loss:11.22368344272423\n",
      "Epoch:400         Loss:11.207513070672787\n",
      "Epoch:401         Loss:11.19128397923803\n",
      "Epoch:402         Loss:11.175156371150145\n",
      "Epoch:403         Loss:11.159181179115883\n",
      "Epoch:404         Loss:11.14349598778335\n",
      "Epoch:405         Loss:11.127820483836677\n",
      "Epoch:406         Loss:11.112129206171279\n",
      "Epoch:407         Loss:11.096546025137899\n",
      "Epoch:408         Loss:11.08109625013647\n",
      "Epoch:409         Loss:11.065748677146553\n",
      "Epoch:410         Loss:11.050324162485104\n",
      "Epoch:411         Loss:11.035250623195232\n",
      "Epoch:412         Loss:11.02029339272237\n",
      "Epoch:413         Loss:11.005327589719926\n",
      "Epoch:414         Loss:10.990427499720163\n",
      "Epoch:415         Loss:10.975644513551522\n",
      "Epoch:416         Loss:10.96096664943205\n",
      "Epoch:417         Loss:10.94637647449956\n",
      "Epoch:418         Loss:10.931873584337353\n",
      "Epoch:419         Loss:10.917461945086158\n",
      "Epoch:420         Loss:10.90314143802528\n",
      "Epoch:421         Loss:10.888910049451408\n",
      "Epoch:422         Loss:10.87476661875886\n",
      "Epoch:423         Loss:10.860710670680835\n",
      "Epoch:424         Loss:10.846741610813185\n",
      "Epoch:425         Loss:10.832858640613285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:426         Loss:10.81906097068854\n",
      "Epoch:427         Loss:10.805347879790807\n",
      "Epoch:428         Loss:10.791718664120998\n",
      "Epoch:429         Loss:10.778172612191957\n",
      "Epoch:430         Loss:10.76470901490458\n",
      "Epoch:431         Loss:10.751327174201592\n",
      "Epoch:432         Loss:10.738026401535748\n",
      "Epoch:433         Loss:10.724806015062907\n",
      "Epoch:434         Loss:10.711665339505537\n",
      "Epoch:435         Loss:10.698603706819096\n",
      "Epoch:436         Loss:10.685628267598359\n",
      "Epoch:437         Loss:10.672925733182028\n",
      "Epoch:438         Loss:10.660196538980644\n",
      "Epoch:439         Loss:10.6474487064079\n",
      "Epoch:440         Loss:10.634793507311167\n",
      "Epoch:441         Loss:10.622243373400172\n",
      "Epoch:442         Loss:10.609767456099993\n",
      "Epoch:443         Loss:10.597356222404423\n",
      "Epoch:444         Loss:10.585016074534973\n",
      "Epoch:445         Loss:10.572750093638673\n",
      "Epoch:446         Loss:10.560556321973268\n",
      "Epoch:447         Loss:10.548432915209006\n",
      "Epoch:448         Loss:10.536379479747891\n",
      "Epoch:449         Loss:10.524395837942857\n",
      "Epoch:450         Loss:10.512481444438505\n",
      "Epoch:451         Loss:10.500635638088712\n",
      "Epoch:452         Loss:10.488857848738721\n",
      "Epoch:453         Loss:10.47714756032036\n",
      "Epoch:454         Loss:10.465504246176435\n",
      "Epoch:455         Loss:10.453927368653167\n",
      "Epoch:456         Loss:10.442416397240905\n",
      "Epoch:457         Loss:10.430970811636572\n",
      "Epoch:458         Loss:10.419590097052646\n",
      "Epoch:459         Loss:10.408273742498698\n",
      "Epoch:460         Loss:10.397021241750856\n",
      "Epoch:461         Loss:10.385832093930347\n",
      "Epoch:462         Loss:10.374705803257875\n",
      "Epoch:463         Loss:10.363641878782293\n",
      "Epoch:464         Loss:10.352639834338857\n",
      "Epoch:465         Loss:10.341699188554236\n",
      "Epoch:466         Loss:10.330819464795182\n",
      "Epoch:467         Loss:10.320000191095797\n",
      "Epoch:468         Loss:10.309267949943525\n",
      "Epoch:469         Loss:10.298582668019492\n",
      "Epoch:470         Loss:10.287942932165251\n",
      "Epoch:471         Loss:10.277363852965157\n",
      "Epoch:472         Loss:10.266847189520423\n",
      "Epoch:473         Loss:10.2563883242765\n",
      "Epoch:474         Loss:10.245985454087206\n",
      "Epoch:475         Loss:10.23563908348487\n",
      "Epoch:476         Loss:10.22534932092184\n",
      "Epoch:477         Loss:10.215115562943938\n",
      "Epoch:478         Loss:10.205077073920675\n",
      "Epoch:479         Loss:10.195017933742562\n",
      "Epoch:480         Loss:10.18494348177018\n",
      "Epoch:481         Loss:10.174942118320585\n",
      "Epoch:482         Loss:10.165076567818891\n",
      "Epoch:483         Loss:10.155192528948275\n",
      "Epoch:484         Loss:10.14535617310576\n",
      "Epoch:485         Loss:10.135589097536121\n",
      "Epoch:486         Loss:10.1258780086381\n",
      "Epoch:487         Loss:10.116214163909595\n",
      "Epoch:488         Loss:10.106599426227266\n",
      "Epoch:489         Loss:10.097036166151726\n",
      "Epoch:490         Loss:10.087525480293731\n",
      "Epoch:491         Loss:10.07806260128033\n",
      "Epoch:492         Loss:10.068657245437079\n",
      "Epoch:493         Loss:10.059304133711167\n",
      "Epoch:494         Loss:10.049999051920587\n",
      "Epoch:495         Loss:10.04074143772243\n",
      "Epoch:496         Loss:10.031531884062906\n",
      "Epoch:497         Loss:10.022370231000012\n",
      "Epoch:498         Loss:10.013255890160494\n",
      "Epoch:499         Loss:10.004188423151398\n",
      "Epoch:500         Loss:9.995167543315052\n",
      "Epoch:501         Loss:9.98619295122142\n",
      "Epoch:502         Loss:9.977264305926104\n",
      "Epoch:503         Loss:9.968381266488484\n",
      "Epoch:504         Loss:9.959543507074175\n",
      "Epoch:505         Loss:9.950750707731078\n",
      "Epoch:506         Loss:9.942002548515724\n",
      "Epoch:507         Loss:9.9332987110683\n",
      "Epoch:508         Loss:9.924638880504677\n",
      "Epoch:509         Loss:9.916022745256488\n",
      "Epoch:510         Loss:9.90744999647404\n",
      "Epoch:511         Loss:9.898920327935524\n",
      "Epoch:512         Loss:9.890433436166436\n",
      "Epoch:513         Loss:9.881989020451913\n",
      "Epoch:514         Loss:9.873586782771872\n",
      "Epoch:515         Loss:9.865226427753802\n",
      "Epoch:516         Loss:9.856907662650766\n",
      "Epoch:517         Loss:9.848630197319164\n",
      "Epoch:518         Loss:9.84039374418976\n",
      "Epoch:519         Loss:9.832198018237952\n",
      "Epoch:520         Loss:9.824042736956374\n",
      "Epoch:521         Loss:9.815927620328326\n",
      "Epoch:522         Loss:9.807852390801187\n",
      "Epoch:523         Loss:9.799816773260014\n",
      "Epoch:524         Loss:9.791820495001396\n",
      "Epoch:525         Loss:9.783974801314177\n",
      "Epoch:526         Loss:9.77617724545532\n",
      "Epoch:527         Loss:9.768284055280708\n",
      "Epoch:528         Loss:9.760446977836697\n",
      "Epoch:529         Loss:9.752689554173598\n",
      "Epoch:530         Loss:9.744965600909168\n",
      "Epoch:531         Loss:9.737268613099667\n",
      "Epoch:532         Loss:9.729607869251723\n",
      "Epoch:533         Loss:9.721986477020542\n",
      "Epoch:534         Loss:9.714402057292732\n",
      "Epoch:535         Loss:9.706853049198815\n",
      "Epoch:536         Loss:9.699339588356773\n",
      "Epoch:537         Loss:9.691861872815599\n",
      "Epoch:538         Loss:9.684419633961362\n",
      "Epoch:539         Loss:9.677012507816434\n",
      "Epoch:540         Loss:9.669640248256062\n",
      "Epoch:541         Loss:9.662302658527615\n",
      "Epoch:542         Loss:9.654999519182693\n",
      "Epoch:543         Loss:9.647730595061018\n",
      "Epoch:544         Loss:9.640495656847724\n",
      "Epoch:545         Loss:9.633294482775002\n",
      "Epoch:546         Loss:9.626126852796116\n",
      "Epoch:547         Loss:9.618992547083305\n",
      "Epoch:548         Loss:9.611891347396842\n",
      "Epoch:549         Loss:9.604824280170922\n",
      "Epoch:550         Loss:9.597791299857832\n",
      "Epoch:551         Loss:9.590790618147228\n",
      "Epoch:552         Loss:9.583822198420187\n",
      "Epoch:553         Loss:9.576885874152296\n",
      "Epoch:554         Loss:9.569981371502115\n",
      "Epoch:555         Loss:9.56310845495046\n",
      "Epoch:556         Loss:9.55626693276661\n",
      "Epoch:557         Loss:9.549456613129097\n",
      "Epoch:558         Loss:9.54267729402902\n",
      "Epoch:559         Loss:9.535928773654405\n",
      "Epoch:560         Loss:9.52921085493896\n",
      "Epoch:561         Loss:9.522523343369178\n",
      "Epoch:562         Loss:9.51586604529781\n",
      "Epoch:563         Loss:9.509240094017022\n",
      "Epoch:564         Loss:9.502644831619271\n",
      "Epoch:565         Loss:9.496079035077369\n",
      "Epoch:566         Loss:9.489542730045034\n",
      "Epoch:567         Loss:9.483035734253503\n",
      "Epoch:568         Loss:9.4765577962151\n",
      "Epoch:569         Loss:9.470108718431407\n",
      "Epoch:570         Loss:9.463690503200915\n",
      "Epoch:571         Loss:9.457435160723387\n",
      "Epoch:572         Loss:9.451105691640814\n",
      "Epoch:573         Loss:9.444751206512388\n",
      "Epoch:574         Loss:9.438407801554002\n",
      "Epoch:575         Loss:9.432168944486577\n",
      "Epoch:576         Loss:9.425934106735449\n",
      "Epoch:577         Loss:9.419717948521583\n",
      "Epoch:578         Loss:9.413534200792158\n",
      "Epoch:579         Loss:9.407380765405021\n",
      "Epoch:580         Loss:9.401253175226646\n",
      "Epoch:581         Loss:9.395119507998908\n",
      "Epoch:582         Loss:9.389063938797184\n",
      "Epoch:583         Loss:9.383014121513565\n",
      "Epoch:584         Loss:9.376986483530034\n",
      "Epoch:585         Loss:9.370990050795095\n",
      "Epoch:586         Loss:9.365021445522915\n",
      "Epoch:587         Loss:9.35907732547606\n",
      "Epoch:588         Loss:9.353157885809134\n",
      "Epoch:589         Loss:9.347263931685035\n",
      "Epoch:590         Loss:9.341395376465742\n",
      "Epoch:591         Loss:9.33555180857549\n",
      "Epoch:592         Loss:9.329733013920967\n",
      "Epoch:593         Loss:9.323938905815925\n",
      "Epoch:594         Loss:9.318169365058518\n",
      "Epoch:595         Loss:9.312424232057396\n",
      "Epoch:596         Loss:9.306703350685131\n",
      "Epoch:597         Loss:9.301006578080582\n",
      "Epoch:598         Loss:9.29533377392951\n",
      "Epoch:599         Loss:9.289684748210775\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# Momentum descent\n",
    "\n",
    "# updating the weights\n",
    "\n",
    "\n",
    "v = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss=0\n",
    "    for j in np.arange(0,x_train.shape[1],batch_size):       \n",
    "        dW = np.zeros(W[0].shape)\n",
    "        dM = np.zeros(W[1].shape)\n",
    "        dT = np.zeros(W[2].shape)\n",
    "        db1 = np.zeros(b[0].shape)\n",
    "        db2 = np.zeros(b[1].shape)\n",
    "        db3 = np.zeros(b[2].shape)\n",
    "        \n",
    "        #Compute interim update\n",
    "        W[0] = W[0] + alpha*v[\"W[0]\"]\n",
    "        W[1] = W[1] + alpha*v[\"W[1]\"]\n",
    "        W[2] = W[2] + alpha*v[\"W[2]\"]\n",
    "        b[0] = b[0] + alpha*v[\"b[0]\"]\n",
    "        b[1] = b[1] + alpha*v[\"b[1]\"]\n",
    "        b[2] = b[2] + alpha*v[\"b[2]\"]\n",
    "        \n",
    "        for k in range(0,batch_size):\n",
    "            # forward pass\n",
    "            x = x_train[:,j+k].reshape(-1,1)\n",
    "            y = y_train[:,j+k].reshape(-1,1)\n",
    "            out,y_pred=forward_path(noHiddenLayers,x,W,b,Actfnvect)\n",
    "            # backpropagation\n",
    "            dWtemp,dMtemp,dTtemp,db1temp,db2temp,db3temp=backprop(y_pred,y,out[1],W[-1],out[0],W[1],x)\n",
    "            \n",
    "            dW=dW+dWtemp\n",
    "            db1=db1+db1temp\n",
    "            \n",
    "            dM=dM+dMtemp\n",
    "            db2=db2+db2temp\n",
    "            \n",
    "            dT=dT+dTtemp\n",
    "            db3=db3+db3temp\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = loss + (-1.0*np.sum(y*np.log(y_pred)))\n",
    "            \n",
    "        # Updating the weights using Nesterov MOMENTUM Approach\n",
    "        \n",
    "        dW=dW*(1.0/batch_size)\n",
    "        dM=dM*(1.0/batch_size)\n",
    "        dT=dT*(1.0/batch_size)\n",
    "        db1=db1*(1.0/batch_size)\n",
    "        db2=db2*(1.0/batch_size)\n",
    "        db3=db3*(1.0/batch_size)\n",
    "    \n",
    "        #Compute velocity update\n",
    "        v[\"W[0]\"] = alpha*v[\"W[0]\"] - learning_rate*(dW)\n",
    "        v[\"W[1]\"] = alpha*v[\"W[1]\"] - learning_rate*(dM)\n",
    "        v[\"W[2]\"] = alpha*v[\"W[2]\"] - learning_rate*(dT)\n",
    "        v[\"b[0]\"] = alpha*v[\"b[0]\"] - learning_rate*(db1)\n",
    "        v[\"b[1]\"] = alpha*v[\"b[1]\"] - learning_rate*(db2)\n",
    "        v[\"b[2]\"] = alpha*v[\"b[2]\"] - learning_rate*(db3)\n",
    "        \n",
    "        #Apply update\n",
    "        W[0] = W[0] + v[\"W[0]\"]\n",
    "        W[1] = W[1] + v[\"W[1]\"]\n",
    "        W[2] = W[2] + v[\"W[2]\"]\n",
    "        b[0] = b[0] + v[\"b[0]\"]\n",
    "        b[1] = b[1] + v[\"b[1]\"]\n",
    "        b[2] = b[2] + v[\"b[2]\"]\n",
    "        \n",
    "    \n",
    "    #print the loss in each epoch\n",
    "    print('Epoch:'+str(i)+'         Loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,y_pred=forward_path(noHiddenLayers,x_test,W,b,Actfnvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(y):\n",
    "    return np.argmax(y)\n",
    "\n",
    "yvect=[]\n",
    "y_trurevect=[]\n",
    "\n",
    "for i in range(0,x_test.shape[1]):\n",
    "    yvect.append(predict(y_pred[:,i]))\n",
    "    y_trurevect.append(predict(y_test[:,i]))\n",
    "\n",
    "# find accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "#predicting test accuracy\n",
    "print(accuracy_score(y_trurevect, yvect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# to see the output vs true values\n",
    "\n",
    "print y_trurevect\n",
    "print yvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
