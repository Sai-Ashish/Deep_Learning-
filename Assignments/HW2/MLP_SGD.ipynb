{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Note:run in python2######\n",
    "# same initialization of weights because same random seed used\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#MLP\n",
    "\n",
    "# initialisation of the weights he_normal\n",
    "def weights(noHiddenLayers,sizeOfLayers):\n",
    "\n",
    "    W=[]\n",
    "    b=[]\n",
    "\n",
    "    for i in range(0,noHiddenLayers+1):\n",
    "        \n",
    "        W.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+sizeOfLayers[i])),\n",
    "                                   (sizeOfLayers[i+1],sizeOfLayers[i])) )\n",
    "        b.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+1)),\n",
    "                                   (sizeOfLayers[i+1],1)) )\n",
    "\n",
    "    W=np.array(W)\n",
    "    b=np.array(b)\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "#mlp forward pass\n",
    "#layer\n",
    "def layer(w,x,b):\n",
    "    out = np.matmul(w,x)+b\n",
    "    return out\n",
    "\n",
    "def apply_activationMLP(Activation_function,inp):\n",
    "    \n",
    "    #activation functions\n",
    "    if Activation_function == 'relu':\n",
    "        return np.where(inp<0,0,inp)\n",
    "    elif Activation_function == \"tanh\":\n",
    "        return np.tanh(inp)\n",
    "    elif Activation_function == \"sigmoid\":\n",
    "        return 1.0/(1+np.exp(-1.0*inp))\n",
    "    elif Activation_function == \"softmax\":\n",
    "        return (1.0/(np.sum(np.exp(inp),axis=0)))*(np.exp(inp))\n",
    "\n",
    "#forward path\n",
    "def forward_path(noHiddenLayers,X,W,b,Actfnvect):\n",
    "\n",
    "    out=[]\n",
    "    \n",
    "    z=apply_activationMLP(Actfnvect[0],np.array(layer(W[0],X,b[0])))\n",
    "    out.append(np.array(z))\n",
    "\n",
    "    for i in range(1,noHiddenLayers):\n",
    "        z=apply_activationMLP(Actfnvect[i],np.array(layer(W[i],out[i-1],b[i])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    if noHiddenLayers > 0:\n",
    "        z=apply_activationMLP(Actfnvect[-1],np.array(layer(W[-1],out[-1],b[-1])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    y_pred = out[-1]\n",
    "\n",
    "    return np.array(out),np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "#only to import data\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    " \n",
    "    \n",
    "################################\n",
    "\n",
    "# #import data\n",
    "iris_data = load_iris() # load the iris dataset\n",
    "\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "# Split the data for training and testing\n",
    "x_train , x_test, y_train , y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(3, 120)\n",
      "(4, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "# run MLP algorithm\n",
    "\n",
    "x_train = np.moveaxis(x_train,0,-1)\n",
    "y_train = np.moveaxis(y_train,0,-1)\n",
    "x_test = np.moveaxis(x_test,0,-1)\n",
    "y_test = np.moveaxis(y_test,0,-1)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_relu(x):\n",
    "    return np.where(x == 0,0,1)\n",
    "\n",
    "# backpropation\n",
    "def backprop(y,y_true,z,T,u,M,x):\n",
    "    \n",
    "    dy = (y-y_true)\n",
    "    \n",
    "    # layer 3\n",
    "    dT  = np.matmul(dy,z.T)\n",
    "    db3 = np.sum(dy,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 2\n",
    "    s   = np.matmul(T.T,dy)\n",
    "    s   = s*der_relu(z)\n",
    "    dM  = np.matmul(s,u.T)\n",
    "    db2 = np.sum(s,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 1\n",
    "    sm  = np.matmul(M.T,s)\n",
    "    sm  = sm*der_relu(u)\n",
    "    dW  = np.matmul(sm,x.T)\n",
    "    db1 = np.sum(sm,axis=1).reshape(-1,1)\n",
    "    \n",
    "    return dW,dM,dT,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Training parameters\n",
    "num_classes = 3\n",
    "epochs = 600\n",
    "learning_rate=1e-4\n",
    "batch_size = 10\n",
    "\n",
    "################################\n",
    "#MLP PARAMETERS\n",
    "noHiddenLayers=2\n",
    "\n",
    "#also includes the input vector dimension and output vector dimension\n",
    "sizeOfLayers=[x_train.shape[0],10,10,num_classes]\n",
    "\n",
    "sizeofOutput=num_classes\n",
    "\n",
    "Actfnvect = ['relu','relu','softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = weights(noHiddenLayers,sizeOfLayers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0         Loss:260.7953073984197\n",
      "Epoch:1         Loss:258.01363975313905\n",
      "Epoch:2         Loss:255.32596057898502\n",
      "Epoch:3         Loss:252.7275082881799\n",
      "Epoch:4         Loss:250.2248580726914\n",
      "Epoch:5         Loss:247.8281201790449\n",
      "Epoch:6         Loss:245.5305658303528\n",
      "Epoch:7         Loss:243.30297692661122\n",
      "Epoch:8         Loss:241.1411458462219\n",
      "Epoch:9         Loss:239.04453378140147\n",
      "Epoch:10         Loss:237.01014632191823\n",
      "Epoch:11         Loss:235.0351740009979\n",
      "Epoch:12         Loss:233.11539841110127\n",
      "Epoch:13         Loss:231.2430049278045\n",
      "Epoch:14         Loss:229.41765984135694\n",
      "Epoch:15         Loss:227.63360828136518\n",
      "Epoch:16         Loss:225.8859418455903\n",
      "Epoch:17         Loss:224.18570758347084\n",
      "Epoch:18         Loss:222.52843521638817\n",
      "Epoch:19         Loss:220.9181021023612\n",
      "Epoch:20         Loss:219.3500532355199\n",
      "Epoch:21         Loss:217.82379163969904\n",
      "Epoch:22         Loss:216.3349153489726\n",
      "Epoch:23         Loss:214.8822412924554\n",
      "Epoch:24         Loss:213.46454141728006\n",
      "Epoch:25         Loss:212.07940266891342\n",
      "Epoch:26         Loss:210.72528947987246\n",
      "Epoch:27         Loss:209.40269923846893\n",
      "Epoch:28         Loss:208.11128981748584\n",
      "Epoch:29         Loss:206.85323903044142\n",
      "Epoch:30         Loss:205.6246902622411\n",
      "Epoch:31         Loss:204.4246209879346\n",
      "Epoch:32         Loss:203.25139260148663\n",
      "Epoch:33         Loss:202.10359043520108\n",
      "Epoch:34         Loss:200.97916701280417\n",
      "Epoch:35         Loss:199.87724341196403\n",
      "Epoch:36         Loss:198.80108891968808\n",
      "Epoch:37         Loss:197.75111928134157\n",
      "Epoch:38         Loss:196.72421027296636\n",
      "Epoch:39         Loss:195.7188456652253\n",
      "Epoch:40         Loss:194.73434816853353\n",
      "Epoch:41         Loss:193.76888346245008\n",
      "Epoch:42         Loss:192.82266175674033\n",
      "Epoch:43         Loss:191.89576958113688\n",
      "Epoch:44         Loss:190.9862875655039\n",
      "Epoch:45         Loss:190.09368386061055\n",
      "Epoch:46         Loss:189.2170050939523\n",
      "Epoch:47         Loss:188.35576788102017\n",
      "Epoch:48         Loss:187.51001731930535\n",
      "Epoch:49         Loss:186.68055899226448\n",
      "Epoch:50         Loss:185.86693045764403\n",
      "Epoch:51         Loss:185.07077557554257\n",
      "Epoch:52         Loss:184.29294012394436\n",
      "Epoch:53         Loss:183.5320089516081\n",
      "Epoch:54         Loss:182.78319027529673\n",
      "Epoch:55         Loss:182.0470321903284\n",
      "Epoch:56         Loss:181.3239624470937\n",
      "Epoch:57         Loss:180.61417838343561\n",
      "Epoch:58         Loss:179.91628582149258\n",
      "Epoch:59         Loss:179.229759135083\n",
      "Epoch:60         Loss:178.55434060006417\n",
      "Epoch:61         Loss:177.88910581321895\n",
      "Epoch:62         Loss:177.23418308578795\n",
      "Epoch:63         Loss:176.58986068417653\n",
      "Epoch:64         Loss:175.95548129944916\n",
      "Epoch:65         Loss:175.3302291453379\n",
      "Epoch:66         Loss:174.7137497554588\n",
      "Epoch:67         Loss:174.10624536718834\n",
      "Epoch:68         Loss:173.50718233748054\n",
      "Epoch:69         Loss:172.91704692676927\n",
      "Epoch:70         Loss:172.33565562130187\n",
      "Epoch:71         Loss:171.7616515975256\n",
      "Epoch:72         Loss:171.19570870774427\n",
      "Epoch:73         Loss:170.63742922185725\n",
      "Epoch:74         Loss:170.08657304457233\n",
      "Epoch:75         Loss:169.54353142248362\n",
      "Epoch:76         Loss:169.00720037944168\n",
      "Epoch:77         Loss:168.477154992812\n",
      "Epoch:78         Loss:167.95298000987097\n",
      "Epoch:79         Loss:167.4343946926999\n",
      "Epoch:80         Loss:166.92204587620486\n",
      "Epoch:81         Loss:166.41454665601938\n",
      "Epoch:82         Loss:165.91234618802795\n",
      "Epoch:83         Loss:165.41577055565918\n",
      "Epoch:84         Loss:164.92574524243773\n",
      "Epoch:85         Loss:164.44146751130603\n",
      "Epoch:86         Loss:163.96263427467514\n",
      "Epoch:87         Loss:163.48848767735555\n",
      "Epoch:88         Loss:163.0195228433713\n",
      "Epoch:89         Loss:162.5559983274035\n",
      "Epoch:90         Loss:162.09693415520132\n",
      "Epoch:91         Loss:161.64239972927896\n",
      "Epoch:92         Loss:161.1926675025612\n",
      "Epoch:93         Loss:160.74806412159782\n",
      "Epoch:94         Loss:160.30799806833042\n",
      "Epoch:95         Loss:159.8722266332417\n",
      "Epoch:96         Loss:159.44058245990226\n",
      "Epoch:97         Loss:159.01226913801744\n",
      "Epoch:98         Loss:158.58716551599233\n",
      "Epoch:99         Loss:158.1655749456227\n",
      "Epoch:100         Loss:157.7478367944593\n",
      "Epoch:101         Loss:157.3338263072398\n",
      "Epoch:102         Loss:156.924831850437\n",
      "Epoch:103         Loss:156.52032048565408\n",
      "Epoch:104         Loss:156.11826321199777\n",
      "Epoch:105         Loss:155.71867144669164\n",
      "Epoch:106         Loss:155.3228903343756\n",
      "Epoch:107         Loss:154.92977083299814\n",
      "Epoch:108         Loss:154.5389507303355\n",
      "Epoch:109         Loss:154.150805277389\n",
      "Epoch:110         Loss:153.76472146730686\n",
      "Epoch:111         Loss:153.38530043730174\n",
      "Epoch:112         Loss:153.01122220916668\n",
      "Epoch:113         Loss:152.63791299529277\n",
      "Epoch:114         Loss:152.26840882525352\n",
      "Epoch:115         Loss:151.90439356209322\n",
      "Epoch:116         Loss:151.54285314339376\n",
      "Epoch:117         Loss:151.1846013812846\n",
      "Epoch:118         Loss:150.8300578198313\n",
      "Epoch:119         Loss:150.48148391032993\n",
      "Epoch:120         Loss:150.13930438477848\n",
      "Epoch:121         Loss:149.79938104226582\n",
      "Epoch:122         Loss:149.45240123853168\n",
      "Epoch:123         Loss:149.10697659216285\n",
      "Epoch:124         Loss:148.76859662537518\n",
      "Epoch:125         Loss:148.42983780156035\n",
      "Epoch:126         Loss:148.08572644653512\n",
      "Epoch:127         Loss:147.73089177266084\n",
      "Epoch:128         Loss:147.38127663351415\n",
      "Epoch:129         Loss:147.03078897432263\n",
      "Epoch:130         Loss:146.66424676279567\n",
      "Epoch:131         Loss:146.27856979799324\n",
      "Epoch:132         Loss:145.88056403891767\n",
      "Epoch:133         Loss:145.48566703994283\n",
      "Epoch:134         Loss:145.1034110488864\n",
      "Epoch:135         Loss:144.73651140977162\n",
      "Epoch:136         Loss:144.3876761237564\n",
      "Epoch:137         Loss:144.04903061122553\n",
      "Epoch:138         Loss:143.71537082110254\n",
      "Epoch:139         Loss:143.38653309047544\n",
      "Epoch:140         Loss:143.06305639861975\n",
      "Epoch:141         Loss:142.74750713720402\n",
      "Epoch:142         Loss:142.444926479559\n",
      "Epoch:143         Loss:142.14856445589592\n",
      "Epoch:144         Loss:141.8579197535821\n",
      "Epoch:145         Loss:141.57168965472823\n",
      "Epoch:146         Loss:141.29142722780537\n",
      "Epoch:147         Loss:141.0142428835302\n",
      "Epoch:148         Loss:140.74000361845003\n",
      "Epoch:149         Loss:140.46846646281054\n",
      "Epoch:150         Loss:140.20065548370118\n",
      "Epoch:151         Loss:139.93594971802773\n",
      "Epoch:152         Loss:139.67352661117673\n",
      "Epoch:153         Loss:139.4133685928454\n",
      "Epoch:154         Loss:139.15448672532008\n",
      "Epoch:155         Loss:138.89878123084705\n",
      "Epoch:156         Loss:138.64606069537155\n",
      "Epoch:157         Loss:138.39668314265856\n",
      "Epoch:158         Loss:138.14997670895053\n",
      "Epoch:159         Loss:137.90484018274896\n",
      "Epoch:160         Loss:137.66092981580928\n",
      "Epoch:161         Loss:137.41820054758182\n",
      "Epoch:162         Loss:137.1762268061873\n",
      "Epoch:163         Loss:136.93520878853082\n",
      "Epoch:164         Loss:136.6952253355748\n",
      "Epoch:165         Loss:136.4560582257157\n",
      "Epoch:166         Loss:136.21769580926895\n",
      "Epoch:167         Loss:135.98036940521862\n",
      "Epoch:168         Loss:135.74396728307886\n",
      "Epoch:169         Loss:135.50814510245138\n",
      "Epoch:170         Loss:135.2731409949148\n",
      "Epoch:171         Loss:135.0390321885856\n",
      "Epoch:172         Loss:134.80615976205027\n",
      "Epoch:173         Loss:134.57442483491525\n",
      "Epoch:174         Loss:134.34337197310398\n",
      "Epoch:175         Loss:134.1131370428255\n",
      "Epoch:176         Loss:133.88348076036027\n",
      "Epoch:177         Loss:133.65424229368438\n",
      "Epoch:178         Loss:133.42572360605163\n",
      "Epoch:179         Loss:133.19786783036093\n",
      "Epoch:180         Loss:132.9705375640291\n",
      "Epoch:181         Loss:132.74396992124764\n",
      "Epoch:182         Loss:132.51810747986104\n",
      "Epoch:183         Loss:132.29268349219225\n",
      "Epoch:184         Loss:132.06773721489992\n",
      "Epoch:185         Loss:131.84350245953647\n",
      "Epoch:186         Loss:131.61976243584215\n",
      "Epoch:187         Loss:131.39617312626746\n",
      "Epoch:188         Loss:131.17245487147653\n",
      "Epoch:189         Loss:130.9493529258026\n",
      "Epoch:190         Loss:130.7267133064753\n",
      "Epoch:191         Loss:130.50417030758496\n",
      "Epoch:192         Loss:130.2816561314891\n",
      "Epoch:193         Loss:130.05940081222232\n",
      "Epoch:194         Loss:129.83590506095632\n",
      "Epoch:195         Loss:129.6129190173514\n",
      "Epoch:196         Loss:129.39064478005807\n",
      "Epoch:197         Loss:129.16900480404217\n",
      "Epoch:198         Loss:128.94797452178463\n",
      "Epoch:199         Loss:128.72664503646095\n",
      "Epoch:200         Loss:128.50395636657092\n",
      "Epoch:201         Loss:128.2811316980382\n",
      "Epoch:202         Loss:128.05924890389107\n",
      "Epoch:203         Loss:127.83923759212126\n",
      "Epoch:204         Loss:127.62003819132144\n",
      "Epoch:205         Loss:127.39986010663006\n",
      "Epoch:206         Loss:127.17888690895711\n",
      "Epoch:207         Loss:126.95835501965433\n",
      "Epoch:208         Loss:126.7379628249004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:209         Loss:126.51824410361986\n",
      "Epoch:210         Loss:126.29953123944081\n",
      "Epoch:211         Loss:126.08133283696182\n",
      "Epoch:212         Loss:125.86362537969391\n",
      "Epoch:213         Loss:125.64603315746723\n",
      "Epoch:214         Loss:125.42774051896332\n",
      "Epoch:215         Loss:125.21014246001947\n",
      "Epoch:216         Loss:124.99286890360418\n",
      "Epoch:217         Loss:124.77591953551581\n",
      "Epoch:218         Loss:124.55954853048536\n",
      "Epoch:219         Loss:124.34319128206346\n",
      "Epoch:220         Loss:124.12484407453371\n",
      "Epoch:221         Loss:123.90658924128913\n",
      "Epoch:222         Loss:123.68907402487355\n",
      "Epoch:223         Loss:123.47226611726921\n",
      "Epoch:224         Loss:123.25567552079393\n",
      "Epoch:225         Loss:123.03790369529\n",
      "Epoch:226         Loss:122.8170744519897\n",
      "Epoch:227         Loss:122.59579038396791\n",
      "Epoch:228         Loss:122.37475857280432\n",
      "Epoch:229         Loss:122.15398237690849\n",
      "Epoch:230         Loss:121.93076376974246\n",
      "Epoch:231         Loss:121.7026160267842\n",
      "Epoch:232         Loss:121.46990823991463\n",
      "Epoch:233         Loss:121.23624160076595\n",
      "Epoch:234         Loss:120.996461473369\n",
      "Epoch:235         Loss:120.75463121813708\n",
      "Epoch:236         Loss:120.50568350828928\n",
      "Epoch:237         Loss:120.2499457522166\n",
      "Epoch:238         Loss:119.98587122323231\n",
      "Epoch:239         Loss:119.7168181312056\n",
      "Epoch:240         Loss:119.44399312677842\n",
      "Epoch:241         Loss:119.15776440139946\n",
      "Epoch:242         Loss:118.85750872963368\n",
      "Epoch:243         Loss:118.54831952758079\n",
      "Epoch:244         Loss:118.22833812034067\n",
      "Epoch:245         Loss:117.90599518183085\n",
      "Epoch:246         Loss:117.58553215411614\n",
      "Epoch:247         Loss:117.25831354180387\n",
      "Epoch:248         Loss:116.92101963886113\n",
      "Epoch:249         Loss:116.58065556399994\n",
      "Epoch:250         Loss:116.23846802788339\n",
      "Epoch:251         Loss:115.8999568643486\n",
      "Epoch:252         Loss:115.55910944092834\n",
      "Epoch:253         Loss:115.21614710092396\n",
      "Epoch:254         Loss:114.87282646027424\n",
      "Epoch:255         Loss:114.52909929339836\n",
      "Epoch:256         Loss:114.18318482176477\n",
      "Epoch:257         Loss:113.83819431018864\n",
      "Epoch:258         Loss:113.49617503873951\n",
      "Epoch:259         Loss:113.1564653419711\n",
      "Epoch:260         Loss:112.81790636546101\n",
      "Epoch:261         Loss:112.48033004321455\n",
      "Epoch:262         Loss:112.14467887059584\n",
      "Epoch:263         Loss:111.80962238067136\n",
      "Epoch:264         Loss:111.48063884453715\n",
      "Epoch:265         Loss:111.15380269671807\n",
      "Epoch:266         Loss:110.82829272361452\n",
      "Epoch:267         Loss:110.50382854965619\n",
      "Epoch:268         Loss:110.18296364846582\n",
      "Epoch:269         Loss:109.86319617160518\n",
      "Epoch:270         Loss:109.54472351439925\n",
      "Epoch:271         Loss:109.22824034554128\n",
      "Epoch:272         Loss:108.91369553055122\n",
      "Epoch:273         Loss:108.59998327657131\n",
      "Epoch:274         Loss:108.28834214965914\n",
      "Epoch:275         Loss:107.97868355656013\n",
      "Epoch:276         Loss:107.67012863645543\n",
      "Epoch:277         Loss:107.36268275180439\n",
      "Epoch:278         Loss:107.05607835181458\n",
      "Epoch:279         Loss:106.75307174104601\n",
      "Epoch:280         Loss:106.45286764280662\n",
      "Epoch:281         Loss:106.15291200792342\n",
      "Epoch:282         Loss:105.85478605865188\n",
      "Epoch:283         Loss:105.55892050029564\n",
      "Epoch:284         Loss:105.26535998544101\n",
      "Epoch:285         Loss:104.97442029558812\n",
      "Epoch:286         Loss:104.68379096153859\n",
      "Epoch:287         Loss:104.39571816387942\n",
      "Epoch:288         Loss:104.11164438860627\n",
      "Epoch:289         Loss:103.83103219891902\n",
      "Epoch:290         Loss:103.55163958887147\n",
      "Epoch:291         Loss:103.27831529774211\n",
      "Epoch:292         Loss:103.01567817642479\n",
      "Epoch:293         Loss:102.75582833234354\n",
      "Epoch:294         Loss:102.4993184936417\n",
      "Epoch:295         Loss:102.24474798665031\n",
      "Epoch:296         Loss:101.99439195472505\n",
      "Epoch:297         Loss:101.7482125734206\n",
      "Epoch:298         Loss:101.50562220748935\n",
      "Epoch:299         Loss:101.2689924379415\n",
      "Epoch:300         Loss:101.03648209563174\n",
      "Epoch:301         Loss:100.80848145786645\n",
      "Epoch:302         Loss:100.58674336021589\n",
      "Epoch:303         Loss:100.36807766087165\n",
      "Epoch:304         Loss:100.1524568576974\n",
      "Epoch:305         Loss:99.94155738055396\n",
      "Epoch:306         Loss:99.73430307546184\n",
      "Epoch:307         Loss:99.52833070820363\n",
      "Epoch:308         Loss:99.32363185921145\n",
      "Epoch:309         Loss:99.11933641231684\n",
      "Epoch:310         Loss:98.91819923946684\n",
      "Epoch:311         Loss:98.72289557894261\n",
      "Epoch:312         Loss:98.5298698483293\n",
      "Epoch:313         Loss:98.33810936039515\n",
      "Epoch:314         Loss:98.1476496149042\n",
      "Epoch:315         Loss:97.95845334213082\n",
      "Epoch:316         Loss:97.77047914976728\n",
      "Epoch:317         Loss:97.58371383108447\n",
      "Epoch:318         Loss:97.39906545198792\n",
      "Epoch:319         Loss:97.21562168147753\n",
      "Epoch:320         Loss:97.03443496266152\n",
      "Epoch:321         Loss:96.85453814742675\n",
      "Epoch:322         Loss:96.67584144086308\n",
      "Epoch:323         Loss:96.4983274116834\n",
      "Epoch:324         Loss:96.32174236338578\n",
      "Epoch:325         Loss:96.14629122790868\n",
      "Epoch:326         Loss:95.97139434892466\n",
      "Epoch:327         Loss:95.79717683208051\n",
      "Epoch:328         Loss:95.62513693775287\n",
      "Epoch:329         Loss:95.45482045049856\n",
      "Epoch:330         Loss:95.28568529082975\n",
      "Epoch:331         Loss:95.11691447935165\n",
      "Epoch:332         Loss:94.9492824138058\n",
      "Epoch:333         Loss:94.78278890258228\n",
      "Epoch:334         Loss:94.6174082711686\n",
      "Epoch:335         Loss:94.45325192437856\n",
      "Epoch:336         Loss:94.28958868909706\n",
      "Epoch:337         Loss:94.12674906157342\n",
      "Epoch:338         Loss:93.96463068455331\n",
      "Epoch:339         Loss:93.80312122035099\n",
      "Epoch:340         Loss:93.64216770014366\n",
      "Epoch:341         Loss:93.482093509793\n",
      "Epoch:342         Loss:93.32296255374243\n",
      "Epoch:343         Loss:93.1648361073552\n",
      "Epoch:344         Loss:93.0076439558374\n",
      "Epoch:345         Loss:92.85137143325667\n",
      "Epoch:346         Loss:92.69587886176278\n",
      "Epoch:347         Loss:92.54103840477903\n",
      "Epoch:348         Loss:92.3861389241311\n",
      "Epoch:349         Loss:92.23224033038663\n",
      "Epoch:350         Loss:92.07881492531781\n",
      "Epoch:351         Loss:91.92598671628444\n",
      "Epoch:352         Loss:91.77416763602776\n",
      "Epoch:353         Loss:91.6233120463688\n",
      "Epoch:354         Loss:91.47277840929196\n",
      "Epoch:355         Loss:91.32283182211161\n",
      "Epoch:356         Loss:91.17414802034803\n",
      "Epoch:357         Loss:91.02752298710494\n",
      "Epoch:358         Loss:90.88092764029018\n",
      "Epoch:359         Loss:90.73532069879614\n",
      "Epoch:360         Loss:90.59010429908837\n",
      "Epoch:361         Loss:90.44556063107865\n",
      "Epoch:362         Loss:90.30166792184184\n",
      "Epoch:363         Loss:90.15859873406097\n",
      "Epoch:364         Loss:90.01604315389343\n",
      "Epoch:365         Loss:89.87403665954064\n",
      "Epoch:366         Loss:89.73300006830833\n",
      "Epoch:367         Loss:89.59181441382526\n",
      "Epoch:368         Loss:89.45130310860455\n",
      "Epoch:369         Loss:89.31174869351175\n",
      "Epoch:370         Loss:89.17416941074842\n",
      "Epoch:371         Loss:89.0373063918174\n",
      "Epoch:372         Loss:88.90118375023744\n",
      "Epoch:373         Loss:88.76505747850312\n",
      "Epoch:374         Loss:88.62979440071659\n",
      "Epoch:375         Loss:88.49631907884901\n",
      "Epoch:376         Loss:88.36460267618308\n",
      "Epoch:377         Loss:88.2337142451373\n",
      "Epoch:378         Loss:88.10405851770952\n",
      "Epoch:379         Loss:87.97543819842495\n",
      "Epoch:380         Loss:87.84856416078755\n",
      "Epoch:381         Loss:87.72201942684072\n",
      "Epoch:382         Loss:87.59699296362442\n",
      "Epoch:383         Loss:87.47235217374512\n",
      "Epoch:384         Loss:87.34879599073811\n",
      "Epoch:385         Loss:87.22598291525962\n",
      "Epoch:386         Loss:87.10420548675634\n",
      "Epoch:387         Loss:86.98323398712617\n",
      "Epoch:388         Loss:86.86352047997872\n",
      "Epoch:389         Loss:86.74500438106745\n",
      "Epoch:390         Loss:86.62752760549469\n",
      "Epoch:391         Loss:86.5111902417865\n",
      "Epoch:392         Loss:86.39500662998036\n",
      "Epoch:393         Loss:86.27975549680316\n",
      "Epoch:394         Loss:86.16521553301293\n",
      "Epoch:395         Loss:86.05165004659872\n",
      "Epoch:396         Loss:85.93821828367817\n",
      "Epoch:397         Loss:85.82547838009394\n",
      "Epoch:398         Loss:85.71339203689459\n",
      "Epoch:399         Loss:85.602412026093\n",
      "Epoch:400         Loss:85.49208780716384\n",
      "Epoch:401         Loss:85.3826459427771\n",
      "Epoch:402         Loss:85.27478219823122\n",
      "Epoch:403         Loss:85.16746906359529\n",
      "Epoch:404         Loss:85.06029538948687\n",
      "Epoch:405         Loss:84.95392981313945\n",
      "Epoch:406         Loss:84.84816488740556\n",
      "Epoch:407         Loss:84.74298239808977\n",
      "Epoch:408         Loss:84.6383649427981\n",
      "Epoch:409         Loss:84.53429589224389\n",
      "Epoch:410         Loss:84.4307284203852\n",
      "Epoch:411         Loss:84.327662010319\n",
      "Epoch:412         Loss:84.22512084848968\n",
      "Epoch:413         Loss:84.12310611462301\n",
      "Epoch:414         Loss:84.02157308950845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:415         Loss:83.92050868525452\n",
      "Epoch:416         Loss:83.81990038581753\n",
      "Epoch:417         Loss:83.71973621967413\n",
      "Epoch:418         Loss:83.62000473382305\n",
      "Epoch:419         Loss:83.52112520977956\n",
      "Epoch:420         Loss:83.42386076788668\n",
      "Epoch:421         Loss:83.32703666010246\n",
      "Epoch:422         Loss:83.23064099601709\n",
      "Epoch:423         Loss:83.13526506527984\n",
      "Epoch:424         Loss:83.04160306292079\n",
      "Epoch:425         Loss:82.94917201835467\n",
      "Epoch:426         Loss:82.85721060359107\n",
      "Epoch:427         Loss:82.76570440820286\n",
      "Epoch:428         Loss:82.6746396866021\n",
      "Epoch:429         Loss:82.58400332521161\n",
      "Epoch:430         Loss:82.49378281125891\n",
      "Epoch:431         Loss:82.4043422662992\n",
      "Epoch:432         Loss:82.31613128552004\n",
      "Epoch:433         Loss:82.22834701424988\n",
      "Epoch:434         Loss:82.14097640380889\n",
      "Epoch:435         Loss:82.05401483244493\n",
      "Epoch:436         Loss:81.96738871086096\n",
      "Epoch:437         Loss:81.88109219166141\n",
      "Epoch:438         Loss:81.79548057413764\n",
      "Epoch:439         Loss:81.71025283150588\n",
      "Epoch:440         Loss:81.62605699219934\n",
      "Epoch:441         Loss:81.54263283604901\n",
      "Epoch:442         Loss:81.45959739704894\n",
      "Epoch:443         Loss:81.37693823279368\n",
      "Epoch:444         Loss:81.29464351551414\n",
      "Epoch:445         Loss:81.21268713698346\n",
      "Epoch:446         Loss:81.1304043340162\n",
      "Epoch:447         Loss:81.04792520862917\n",
      "Epoch:448         Loss:80.96573249647226\n",
      "Epoch:449         Loss:80.88402636913213\n",
      "Epoch:450         Loss:80.8039056425546\n",
      "Epoch:451         Loss:80.72482920692923\n",
      "Epoch:452         Loss:80.64607233951646\n",
      "Epoch:453         Loss:80.56783202128995\n",
      "Epoch:454         Loss:80.49058648102691\n",
      "Epoch:455         Loss:80.41367033093856\n",
      "Epoch:456         Loss:80.33702759873266\n",
      "Epoch:457         Loss:80.26080612389448\n",
      "Epoch:458         Loss:80.184763785067\n",
      "Epoch:459         Loss:80.10913026226243\n",
      "Epoch:460         Loss:80.03374776055014\n",
      "Epoch:461         Loss:79.95944150161264\n",
      "Epoch:462         Loss:79.88591472991446\n",
      "Epoch:463         Loss:79.81277214134148\n",
      "Epoch:464         Loss:79.7399356727058\n",
      "Epoch:465         Loss:79.66767826379812\n",
      "Epoch:466         Loss:79.59596578113954\n",
      "Epoch:467         Loss:79.52449593582625\n",
      "Epoch:468         Loss:79.45406860481998\n",
      "Epoch:469         Loss:79.384058817077\n",
      "Epoch:470         Loss:79.31437858989356\n",
      "Epoch:471         Loss:79.24501492739425\n",
      "Epoch:472         Loss:79.17595556941245\n",
      "Epoch:473         Loss:79.10718894753145\n",
      "Epoch:474         Loss:79.03870132689899\n",
      "Epoch:475         Loss:78.97044731849185\n",
      "Epoch:476         Loss:78.9024528381074\n",
      "Epoch:477         Loss:78.83470888195626\n",
      "Epoch:478         Loss:78.7671723693754\n",
      "Epoch:479         Loss:78.69981377112761\n",
      "Epoch:480         Loss:78.63262868906206\n",
      "Epoch:481         Loss:78.56565983262107\n",
      "Epoch:482         Loss:78.4989006378144\n",
      "Epoch:483         Loss:78.43234489001391\n",
      "Epoch:484         Loss:78.36598670335867\n",
      "Epoch:485         Loss:78.2998205014071\n",
      "Epoch:486         Loss:78.23384099895989\n",
      "Epoch:487         Loss:78.16803704988104\n",
      "Epoch:488         Loss:78.10232613979375\n",
      "Epoch:489         Loss:78.03678782712586\n",
      "Epoch:490         Loss:77.9714178319706\n",
      "Epoch:491         Loss:77.90621209009277\n",
      "Epoch:492         Loss:77.8411667404315\n",
      "Epoch:493         Loss:77.77626776040886\n",
      "Epoch:494         Loss:77.71147363758888\n",
      "Epoch:495         Loss:77.64682868930531\n",
      "Epoch:496         Loss:77.58232984006423\n",
      "Epoch:497         Loss:77.51793540848732\n",
      "Epoch:498         Loss:77.45447655161563\n",
      "Epoch:499         Loss:77.39117540565269\n",
      "Epoch:500         Loss:77.32816709727973\n",
      "Epoch:501         Loss:77.26506270217789\n",
      "Epoch:502         Loss:77.20234088982497\n",
      "Epoch:503         Loss:77.13953729290893\n",
      "Epoch:504         Loss:77.07708966698753\n",
      "Epoch:505         Loss:77.01457219633537\n",
      "Epoch:506         Loss:76.95227554672313\n",
      "Epoch:507         Loss:76.89022962620979\n",
      "Epoch:508         Loss:76.82799000288612\n",
      "Epoch:509         Loss:76.76612222936984\n",
      "Epoch:510         Loss:76.7041383182073\n",
      "Epoch:511         Loss:76.64259995449062\n",
      "Epoch:512         Loss:76.58144181186917\n",
      "Epoch:513         Loss:76.52047559981634\n",
      "Epoch:514         Loss:76.45968153402927\n",
      "Epoch:515         Loss:76.39910878495314\n",
      "Epoch:516         Loss:76.3386200809938\n",
      "Epoch:517         Loss:76.27843583297764\n",
      "Epoch:518         Loss:76.21815804336757\n",
      "Epoch:519         Loss:76.15809336500045\n",
      "Epoch:520         Loss:76.09827316030946\n",
      "Epoch:521         Loss:76.03833952539796\n",
      "Epoch:522         Loss:75.9787284839253\n",
      "Epoch:523         Loss:75.91903736666902\n",
      "Epoch:524         Loss:75.8594835606767\n",
      "Epoch:525         Loss:75.80035733607585\n",
      "Epoch:526         Loss:75.74143715146622\n",
      "Epoch:527         Loss:75.68315064974628\n",
      "Epoch:528         Loss:75.62498671042943\n",
      "Epoch:529         Loss:75.56694243624653\n",
      "Epoch:530         Loss:75.50890360385098\n",
      "Epoch:531         Loss:75.45162831423171\n",
      "Epoch:532         Loss:75.39448259759396\n",
      "Epoch:533         Loss:75.33746289464698\n",
      "Epoch:534         Loss:75.28056585869251\n",
      "Epoch:535         Loss:75.22378834144682\n",
      "Epoch:536         Loss:75.1671273798328\n",
      "Epoch:537         Loss:75.11058018367623\n",
      "Epoch:538         Loss:75.05414412424417\n",
      "Epoch:539         Loss:74.99785710653607\n",
      "Epoch:540         Loss:74.94205929364887\n",
      "Epoch:541         Loss:74.88631903495904\n",
      "Epoch:542         Loss:74.83069742129359\n",
      "Epoch:543         Loss:74.77519127631055\n",
      "Epoch:544         Loss:74.71979761622755\n",
      "Epoch:545         Loss:74.66444775325495\n",
      "Epoch:546         Loss:74.60954693242451\n",
      "Epoch:547         Loss:74.55476268346793\n",
      "Epoch:548         Loss:74.50038547430984\n",
      "Epoch:549         Loss:74.44625391503092\n",
      "Epoch:550         Loss:74.39225001010665\n",
      "Epoch:551         Loss:74.3383694975594\n",
      "Epoch:552         Loss:74.28461967511792\n",
      "Epoch:553         Loss:74.23101596689712\n",
      "Epoch:554         Loss:74.17743660109876\n",
      "Epoch:555         Loss:74.12421574933583\n",
      "Epoch:556         Loss:74.0711183484909\n",
      "Epoch:557         Loss:74.01814023363343\n",
      "Epoch:558         Loss:73.96527752484366\n",
      "Epoch:559         Loss:73.91253019441865\n",
      "Epoch:560         Loss:73.85973231527171\n",
      "Epoch:561         Loss:73.807477228883\n",
      "Epoch:562         Loss:73.75534419449741\n",
      "Epoch:563         Loss:73.70332898079542\n",
      "Epoch:564         Loss:73.65142764944736\n",
      "Epoch:565         Loss:73.59962481526387\n",
      "Epoch:566         Loss:73.54809035750706\n",
      "Epoch:567         Loss:73.49657753988797\n",
      "Epoch:568         Loss:73.44552692584587\n",
      "Epoch:569         Loss:73.39471120444198\n",
      "Epoch:570         Loss:73.343856767546\n",
      "Epoch:571         Loss:73.29326888106779\n",
      "Epoch:572         Loss:73.24279368727895\n",
      "Epoch:573         Loss:73.19271866242636\n",
      "Epoch:574         Loss:73.14279987822145\n",
      "Epoch:575         Loss:73.09290001316465\n",
      "Epoch:576         Loss:73.04314053136822\n",
      "Epoch:577         Loss:72.99348797893639\n",
      "Epoch:578         Loss:72.94393896922337\n",
      "Epoch:579         Loss:72.89449034742657\n",
      "Epoch:580         Loss:72.84500837995058\n",
      "Epoch:581         Loss:72.7959740711333\n",
      "Epoch:582         Loss:72.74702973912365\n",
      "Epoch:583         Loss:72.69819226451781\n",
      "Epoch:584         Loss:72.64951078038888\n",
      "Epoch:585         Loss:72.60091645748435\n",
      "Epoch:586         Loss:72.5524072592795\n",
      "Epoch:587         Loss:72.50398127434137\n",
      "Epoch:588         Loss:72.45580406200519\n",
      "Epoch:589         Loss:72.40789384255248\n",
      "Epoch:590         Loss:72.36011356407528\n",
      "Epoch:591         Loss:72.31255529997627\n",
      "Epoch:592         Loss:72.26507939089807\n",
      "Epoch:593         Loss:72.21765214046836\n",
      "Epoch:594         Loss:72.1702530704242\n",
      "Epoch:595         Loss:72.12293140424022\n",
      "Epoch:596         Loss:72.07566380153224\n",
      "Epoch:597         Loss:72.02842776894678\n",
      "Epoch:598         Loss:71.98109635209218\n",
      "Epoch:599         Loss:71.93383970596992\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# stochastic gradient descent\n",
    "\n",
    "# updating the weights\n",
    "\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss=0\n",
    "    for j in np.arange(0,x_train.shape[1],batch_size):       \n",
    "        dW = np.zeros(W[0].shape)\n",
    "        dM = np.zeros(W[1].shape)\n",
    "        dT = np.zeros(W[2].shape)\n",
    "        db1 = np.zeros(b[0].shape)\n",
    "        db2 = np.zeros(b[1].shape)\n",
    "        db3 = np.zeros(b[2].shape)\n",
    "        for k in range(0,batch_size):\n",
    "            # forward pass\n",
    "            x = x_train[:,j+k].reshape(-1,1)\n",
    "            y = y_train[:,j+k].reshape(-1,1)\n",
    "            out,y_pred=forward_path(noHiddenLayers,x,W,b,Actfnvect)\n",
    "            # backpropagation\n",
    "            dWtemp,dMtemp,dTtemp,db1temp,db2temp,db3temp=backprop(y_pred,y,out[1],W[-1],out[0],W[1],x)\n",
    "            \n",
    "            dW=dW+dWtemp\n",
    "            db1=db1+db1temp\n",
    "            \n",
    "            dM=dM+dMtemp\n",
    "            db2=db2+db2temp\n",
    "            \n",
    "            dT=dT+dTtemp\n",
    "            db3=db3+db3temp\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = loss + (-1.0*np.sum(y*np.log(y_pred)))\n",
    "            \n",
    "        # Updating the weights SGD\n",
    "        b[0]=b[0]-learning_rate*db1*(1.0/batch_size)\n",
    "        W[0]=W[0]-learning_rate*dW*(1.0/batch_size)\n",
    "\n",
    "        b[1]=b[1]-learning_rate*db2*(1.0/batch_size)\n",
    "        W[1]=W[1]-learning_rate*dM*(1.0/batch_size)\n",
    "\n",
    "        b[2]=b[2]-learning_rate*db3*(1.0/batch_size)\n",
    "        W[2]=W[2]-learning_rate*dT*(1.0/batch_size)\n",
    "    \n",
    "    #print the loss in each epoch\n",
    "    print('Epoch:'+str(i)+'         Loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,y_pred=forward_path(noHiddenLayers,x_test,W,b,Actfnvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(y):\n",
    "    return np.argmax(y)\n",
    "\n",
    "yvect=[]\n",
    "y_trurevect=[]\n",
    "\n",
    "for i in range(0,x_test.shape[1]):\n",
    "    yvect.append(predict(y_pred[:,i]))\n",
    "    y_trurevect.append(predict(y_test[:,i]))\n",
    "\n",
    "# find accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "#predicting test accuracy\n",
    "print(accuracy_score(y_trurevect, yvect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# to see the output vs true values\n",
    "\n",
    "print y_trurevect\n",
    "print yvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
