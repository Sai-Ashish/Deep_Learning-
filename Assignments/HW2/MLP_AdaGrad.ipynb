{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Note:run in python2######\n",
    "# same initialization of weights because same random seed used\n",
    "\n",
    "# Observations:\n",
    "# for training deep neural network models, the accumulation of squared gradients from the beginning of\n",
    "# training can result in a premature and excessive decrease in the eﬀective learning\n",
    "# rate. AdaGrad performs well for some but not all deep learning models.\n",
    "# When applied to a nonconvex function to train a neural network,\n",
    "# the learning trajectory may pass through many diﬀerent structures and eventually\n",
    "# arrive at a region that is a locally convex bowl. AdaGrad shrinks the learning rate\n",
    "# according to the entire history of the squared gradient and may have made the\n",
    "# learning rate too small before arriving at such a convex structure.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#MLP\n",
    "\n",
    "# initialisation of the weights he_normal\n",
    "def weights(noHiddenLayers,sizeOfLayers):\n",
    "\n",
    "    W=[]\n",
    "    b=[]\n",
    "\n",
    "    for i in range(0,noHiddenLayers+1):\n",
    "        \n",
    "        W.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+sizeOfLayers[i])),\n",
    "                                   (sizeOfLayers[i+1],sizeOfLayers[i])) )\n",
    "        b.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+1)),\n",
    "                                   (sizeOfLayers[i+1],1)) )\n",
    "\n",
    "    W=np.array(W)\n",
    "    b=np.array(b)\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "\n",
    "#mlp forward pass\n",
    "#layer\n",
    "def layer(w,x,b):\n",
    "    out = np.matmul(w,x)+b\n",
    "    return out\n",
    "\n",
    "def apply_activationMLP(Activation_function,inp):\n",
    "    \n",
    "    #activation functions\n",
    "    if Activation_function == 'relu':\n",
    "        return np.where(inp<0,0,inp)\n",
    "    elif Activation_function == \"tanh\":\n",
    "        return np.tanh(inp)\n",
    "    elif Activation_function == \"sigmoid\":\n",
    "        return 1.0/(1+np.exp(-1.0*inp))\n",
    "    elif Activation_function == \"softmax\":\n",
    "        return (1.0/(np.sum(np.exp(inp),axis=0)))*(np.exp(inp))\n",
    "\n",
    "#forward path\n",
    "def forward_path(noHiddenLayers,X,W,b,Actfnvect):\n",
    "\n",
    "    out=[]\n",
    "    \n",
    "    z=apply_activationMLP(Actfnvect[0],np.array(layer(W[0],X,b[0])))\n",
    "    out.append(np.array(z))\n",
    "\n",
    "    for i in range(1,noHiddenLayers):\n",
    "        z=apply_activationMLP(Actfnvect[i],np.array(layer(W[i],out[i-1],b[i])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    if noHiddenLayers > 0:\n",
    "        z=apply_activationMLP(Actfnvect[-1],np.array(layer(W[-1],out[-1],b[-1])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    y_pred = out[-1]\n",
    "\n",
    "    return np.array(out),np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "#only to import data\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# #import data\n",
    "iris_data = load_iris() # load the iris dataset\n",
    "\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "# Split the data for training and testing\n",
    "x_train , x_test, y_train , y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(3, 120)\n",
      "(4, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "# run MLP algorithm\n",
    "\n",
    "x_train = np.moveaxis(x_train,0,-1)\n",
    "y_train = np.moveaxis(y_train,0,-1)\n",
    "x_test = np.moveaxis(x_test,0,-1)\n",
    "y_test = np.moveaxis(y_test,0,-1)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_relu(x):\n",
    "    return np.where(x == 0,0,1)\n",
    "\n",
    "# backpropation\n",
    "def backprop(y,y_true,z,T,u,M,x):\n",
    "    \n",
    "    dy = (y-y_true)\n",
    "    \n",
    "    # layer 3\n",
    "    dT  = np.matmul(dy,z.T)\n",
    "    db3 = np.sum(dy,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 2\n",
    "    s   = np.matmul(T.T,dy)\n",
    "    s   = s*der_relu(z)\n",
    "    dM  = np.matmul(s,u.T)\n",
    "    db2 = np.sum(s,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 1\n",
    "    sm  = np.matmul(M.T,s)\n",
    "    sm  = sm*der_relu(u)\n",
    "    dW  = np.matmul(sm,x.T)\n",
    "    db1 = np.sum(sm,axis=1).reshape(-1,1)\n",
    "    \n",
    "    return dW,dM,dT,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "################################\n",
    "# Training parameters\n",
    "num_classes = 3\n",
    "epochs = 500\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "delta=1e-7\n",
    "################################\n",
    "\n",
    "#MLP PARAMETERS\n",
    "noHiddenLayers=2\n",
    "\n",
    "#also includes the input vector dimension and output vector dimension\n",
    "sizeOfLayers=[x_train.shape[0],10,10,num_classes]\n",
    "\n",
    "sizeofOutput=num_classes\n",
    "\n",
    "Actfnvect = ['relu','relu','softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = weights(noHiddenLayers,sizeOfLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0         Loss:188.95941234972648\n",
      "Epoch:1         Loss:115.06341514368438\n",
      "Epoch:2         Loss:83.3120040708373\n",
      "Epoch:3         Loss:67.55527781826775\n",
      "Epoch:4         Loss:58.655780160441665\n",
      "Epoch:5         Loss:53.793604069510366\n",
      "Epoch:6         Loss:50.70087354903374\n",
      "Epoch:7         Loss:48.47551262865901\n",
      "Epoch:8         Loss:46.7294406898982\n",
      "Epoch:9         Loss:45.292066368028294\n",
      "Epoch:10         Loss:44.05941542133694\n",
      "Epoch:11         Loss:42.96943285646006\n",
      "Epoch:12         Loss:41.98166707758445\n",
      "Epoch:13         Loss:41.08473998205652\n",
      "Epoch:14         Loss:40.24259374454702\n",
      "Epoch:15         Loss:39.37496964260255\n",
      "Epoch:16         Loss:38.526671170022915\n",
      "Epoch:17         Loss:37.61299552092703\n",
      "Epoch:18         Loss:36.65183795360373\n",
      "Epoch:19         Loss:35.7079251677687\n",
      "Epoch:20         Loss:34.83941827686435\n",
      "Epoch:21         Loss:34.0160820145412\n",
      "Epoch:22         Loss:33.233235875094365\n",
      "Epoch:23         Loss:32.45567141145997\n",
      "Epoch:24         Loss:31.691722786003947\n",
      "Epoch:25         Loss:30.9425195713853\n",
      "Epoch:26         Loss:30.22923855862205\n",
      "Epoch:27         Loss:29.570437580629264\n",
      "Epoch:28         Loss:28.928186874854948\n",
      "Epoch:29         Loss:28.341356634034007\n",
      "Epoch:30         Loss:27.79212179154725\n",
      "Epoch:31         Loss:27.265294101555636\n",
      "Epoch:32         Loss:26.765858267026143\n",
      "Epoch:33         Loss:26.28634644043008\n",
      "Epoch:34         Loss:25.826175092624034\n",
      "Epoch:35         Loss:25.38142762776945\n",
      "Epoch:36         Loss:24.96092208030267\n",
      "Epoch:37         Loss:24.552953834525976\n",
      "Epoch:38         Loss:24.164053457634054\n",
      "Epoch:39         Loss:23.78805885853194\n",
      "Epoch:40         Loss:23.425701648450087\n",
      "Epoch:41         Loss:23.07638692006059\n",
      "Epoch:42         Loss:22.73965482094933\n",
      "Epoch:43         Loss:22.41462809906907\n",
      "Epoch:44         Loss:22.099936338603044\n",
      "Epoch:45         Loss:21.798241424097217\n",
      "Epoch:46         Loss:21.505072044755046\n",
      "Epoch:47         Loss:21.223046713998798\n",
      "Epoch:48         Loss:20.949906405204068\n",
      "Epoch:49         Loss:20.685624331395232\n",
      "Epoch:50         Loss:20.429372245229345\n",
      "Epoch:51         Loss:20.182928646260695\n",
      "Epoch:52         Loss:19.938378527705428\n",
      "Epoch:53         Loss:19.711828535171875\n",
      "Epoch:54         Loss:19.48622688741311\n",
      "Epoch:55         Loss:19.264206255469244\n",
      "Epoch:56         Loss:19.056611665732852\n",
      "Epoch:57         Loss:18.851302321615666\n",
      "Epoch:58         Loss:18.651802431271747\n",
      "Epoch:59         Loss:18.45795651726797\n",
      "Epoch:60         Loss:18.269412752454272\n",
      "Epoch:61         Loss:18.086588333553618\n",
      "Epoch:62         Loss:17.908455906550085\n",
      "Epoch:63         Loss:17.734530544424228\n",
      "Epoch:64         Loss:17.566143005149204\n",
      "Epoch:65         Loss:17.40209096541458\n",
      "Epoch:66         Loss:17.242354250050166\n",
      "Epoch:67         Loss:17.086604083321337\n",
      "Epoch:68         Loss:16.934699739045797\n",
      "Epoch:69         Loss:16.787080854312947\n",
      "Epoch:70         Loss:16.642952800956195\n",
      "Epoch:71         Loss:16.5024616117076\n",
      "Epoch:72         Loss:16.365486329362056\n",
      "Epoch:73         Loss:16.231650040902423\n",
      "Epoch:74         Loss:16.101519705708455\n",
      "Epoch:75         Loss:15.97340655638313\n",
      "Epoch:76         Loss:15.849499786330169\n",
      "Epoch:77         Loss:15.72790365028588\n",
      "Epoch:78         Loss:15.60893419844758\n",
      "Epoch:79         Loss:15.49398044023838\n",
      "Epoch:80         Loss:15.380832205650385\n",
      "Epoch:81         Loss:15.269130820874638\n",
      "Epoch:82         Loss:15.161714238077854\n",
      "Epoch:83         Loss:15.056235182321299\n",
      "Epoch:84         Loss:14.952702273958286\n",
      "Epoch:85         Loss:14.852027606130303\n",
      "Epoch:86         Loss:14.753536422023993\n",
      "Epoch:87         Loss:14.656772768119852\n",
      "Epoch:88         Loss:14.562585776240544\n",
      "Epoch:89         Loss:14.470300825078382\n",
      "Epoch:90         Loss:14.37976689282683\n",
      "Epoch:91         Loss:14.291591594324496\n",
      "Epoch:92         Loss:14.205618342996443\n",
      "Epoch:93         Loss:14.120735540980796\n",
      "Epoch:94         Loss:14.038127408366305\n",
      "Epoch:95         Loss:13.956906794387127\n",
      "Epoch:96         Loss:13.877686911711095\n",
      "Epoch:97         Loss:13.79976515114021\n",
      "Epoch:98         Loss:13.723590480075902\n",
      "Epoch:99         Loss:13.648704662840595\n",
      "Epoch:100         Loss:13.57552910872288\n",
      "Epoch:101         Loss:13.503611211191211\n",
      "Epoch:102         Loss:13.43337080709898\n",
      "Epoch:103         Loss:13.364614280145762\n",
      "Epoch:104         Loss:13.296984871482183\n",
      "Epoch:105         Loss:13.230477728329571\n",
      "Epoch:106         Loss:13.16549780183593\n",
      "Epoch:107         Loss:13.101190605238138\n",
      "Epoch:108         Loss:13.038258441646722\n",
      "Epoch:109         Loss:12.976522878555148\n",
      "Epoch:110         Loss:12.916051966624897\n",
      "Epoch:111         Loss:12.856437850823335\n",
      "Epoch:112         Loss:12.797976035098838\n",
      "Epoch:113         Loss:12.740575831600912\n",
      "Epoch:114         Loss:12.68427814184872\n",
      "Epoch:115         Loss:12.628802851250288\n",
      "Epoch:116         Loss:12.574359682642285\n",
      "Epoch:117         Loss:12.520871473191487\n",
      "Epoch:118         Loss:12.46815213492505\n",
      "Epoch:119         Loss:12.416270106665392\n",
      "Epoch:120         Loss:12.365743861831609\n",
      "Epoch:121         Loss:12.3156283679155\n",
      "Epoch:122         Loss:12.266482217268038\n",
      "Epoch:123         Loss:12.217983861603848\n",
      "Epoch:124         Loss:12.170336932539426\n",
      "Epoch:125         Loss:12.123464630298857\n",
      "Epoch:126         Loss:12.077362108327684\n",
      "Epoch:127         Loss:12.0318502104721\n",
      "Epoch:128         Loss:11.98711312671315\n",
      "Epoch:129         Loss:11.943327130028095\n",
      "Epoch:130         Loss:11.899163822277494\n",
      "Epoch:131         Loss:11.856543388632083\n",
      "Epoch:132         Loss:11.814621869919597\n",
      "Epoch:133         Loss:11.773351872900447\n",
      "Epoch:134         Loss:11.732710872793529\n",
      "Epoch:135         Loss:11.692682800866654\n",
      "Epoch:136         Loss:11.653253514781323\n",
      "Epoch:137         Loss:11.614409652147783\n",
      "Epoch:138         Loss:11.576145402299673\n",
      "Epoch:139         Loss:11.538441202960964\n",
      "Epoch:140         Loss:11.501285720566013\n",
      "Epoch:141         Loss:11.46466661504815\n",
      "Epoch:142         Loss:11.42857226423548\n",
      "Epoch:143         Loss:11.392991470741164\n",
      "Epoch:144         Loss:11.357913376194183\n",
      "Epoch:145         Loss:11.323327430460662\n",
      "Epoch:146         Loss:11.289223375818455\n",
      "Epoch:147         Loss:11.25559123547097\n",
      "Epoch:148         Loss:11.22242130355079\n",
      "Epoch:149         Loss:11.189704135828405\n",
      "Epoch:150         Loss:11.15743054089473\n",
      "Epoch:151         Loss:11.1255915717379\n",
      "Epoch:152         Loss:11.09417851767667\n",
      "Epoch:153         Loss:11.06318289662595\n",
      "Epoch:154         Loss:11.032596447675482\n",
      "Epoch:155         Loss:11.002411123964023\n",
      "Epoch:156         Loss:10.972619085833927\n",
      "Epoch:157         Loss:10.943212694251706\n",
      "Epoch:158         Loss:10.914184504481552\n",
      "Epoch:159         Loss:10.885527259999485\n",
      "Epoch:160         Loss:10.857233886636946\n",
      "Epoch:161         Loss:10.82929748694295\n",
      "Epoch:162         Loss:10.80171133475503\n",
      "Epoch:163         Loss:10.774468869969503\n",
      "Epoch:164         Loss:10.747563693502293\n",
      "Epoch:165         Loss:10.721219844983274\n",
      "Epoch:166         Loss:10.69494677989229\n",
      "Epoch:167         Loss:10.669020174485963\n",
      "Epoch:168         Loss:10.643413425265493\n",
      "Epoch:169         Loss:10.618118541296457\n",
      "Epoch:170         Loss:10.593126268535263\n",
      "Epoch:171         Loss:10.568430638107039\n",
      "Epoch:172         Loss:10.544026200121383\n",
      "Epoch:173         Loss:10.519907749913257\n",
      "Epoch:174         Loss:10.496070240669292\n",
      "Epoch:175         Loss:10.472508753837642\n",
      "Epoch:176         Loss:10.449218487614655\n",
      "Epoch:177         Loss:10.426194751165305\n",
      "Epoch:178         Loss:10.403432960718813\n",
      "Epoch:179         Loss:10.380928636325216\n",
      "Epoch:180         Loss:10.358177728221877\n",
      "Epoch:181         Loss:10.336173645277045\n",
      "Epoch:182         Loss:10.314423349172444\n",
      "Epoch:183         Loss:10.292916503488502\n",
      "Epoch:184         Loss:10.271649132443661\n",
      "Epoch:185         Loss:10.250616157963277\n",
      "Epoch:186         Loss:10.229813209543018\n",
      "Epoch:187         Loss:10.209235728478605\n",
      "Epoch:188         Loss:10.188879763899694\n",
      "Epoch:189         Loss:10.168741620781262\n",
      "Epoch:190         Loss:10.149166982553197\n",
      "Epoch:191         Loss:10.129431147177307\n",
      "Epoch:192         Loss:10.10992122051387\n",
      "Epoch:193         Loss:10.090288085672594\n",
      "Epoch:194         Loss:10.071550517987571\n",
      "Epoch:195         Loss:10.052635930449831\n",
      "Epoch:196         Loss:10.033926696422155\n",
      "Epoch:197         Loss:10.015412633896814\n",
      "Epoch:198         Loss:9.996766731513652\n",
      "Epoch:199         Loss:9.97897294955782\n",
      "Epoch:200         Loss:9.961003687352976\n",
      "Epoch:201         Loss:9.943223616863753\n",
      "Epoch:202         Loss:9.925623974068765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:203         Loss:9.907888772452372\n",
      "Epoch:204         Loss:9.89096834324848\n",
      "Epoch:205         Loss:9.873874101902816\n",
      "Epoch:206         Loss:9.85695467284154\n",
      "Epoch:207         Loss:9.84020241736283\n",
      "Epoch:208         Loss:9.823613132308237\n",
      "Epoch:209         Loss:9.806885324553527\n",
      "Epoch:210         Loss:9.790928712511139\n",
      "Epoch:211         Loss:9.774801619577005\n",
      "Epoch:212         Loss:9.758833869964564\n",
      "Epoch:213         Loss:9.743019017332061\n",
      "Epoch:214         Loss:9.72735342186598\n",
      "Epoch:215         Loss:9.711547201313472\n",
      "Epoch:216         Loss:9.696580828520187\n",
      "Epoch:217         Loss:9.681340429401914\n",
      "Epoch:218         Loss:9.66625113913943\n",
      "Epoch:219         Loss:9.651303792337428\n",
      "Epoch:220         Loss:9.63649403148035\n",
      "Epoch:221         Loss:9.621819099071319\n",
      "Epoch:222         Loss:9.607276824228395\n",
      "Epoch:223         Loss:9.592865271392668\n",
      "Epoch:224         Loss:9.578582612772117\n",
      "Epoch:225         Loss:9.564427080798865\n",
      "Epoch:226         Loss:9.550396949961922\n",
      "Epoch:227         Loss:9.53649052956484\n",
      "Epoch:228         Loss:9.522706160579363\n",
      "Epoch:229         Loss:9.509042214048005\n",
      "Epoch:230         Loss:9.495497090080882\n",
      "Epoch:231         Loss:9.48206921708753\n",
      "Epoch:232         Loss:9.468757051107895\n",
      "Epoch:233         Loss:9.455559075190992\n",
      "Epoch:234         Loss:9.44250434013286\n",
      "Epoch:235         Loss:9.429501738332238\n",
      "Epoch:236         Loss:9.416669272600375\n",
      "Epoch:237         Loss:9.40388430081417\n",
      "Epoch:238         Loss:9.391268208451185\n",
      "Epoch:239         Loss:9.378725662576715\n",
      "Epoch:240         Loss:9.36625872987864\n",
      "Epoch:241         Loss:9.353955082736832\n",
      "Epoch:242         Loss:9.341721774256124\n",
      "Epoch:243         Loss:9.329589146044126\n",
      "Epoch:244         Loss:9.317528381275082\n",
      "Epoch:245         Loss:9.305622236716271\n",
      "Epoch:246         Loss:9.294024480198193\n",
      "Epoch:247         Loss:9.282258294668871\n",
      "Epoch:248         Loss:9.270644506493461\n",
      "Epoch:249         Loss:9.259095048886525\n",
      "Epoch:250         Loss:9.247637926237111\n",
      "Epoch:251         Loss:9.23624487805558\n",
      "Epoch:252         Loss:9.22499684164159\n",
      "Epoch:253         Loss:9.213809105528313\n",
      "Epoch:254         Loss:9.202709100021721\n",
      "Epoch:255         Loss:9.191695722916194\n",
      "Epoch:256         Loss:9.180742032180373\n",
      "Epoch:257         Loss:9.169925664014377\n",
      "Epoch:258         Loss:9.159165803557956\n",
      "Epoch:259         Loss:9.148488539819422\n",
      "Epoch:260         Loss:9.137892876153852\n",
      "Epoch:261         Loss:9.127352403412248\n",
      "Epoch:262         Loss:9.116943279374555\n",
      "Epoch:263         Loss:9.106586561302127\n",
      "Epoch:264         Loss:9.096307548950934\n",
      "Epoch:265         Loss:9.086105490793866\n",
      "Epoch:266         Loss:9.075979502066275\n",
      "Epoch:267         Loss:9.06590385742442\n",
      "Epoch:268         Loss:9.0559528015617\n",
      "Epoch:269         Loss:9.046049840993634\n",
      "Epoch:270         Loss:9.036219528724422\n",
      "Epoch:271         Loss:9.026461058081617\n",
      "Epoch:272         Loss:9.016773629812105\n",
      "Epoch:273         Loss:9.00715645364183\n",
      "Epoch:274         Loss:8.997608749459083\n",
      "Epoch:275         Loss:8.988129747922942\n",
      "Epoch:276         Loss:8.978695269741742\n",
      "Epoch:277         Loss:8.969374301537462\n",
      "Epoch:278         Loss:8.960097131579492\n",
      "Epoch:279         Loss:8.950885626240558\n",
      "Epoch:280         Loss:8.941739105556877\n",
      "Epoch:281         Loss:8.932656881988896\n",
      "Epoch:282         Loss:8.923638269360326\n",
      "Epoch:283         Loss:8.914682587270335\n",
      "Epoch:284         Loss:8.905789163145093\n",
      "Epoch:285         Loss:8.896957333127801\n",
      "Epoch:286         Loss:8.888186442411374\n",
      "Epoch:287         Loss:8.87947584530766\n",
      "Epoch:288         Loss:8.870824905193725\n",
      "Epoch:289         Loss:8.862211025310868\n",
      "Epoch:290         Loss:8.853699374574775\n",
      "Epoch:291         Loss:8.84522384187056\n",
      "Epoch:292         Loss:8.836805398864348\n",
      "Epoch:293         Loss:8.828443510649512\n",
      "Epoch:294         Loss:8.820137619869241\n",
      "Epoch:295         Loss:8.811884526045304\n",
      "Epoch:296         Loss:8.803691797573325\n",
      "Epoch:297         Loss:8.795553437670716\n",
      "Epoch:298         Loss:8.787468845309984\n",
      "Epoch:299         Loss:8.779437452638284\n",
      "Epoch:300         Loss:8.771458711181857\n",
      "Epoch:301         Loss:8.763532085369686\n",
      "Epoch:302         Loss:8.755657049435557\n",
      "Epoch:303         Loss:8.747833085898128\n",
      "Epoch:304         Loss:8.74005968478315\n",
      "Epoch:305         Loss:8.732336343196396\n",
      "Epoch:306         Loss:8.724662565063085\n",
      "Epoch:307         Loss:8.717037860947299\n",
      "Epoch:308         Loss:8.709461747910243\n",
      "Epoch:309         Loss:8.701933749387992\n",
      "Epoch:310         Loss:8.694453395079714\n",
      "Epoch:311         Loss:8.687020220841712\n",
      "Epoch:312         Loss:8.679633768585367\n",
      "Epoch:313         Loss:8.67229436056523\n",
      "Epoch:314         Loss:8.665000110030839\n",
      "Epoch:315         Loss:8.657751233867513\n",
      "Epoch:316         Loss:8.650547300555978\n",
      "Epoch:317         Loss:8.643387882472789\n",
      "Epoch:318         Loss:8.636272556675243\n",
      "Epoch:319         Loss:8.629200905233487\n",
      "Epoch:320         Loss:8.62217251534615\n",
      "Epoch:321         Loss:8.615186979353094\n",
      "Epoch:322         Loss:8.608243894699394\n",
      "Epoch:323         Loss:8.601342863876743\n",
      "Epoch:324         Loss:8.594483494354701\n",
      "Epoch:325         Loss:8.587665398507887\n",
      "Epoch:326         Loss:8.580888193541867\n",
      "Epoch:327         Loss:8.574151501419221\n",
      "Epoch:328         Loss:8.567454948786287\n",
      "Epoch:329         Loss:8.560798166901087\n",
      "Epoch:330         Loss:8.5541807915624\n",
      "Epoch:331         Loss:8.547602463039963\n",
      "Epoch:332         Loss:8.541062826006147\n",
      "Epoch:333         Loss:8.534561529468657\n",
      "Epoch:334         Loss:8.528098226704605\n",
      "Epoch:335         Loss:8.521672575195641\n",
      "Epoch:336         Loss:8.515284236564368\n",
      "Epoch:337         Loss:8.508932876511903\n",
      "Epoch:338         Loss:8.50261816475648\n",
      "Epoch:339         Loss:8.496339774973233\n",
      "Epoch:340         Loss:8.490097384735055\n",
      "Epoch:341         Loss:8.483890675454425\n",
      "Epoch:342         Loss:8.477719332326448\n",
      "Epoch:343         Loss:8.47158304427266\n",
      "Epoch:344         Loss:8.465481503886108\n",
      "Epoch:345         Loss:8.45941440737713\n",
      "Epoch:346         Loss:8.453381636835372\n",
      "Epoch:347         Loss:8.44754279890609\n",
      "Epoch:348         Loss:8.441582978074939\n",
      "Epoch:349         Loss:8.435653997434743\n",
      "Epoch:350         Loss:8.429756744813307\n",
      "Epoch:351         Loss:8.42389155210171\n",
      "Epoch:352         Loss:8.418058454731135\n",
      "Epoch:353         Loss:8.412257336112651\n",
      "Epoch:354         Loss:8.406488003630605\n",
      "Epoch:355         Loss:8.400750227658236\n",
      "Epoch:356         Loss:8.395043761364054\n",
      "Epoch:357         Loss:8.38936835071035\n",
      "Epoch:358         Loss:8.38372373948183\n",
      "Epoch:359         Loss:8.378109671804014\n",
      "Epoch:360         Loss:8.372525893395137\n",
      "Epoch:361         Loss:8.366972152178853\n",
      "Epoch:362         Loss:8.361448198574749\n",
      "Epoch:363         Loss:8.355953785626017\n",
      "Epoch:364         Loss:8.35048866904522\n",
      "Epoch:365         Loss:8.34505311492238\n",
      "Epoch:366         Loss:8.339654626061542\n",
      "Epoch:367         Loss:8.334281161346677\n",
      "Epoch:368         Loss:8.328934161798617\n",
      "Epoch:369         Loss:8.323614298459091\n",
      "Epoch:370         Loss:8.318321817059488\n",
      "Epoch:371         Loss:8.313056738066926\n",
      "Epoch:372         Loss:8.307818965305007\n",
      "Epoch:373         Loss:8.302608343245366\n",
      "Epoch:374         Loss:8.297424686823135\n",
      "Epoch:375         Loss:8.292267796854482\n",
      "Epoch:376         Loss:8.28713746798239\n",
      "Epoch:377         Loss:8.282033492761954\n",
      "Epoch:378         Loss:8.27695566375404\n",
      "Epoch:379         Loss:8.271903774592166\n",
      "Epoch:380         Loss:8.266877620519558\n",
      "Epoch:381         Loss:8.261876998652975\n",
      "Epoch:382         Loss:8.256901708105591\n",
      "Epoch:383         Loss:8.251951550037012\n",
      "Epoch:384         Loss:8.24702632766593\n",
      "Epoch:385         Loss:8.242125846263491\n",
      "Epoch:386         Loss:8.237249913136969\n",
      "Epoch:387         Loss:8.23239833760851\n",
      "Epoch:388         Loss:8.227377690299416\n",
      "Epoch:389         Loss:8.222582439369816\n",
      "Epoch:390         Loss:8.21780916967797\n",
      "Epoch:391         Loss:8.213058529274242\n",
      "Epoch:392         Loss:8.20833078302875\n",
      "Epoch:393         Loss:8.203625989182651\n",
      "Epoch:394         Loss:8.198944096440387\n",
      "Epoch:395         Loss:8.19428499619812\n",
      "Epoch:396         Loss:8.189648550343925\n",
      "Epoch:397         Loss:8.185034606187534\n",
      "Epoch:398         Loss:8.180443004155444\n",
      "Epoch:399         Loss:8.17587358109717\n",
      "Epoch:400         Loss:8.171326173343063\n",
      "Epoch:401         Loss:8.166800617652106\n",
      "Epoch:402         Loss:8.162296751778715\n",
      "Epoch:403         Loss:8.157814414763424\n",
      "Epoch:404         Loss:8.153353447077608\n",
      "Epoch:405         Loss:8.148913690690788\n",
      "Epoch:406         Loss:8.144494989097394\n",
      "Epoch:407         Loss:8.140097187321684\n",
      "Epoch:408         Loss:8.135720131911585\n",
      "Epoch:409         Loss:8.13136367092652\n",
      "Epoch:410         Loss:8.127090979395867\n",
      "Epoch:411         Loss:8.1227805398862\n",
      "Epoch:412         Loss:8.118488275675842\n",
      "Epoch:413         Loss:8.114214935202757\n",
      "Epoch:414         Loss:8.109960863890551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:415         Loss:8.1057261850088\n",
      "Epoch:416         Loss:8.101510901161712\n",
      "Epoch:417         Loss:8.09731495008557\n",
      "Epoch:418         Loss:8.09313823501995\n",
      "Epoch:419         Loss:8.088979811261927\n",
      "Epoch:420         Loss:8.084840582957495\n",
      "Epoch:421         Loss:8.080720228283822\n",
      "Epoch:422         Loss:8.076618617219165\n",
      "Epoch:423         Loss:8.07253561932588\n",
      "Epoch:424         Loss:8.06847110450995\n",
      "Epoch:425         Loss:8.064424943425667\n",
      "Epoch:426         Loss:8.060397007688074\n",
      "Epoch:427         Loss:8.05638716998184\n",
      "Epoch:428         Loss:8.052395304113732\n",
      "Epoch:429         Loss:8.048421285034856\n",
      "Epoch:430         Loss:8.044464988846396\n",
      "Epoch:431         Loss:8.04052629279656\n",
      "Epoch:432         Loss:8.03660507527286\n",
      "Epoch:433         Loss:8.032701215791828\n",
      "Epoch:434         Loss:8.028800357961476\n",
      "Epoch:435         Loss:8.024932675166298\n",
      "Epoch:436         Loss:8.0210817362953\n",
      "Epoch:437         Loss:8.017247564193035\n",
      "Epoch:438         Loss:8.013445620271874\n",
      "Epoch:439         Loss:8.009319185418615\n",
      "Epoch:440         Loss:8.005547484096285\n",
      "Epoch:441         Loss:8.001788867370916\n",
      "Epoch:442         Loss:7.998112923153886\n",
      "Epoch:443         Loss:7.9943895388736514\n",
      "Epoch:444         Loss:7.990679361663982\n",
      "Epoch:445         Loss:7.986998701095331\n",
      "Epoch:446         Loss:7.983302267135308\n",
      "Epoch:447         Loss:7.979636964971121\n",
      "Epoch:448         Loss:7.9760008069712525\n",
      "Epoch:449         Loss:7.97236711899734\n",
      "Epoch:450         Loss:7.9687489479701545\n",
      "Epoch:451         Loss:7.965135825298348\n",
      "Epoch:452         Loss:7.961547897164867\n",
      "Epoch:453         Loss:7.957975527439874\n",
      "Epoch:454         Loss:7.9544297295724675\n",
      "Epoch:455         Loss:7.9508776697544965\n",
      "Epoch:456         Loss:7.947350084295659\n",
      "Epoch:457         Loss:7.943848545318907\n",
      "Epoch:458         Loss:7.940340889890582\n",
      "Epoch:459         Loss:7.936857191248873\n",
      "Epoch:460         Loss:7.9333990739283\n",
      "Epoch:461         Loss:7.92993491824002\n",
      "Epoch:462         Loss:7.92649425461423\n",
      "Epoch:463         Loss:7.92307875413853\n",
      "Epoch:464         Loss:7.919657226060446\n",
      "Epoch:465         Loss:7.916283819251646\n",
      "Epoch:466         Loss:7.912915743511128\n",
      "Epoch:467         Loss:7.909539552141215\n",
      "Epoch:468         Loss:7.906184782318524\n",
      "Epoch:469         Loss:7.902843161851731\n",
      "Epoch:470         Loss:7.899525639948565\n",
      "Epoch:471         Loss:7.896201088560406\n",
      "Epoch:472         Loss:7.892909293614784\n",
      "Epoch:473         Loss:7.889611968545286\n",
      "Epoch:474         Loss:7.886283470999591\n",
      "Epoch:475         Loss:7.883091096009408\n",
      "Epoch:476         Loss:7.879778432953843\n",
      "Epoch:477         Loss:7.876662100115122\n",
      "Epoch:478         Loss:7.873398283156297\n",
      "Epoch:479         Loss:7.870238705301107\n",
      "Epoch:480         Loss:7.8669875556770315\n",
      "Epoch:481         Loss:7.8638703087862325\n",
      "Epoch:482         Loss:7.860635235281333\n",
      "Epoch:483         Loss:7.857531789971549\n",
      "Epoch:484         Loss:7.85439898025425\n",
      "Epoch:485         Loss:7.851206365229673\n",
      "Epoch:486         Loss:7.848139413874465\n",
      "Epoch:487         Loss:7.8450438775784095\n",
      "Epoch:488         Loss:7.841882627942699\n",
      "Epoch:489         Loss:7.83886106093989\n",
      "Epoch:490         Loss:7.835782728499887\n",
      "Epoch:491         Loss:7.832666266059786\n",
      "Epoch:492         Loss:7.829680125478497\n",
      "Epoch:493         Loss:7.826637173585766\n",
      "Epoch:494         Loss:7.82356645550877\n",
      "Epoch:495         Loss:7.820596410116908\n",
      "Epoch:496         Loss:7.817606367236558\n",
      "Epoch:497         Loss:7.814608905616809\n",
      "Epoch:498         Loss:7.811574395162767\n",
      "Epoch:499         Loss:7.808667261171975\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# Momentum descent\n",
    "\n",
    "# updating the weights\n",
    "\n",
    "\n",
    "r = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss=0\n",
    "    for j in np.arange(0,x_train.shape[1],batch_size):       \n",
    "        dW = np.zeros(W[0].shape)\n",
    "        dM = np.zeros(W[1].shape)\n",
    "        dT = np.zeros(W[2].shape)\n",
    "        db1 = np.zeros(b[0].shape)\n",
    "        db2 = np.zeros(b[1].shape)\n",
    "        db3 = np.zeros(b[2].shape)\n",
    "        for k in range(0,batch_size):\n",
    "            # forward pass\n",
    "            x = x_train[:,j+k].reshape(-1,1)\n",
    "            y = y_train[:,j+k].reshape(-1,1)\n",
    "            out,y_pred=forward_path(noHiddenLayers,x,W,b,Actfnvect)\n",
    "            # backpropagation\n",
    "            dWtemp,dMtemp,dTtemp,db1temp,db2temp,db3temp=backprop(y_pred,y,out[1],W[-1],out[0],W[1],x)\n",
    "            \n",
    "            dW=dW+dWtemp\n",
    "            db1=db1+db1temp\n",
    "            \n",
    "            dM=dM+dMtemp\n",
    "            db2=db2+db2temp\n",
    "            \n",
    "            dT=dT+dTtemp\n",
    "            db3=db3+db3temp\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = loss + (-1.0*np.sum(y*np.log(y_pred)))\n",
    "            \n",
    "        # Updating the weights using Adagrad Approach\n",
    "        dW=dW*(1.0/batch_size)\n",
    "        dM=dM*(1.0/batch_size)\n",
    "        dT=dT*(1.0/batch_size)\n",
    "        db1=db1*(1.0/batch_size)\n",
    "        db2=db2*(1.0/batch_size)\n",
    "        db3=db3*(1.0/batch_size)\n",
    "        \n",
    "        r[\"W[0]\"] = r[\"W[0]\"] + ((dW*dW))\n",
    "        r[\"W[1]\"] = r[\"W[1]\"] + ((dM*dM))\n",
    "        r[\"W[2]\"] = r[\"W[2]\"] + ((dT*dT))\n",
    "        r[\"b[0]\"] = r[\"b[0]\"] + ((db1*db1))\n",
    "        r[\"b[1]\"] = r[\"b[1]\"] + ((db2*db2))\n",
    "        r[\"b[2]\"] = r[\"b[2]\"] + ((db3*db3))\n",
    "        \n",
    "        W[0] = W[0] - ( (learning_rate*dW)/(delta +np.sqrt(r[\"W[0]\"])))\n",
    "        W[1] = W[1] - ( (learning_rate*dM)/(delta +np.sqrt(r[\"W[1]\"])))\n",
    "        W[2] = W[2] - ( (learning_rate*dT)/(delta +np.sqrt(r[\"W[2]\"])))\n",
    "        b[0] = b[0] - ((learning_rate*db1)/(delta +np.sqrt(r[\"b[0]\"])))\n",
    "        b[1] = b[1] - ((learning_rate*db2)/(delta +np.sqrt(r[\"b[1]\"])))\n",
    "        b[2] = b[2] - ((learning_rate*db3)/(delta +np.sqrt(r[\"b[2]\"])))\n",
    "    \n",
    "    #print the loss in each epoch\n",
    "    print('Epoch:'+str(i)+'         Loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,y_pred=forward_path(noHiddenLayers,x_test,W,b,Actfnvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(y):\n",
    "    return np.argmax(y)\n",
    "\n",
    "yvect=[]\n",
    "y_trurevect=[]\n",
    "\n",
    "for i in range(0,x_test.shape[1]):\n",
    "    yvect.append(predict(y_pred[:,i]))\n",
    "    y_trurevect.append(predict(y_test[:,i]))\n",
    "\n",
    "# find accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "#predicting test accuracy\n",
    "print(accuracy_score(y_trurevect, yvect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# to see the output vs true values\n",
    "\n",
    "print y_trurevect\n",
    "print yvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
