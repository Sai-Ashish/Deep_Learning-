{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Note:run in python2######\n",
    "# same initialization of weights because same random seed used\n",
    "\n",
    "#SGD has\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#MLP\n",
    "\n",
    "# initialisation of the weights he_normal\n",
    "def weights(noHiddenLayers,sizeOfLayers):\n",
    "\n",
    "    W=[]\n",
    "    b=[]\n",
    "\n",
    "    for i in range(0,noHiddenLayers+1):\n",
    "        \n",
    "        W.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+sizeOfLayers[i])),\n",
    "                                   (sizeOfLayers[i+1],sizeOfLayers[i])) )\n",
    "        b.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+1)),\n",
    "                                   (sizeOfLayers[i+1],1)) )\n",
    "\n",
    "    W=np.array(W)\n",
    "    b=np.array(b)\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "\n",
    "#mlp forward pass\n",
    "#layer\n",
    "def layer(w,x,b):\n",
    "    out = np.matmul(w,x)+b\n",
    "    return out\n",
    "\n",
    "def apply_activationMLP(Activation_function,inp):\n",
    "    \n",
    "    #activation functions\n",
    "    if Activation_function == 'relu':\n",
    "        return np.where(inp<0,0,inp)\n",
    "    elif Activation_function == \"tanh\":\n",
    "        return np.tanh(inp)\n",
    "    elif Activation_function == \"sigmoid\":\n",
    "        return 1.0/(1+np.exp(-1.0*inp))\n",
    "    elif Activation_function == \"softmax\":\n",
    "        return (1.0/(np.sum(np.exp(inp),axis=0)))*(np.exp(inp))\n",
    "\n",
    "#forward path\n",
    "def forward_path(noHiddenLayers,X,W,b,Actfnvect):\n",
    "\n",
    "    out=[]\n",
    "    \n",
    "    z=apply_activationMLP(Actfnvect[0],np.array(layer(W[0],X,b[0])))\n",
    "    out.append(np.array(z))\n",
    "\n",
    "    for i in range(1,noHiddenLayers):\n",
    "        z=apply_activationMLP(Actfnvect[i],np.array(layer(W[i],out[i-1],b[i])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    if noHiddenLayers > 0:\n",
    "        z=apply_activationMLP(Actfnvect[-1],np.array(layer(W[-1],out[-1],b[-1])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    y_pred = out[-1]\n",
    "\n",
    "    return np.array(out),np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "#only to import data\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# #import data\n",
    "iris_data = load_iris() # load the iris dataset\n",
    "\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "# Split the data for training and testing\n",
    "x_train , x_test, y_train , y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(3, 120)\n",
      "(4, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "# run MLP algorithm\n",
    "\n",
    "x_train = np.moveaxis(x_train,0,-1)\n",
    "y_train = np.moveaxis(y_train,0,-1)\n",
    "x_test = np.moveaxis(x_test,0,-1)\n",
    "y_test = np.moveaxis(y_test,0,-1)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_relu(x):\n",
    "    return np.where(x == 0,0,1)\n",
    "\n",
    "# backpropation\n",
    "def backprop(y,y_true,z,T,u,M,x):\n",
    "    \n",
    "    dy = (y-y_true)\n",
    "    \n",
    "    # layer 3\n",
    "    dT  = np.matmul(dy,z.T)\n",
    "    db3 = np.sum(dy,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 2\n",
    "    s   = np.matmul(T.T,dy)\n",
    "    s   = s*der_relu(z)\n",
    "    dM  = np.matmul(s,u.T)\n",
    "    db2 = np.sum(s,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 1\n",
    "    sm  = np.matmul(M.T,s)\n",
    "    sm  = sm*der_relu(u)\n",
    "    dW  = np.matmul(sm,x.T)\n",
    "    db1 = np.sum(sm,axis=1).reshape(-1,1)\n",
    "    \n",
    "    return dW,dM,dT,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "################################\n",
    "# Training parameters\n",
    "num_classes = 3\n",
    "epochs = 600\n",
    "learning_rate=0.0001\n",
    "batch_size = 10\n",
    "alpha = 0.9\n",
    "################################\n",
    "\n",
    "#MLP PARAMETERS\n",
    "noHiddenLayers=2\n",
    "\n",
    "#also includes the input vector dimension and output vector dimension\n",
    "sizeOfLayers=[x_train.shape[0],10,10,num_classes]\n",
    "\n",
    "sizeofOutput=num_classes\n",
    "\n",
    "Actfnvect = ['relu','relu','softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = weights(noHiddenLayers,sizeOfLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0         Loss:257.79156193871694\n",
      "Epoch:1         Loss:240.90412559731556\n",
      "Epoch:2         Loss:222.69811766059388\n",
      "Epoch:3         Loss:207.4910184586755\n",
      "Epoch:4         Loss:195.5050476557887\n",
      "Epoch:5         Loss:186.02023232454906\n",
      "Epoch:6         Loss:178.38858420714206\n",
      "Epoch:7         Loss:172.05413158964694\n",
      "Epoch:8         Loss:166.61362488404038\n",
      "Epoch:9         Loss:161.8280765073398\n",
      "Epoch:10         Loss:157.5536642874321\n",
      "Epoch:11         Loss:153.7140073719533\n",
      "Epoch:12         Loss:150.37153645401557\n",
      "Epoch:13         Loss:147.36929820382213\n",
      "Epoch:14         Loss:144.55448733559143\n",
      "Epoch:15         Loss:141.69241160108237\n",
      "Epoch:16         Loss:138.65085032406907\n",
      "Epoch:17         Loss:136.0144766046348\n",
      "Epoch:18         Loss:133.6901196716816\n",
      "Epoch:19         Loss:131.49217324561207\n",
      "Epoch:20         Loss:129.35582962309476\n",
      "Epoch:21         Loss:127.24839858599891\n",
      "Epoch:22         Loss:125.15234246712328\n",
      "Epoch:23         Loss:123.07183441223826\n",
      "Epoch:24         Loss:120.98995019877056\n",
      "Epoch:25         Loss:118.83682733036139\n",
      "Epoch:26         Loss:116.34497548908999\n",
      "Epoch:27         Loss:113.37191006133577\n",
      "Epoch:28         Loss:110.12298365258452\n",
      "Epoch:29         Loss:106.95909202601224\n",
      "Epoch:30         Loss:103.9567986398602\n",
      "Epoch:31         Loss:101.21777284183118\n",
      "Epoch:32         Loss:98.88402343428534\n",
      "Epoch:33         Loss:96.9454090544444\n",
      "Epoch:34         Loss:95.20743811247117\n",
      "Epoch:35         Loss:93.58623662868176\n",
      "Epoch:36         Loss:92.04830818662649\n",
      "Epoch:37         Loss:90.58273998641587\n",
      "Epoch:38         Loss:89.1895618715154\n",
      "Epoch:39         Loss:87.87817681884613\n",
      "Epoch:40         Loss:86.66239159914517\n",
      "Epoch:41         Loss:85.52784196218082\n",
      "Epoch:42         Loss:84.45533516353998\n",
      "Epoch:43         Loss:83.44638919727937\n",
      "Epoch:44         Loss:82.494603729982\n",
      "Epoch:45         Loss:81.61302240705861\n",
      "Epoch:46         Loss:80.78099145629756\n",
      "Epoch:47         Loss:80.0007230121321\n",
      "Epoch:48         Loss:79.26930934447951\n",
      "Epoch:49         Loss:78.57784611761782\n",
      "Epoch:50         Loss:77.9146484130519\n",
      "Epoch:51         Loss:77.27594061340699\n",
      "Epoch:52         Loss:76.6537209855878\n",
      "Epoch:53         Loss:76.0445358826875\n",
      "Epoch:54         Loss:75.45549016096211\n",
      "Epoch:55         Loss:74.88516493153496\n",
      "Epoch:56         Loss:74.33370968310103\n",
      "Epoch:57         Loss:73.80033208383408\n",
      "Epoch:58         Loss:73.28402647151024\n",
      "Epoch:59         Loss:72.78390893791692\n",
      "Epoch:60         Loss:72.29975086305154\n",
      "Epoch:61         Loss:71.82510467677362\n",
      "Epoch:62         Loss:71.36162755877173\n",
      "Epoch:63         Loss:70.90619633977752\n",
      "Epoch:64         Loss:70.45362121376934\n",
      "Epoch:65         Loss:70.00049251267451\n",
      "Epoch:66         Loss:69.551961123752\n",
      "Epoch:67         Loss:69.10780285027579\n",
      "Epoch:68         Loss:68.66645660001508\n",
      "Epoch:69         Loss:68.22660176477483\n",
      "Epoch:70         Loss:67.78244326354341\n",
      "Epoch:71         Loss:67.33199482229064\n",
      "Epoch:72         Loss:66.88123852347147\n",
      "Epoch:73         Loss:66.43021101474596\n",
      "Epoch:74         Loss:65.9768194970126\n",
      "Epoch:75         Loss:65.52172262511758\n",
      "Epoch:76         Loss:65.07217275459037\n",
      "Epoch:77         Loss:64.62466581169812\n",
      "Epoch:78         Loss:64.18009971162071\n",
      "Epoch:79         Loss:63.740100706708\n",
      "Epoch:80         Loss:63.30993477078333\n",
      "Epoch:81         Loss:62.88545017725648\n",
      "Epoch:82         Loss:62.47021519345441\n",
      "Epoch:83         Loss:62.06273885392997\n",
      "Epoch:84         Loss:61.6640921837397\n",
      "Epoch:85         Loss:61.2747519074291\n",
      "Epoch:86         Loss:60.894749370212516\n",
      "Epoch:87         Loss:60.51848243344727\n",
      "Epoch:88         Loss:60.14611483872705\n",
      "Epoch:89         Loss:59.777735627452195\n",
      "Epoch:90         Loss:59.403646452765535\n",
      "Epoch:91         Loss:59.0347651762013\n",
      "Epoch:92         Loss:58.676598915341245\n",
      "Epoch:93         Loss:58.3254529578339\n",
      "Epoch:94         Loss:57.98590242270663\n",
      "Epoch:95         Loss:57.64940737906324\n",
      "Epoch:96         Loss:57.32026451697538\n",
      "Epoch:97         Loss:56.995420844047835\n",
      "Epoch:98         Loss:56.67417941294332\n",
      "Epoch:99         Loss:56.35840597099174\n",
      "Epoch:100         Loss:56.046926603333176\n",
      "Epoch:101         Loss:55.74514057645914\n",
      "Epoch:102         Loss:55.44420544911917\n",
      "Epoch:103         Loss:55.14880442229695\n",
      "Epoch:104         Loss:54.85194883514567\n",
      "Epoch:105         Loss:54.558515453524855\n",
      "Epoch:106         Loss:54.26606395314722\n",
      "Epoch:107         Loss:53.97486537857429\n",
      "Epoch:108         Loss:53.68824097378027\n",
      "Epoch:109         Loss:53.39950392645679\n",
      "Epoch:110         Loss:53.10844778541143\n",
      "Epoch:111         Loss:52.819779586240095\n",
      "Epoch:112         Loss:52.53598734109231\n",
      "Epoch:113         Loss:52.25455944270433\n",
      "Epoch:114         Loss:51.972481703568896\n",
      "Epoch:115         Loss:51.69322586827844\n",
      "Epoch:116         Loss:51.416487912055196\n",
      "Epoch:117         Loss:51.14441463407726\n",
      "Epoch:118         Loss:50.87500870997306\n",
      "Epoch:119         Loss:50.60852246460334\n",
      "Epoch:120         Loss:50.339916948314546\n",
      "Epoch:121         Loss:50.07312094971634\n",
      "Epoch:122         Loss:49.81091661021448\n",
      "Epoch:123         Loss:49.54706008129905\n",
      "Epoch:124         Loss:49.28758087477779\n",
      "Epoch:125         Loss:49.03306823067042\n",
      "Epoch:126         Loss:48.78278821580451\n",
      "Epoch:127         Loss:48.5359046372767\n",
      "Epoch:128         Loss:48.29241537368735\n",
      "Epoch:129         Loss:48.05324709788695\n",
      "Epoch:130         Loss:47.818794368856935\n",
      "Epoch:131         Loss:47.587351282024265\n",
      "Epoch:132         Loss:47.35766078794437\n",
      "Epoch:133         Loss:47.129031109473026\n",
      "Epoch:134         Loss:46.9023778455895\n",
      "Epoch:135         Loss:46.68080343051651\n",
      "Epoch:136         Loss:46.46296333809842\n",
      "Epoch:137         Loss:46.24813279149804\n",
      "Epoch:138         Loss:46.03628003887659\n",
      "Epoch:139         Loss:45.827649883974345\n",
      "Epoch:140         Loss:45.62099235357437\n",
      "Epoch:141         Loss:45.41554385830151\n",
      "Epoch:142         Loss:45.21174069860395\n",
      "Epoch:143         Loss:45.00999664859735\n",
      "Epoch:144         Loss:44.81075714675256\n",
      "Epoch:145         Loss:44.61509669067341\n",
      "Epoch:146         Loss:44.42054459928893\n",
      "Epoch:147         Loss:44.22668545065172\n",
      "Epoch:148         Loss:44.03458629017746\n",
      "Epoch:149         Loss:43.84359241494211\n",
      "Epoch:150         Loss:43.65476277538936\n",
      "Epoch:151         Loss:43.46742762519061\n",
      "Epoch:152         Loss:43.28208486653638\n",
      "Epoch:153         Loss:43.09804710028182\n",
      "Epoch:154         Loss:42.915662804384176\n",
      "Epoch:155         Loss:42.734765560344805\n",
      "Epoch:156         Loss:42.55517131624169\n",
      "Epoch:157         Loss:42.376550214388914\n",
      "Epoch:158         Loss:42.199120189428584\n",
      "Epoch:159         Loss:42.0231975516397\n",
      "Epoch:160         Loss:41.84875974827153\n",
      "Epoch:161         Loss:41.67606668395674\n",
      "Epoch:162         Loss:41.504018257287704\n",
      "Epoch:163         Loss:41.33151656670233\n",
      "Epoch:164         Loss:41.16152323795279\n",
      "Epoch:165         Loss:40.993092538028826\n",
      "Epoch:166         Loss:40.82625762184264\n",
      "Epoch:167         Loss:40.65856483061149\n",
      "Epoch:168         Loss:40.49199307085836\n",
      "Epoch:169         Loss:40.32772307584256\n",
      "Epoch:170         Loss:40.16470588059781\n",
      "Epoch:171         Loss:40.00306306204217\n",
      "Epoch:172         Loss:39.84159923586139\n",
      "Epoch:173         Loss:39.68000645282233\n",
      "Epoch:174         Loss:39.520226788957956\n",
      "Epoch:175         Loss:39.36157347834672\n",
      "Epoch:176         Loss:39.203766252276246\n",
      "Epoch:177         Loss:39.04682221158481\n",
      "Epoch:178         Loss:38.89024043691608\n",
      "Epoch:179         Loss:38.735116390205576\n",
      "Epoch:180         Loss:38.58046465446755\n",
      "Epoch:181         Loss:38.428028382015626\n",
      "Epoch:182         Loss:38.27577258034302\n",
      "Epoch:183         Loss:38.123127776899736\n",
      "Epoch:184         Loss:37.972410016466924\n",
      "Epoch:185         Loss:37.82279116586924\n",
      "Epoch:186         Loss:37.67386435239977\n",
      "Epoch:187         Loss:37.52586286476641\n",
      "Epoch:188         Loss:37.37791005401756\n",
      "Epoch:189         Loss:37.23227800766307\n",
      "Epoch:190         Loss:37.08566449684827\n",
      "Epoch:191         Loss:36.940042433766166\n",
      "Epoch:192         Loss:36.79510535366282\n",
      "Epoch:193         Loss:36.65214486956765\n",
      "Epoch:194         Loss:36.50894838148914\n",
      "Epoch:195         Loss:36.36717285670982\n",
      "Epoch:196         Loss:36.225841910293546\n",
      "Epoch:197         Loss:36.08516618359066\n",
      "Epoch:198         Loss:35.94580612949635\n",
      "Epoch:199         Loss:35.80658013737905\n",
      "Epoch:200         Loss:35.66891124346507\n",
      "Epoch:201         Loss:35.531560313561066\n",
      "Epoch:202         Loss:35.394349196525184\n",
      "Epoch:203         Loss:35.2584271126312\n",
      "Epoch:204         Loss:35.12313889290326\n",
      "Epoch:205         Loss:34.98820677330171\n",
      "Epoch:206         Loss:34.85495284537831\n",
      "Epoch:207         Loss:34.72202337080043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:208         Loss:34.589217654215034\n",
      "Epoch:209         Loss:34.457676467429046\n",
      "Epoch:210         Loss:34.32699737606945\n",
      "Epoch:211         Loss:34.1959291062826\n",
      "Epoch:212         Loss:34.06625429321151\n",
      "Epoch:213         Loss:33.937544428558375\n",
      "Epoch:214         Loss:33.80967001896952\n",
      "Epoch:215         Loss:33.68312756736829\n",
      "Epoch:216         Loss:33.55619897451095\n",
      "Epoch:217         Loss:33.43051812085814\n",
      "Epoch:218         Loss:33.30478593397724\n",
      "Epoch:219         Loss:33.17996131482233\n",
      "Epoch:220         Loss:33.05611277483098\n",
      "Epoch:221         Loss:32.9324507580483\n",
      "Epoch:222         Loss:32.80997417451107\n",
      "Epoch:223         Loss:32.68732146358726\n",
      "Epoch:224         Loss:32.565638404469155\n",
      "Epoch:225         Loss:32.445127646416616\n",
      "Epoch:226         Loss:32.32441065832353\n",
      "Epoch:227         Loss:32.204671565496156\n",
      "Epoch:228         Loss:32.08600080043926\n",
      "Epoch:229         Loss:31.967107209277142\n",
      "Epoch:230         Loss:31.849091757715065\n",
      "Epoch:231         Loss:31.732435117721\n",
      "Epoch:232         Loss:31.615428722288847\n",
      "Epoch:233         Loss:31.49942503784424\n",
      "Epoch:234         Loss:31.383650724954187\n",
      "Epoch:235         Loss:31.269056893690077\n",
      "Epoch:236         Loss:31.154070578581766\n",
      "Epoch:237         Loss:31.040404186315815\n",
      "Epoch:238         Loss:30.927220009392165\n",
      "Epoch:239         Loss:30.814344486802998\n",
      "Epoch:240         Loss:30.702334812180563\n",
      "Epoch:241         Loss:30.590589355320375\n",
      "Epoch:242         Loss:30.479867882706095\n",
      "Epoch:243         Loss:30.36888383081137\n",
      "Epoch:244         Loss:30.258800436768645\n",
      "Epoch:245         Loss:30.149718180504525\n",
      "Epoch:246         Loss:30.040301519276593\n",
      "Epoch:247         Loss:29.93208286731756\n",
      "Epoch:248         Loss:29.824304989502075\n",
      "Epoch:249         Loss:29.71689686571998\n",
      "Epoch:250         Loss:29.6100515616698\n",
      "Epoch:251         Loss:29.503737321066993\n",
      "Epoch:252         Loss:29.397963819500557\n",
      "Epoch:253         Loss:29.292378800624448\n",
      "Epoch:254         Loss:29.187944495617977\n",
      "Epoch:255         Loss:29.08321696567867\n",
      "Epoch:256         Loss:28.979725949896316\n",
      "Epoch:257         Loss:28.87600734174415\n",
      "Epoch:258         Loss:28.773505062326873\n",
      "Epoch:259         Loss:28.670702750930598\n",
      "Epoch:260         Loss:28.569301990802913\n",
      "Epoch:261         Loss:28.467871363618734\n",
      "Epoch:262         Loss:28.36704434586162\n",
      "Epoch:263         Loss:28.266946888788453\n",
      "Epoch:264         Loss:28.16679998600578\n",
      "Epoch:265         Loss:28.067309416985\n",
      "Epoch:266         Loss:27.968578929767727\n",
      "Epoch:267         Loss:27.869848584800238\n",
      "Epoch:268         Loss:27.771751890010446\n",
      "Epoch:269         Loss:27.674404724976913\n",
      "Epoch:270         Loss:27.57672123179904\n",
      "Epoch:271         Loss:27.480487289244465\n",
      "Epoch:272         Loss:27.384496391609375\n",
      "Epoch:273         Loss:27.288028773265307\n",
      "Epoch:274         Loss:27.193037996922506\n",
      "Epoch:275         Loss:27.0981024020838\n",
      "Epoch:276         Loss:27.003788604667417\n",
      "Epoch:277         Loss:26.909823070963334\n",
      "Epoch:278         Loss:26.815807762936767\n",
      "Epoch:279         Loss:26.72290809482764\n",
      "Epoch:280         Loss:26.630096675304884\n",
      "Epoch:281         Loss:26.538197895022325\n",
      "Epoch:282         Loss:26.44615170469764\n",
      "Epoch:283         Loss:26.354427567828253\n",
      "Epoch:284         Loss:26.263640990028517\n",
      "Epoch:285         Loss:26.173515552238108\n",
      "Epoch:286         Loss:26.083192236844347\n",
      "Epoch:287         Loss:25.993183641069216\n",
      "Epoch:288         Loss:25.904322004187485\n",
      "Epoch:289         Loss:25.815226699928346\n",
      "Epoch:290         Loss:25.726907068054935\n",
      "Epoch:291         Loss:25.639500758541992\n",
      "Epoch:292         Loss:25.551971570856132\n",
      "Epoch:293         Loss:25.464339174376157\n",
      "Epoch:294         Loss:25.377988037542917\n",
      "Epoch:295         Loss:25.2921926778973\n",
      "Epoch:296         Loss:25.206036835764376\n",
      "Epoch:297         Loss:25.120743117592784\n",
      "Epoch:298         Loss:25.03624942714917\n",
      "Epoch:299         Loss:24.95155136465483\n",
      "Epoch:300         Loss:24.867911912867225\n",
      "Epoch:301         Loss:24.784163064131\n",
      "Epoch:302         Loss:24.701330655974704\n",
      "Epoch:303         Loss:24.619080106277185\n",
      "Epoch:304         Loss:24.536727392721964\n",
      "Epoch:305         Loss:24.45541891644728\n",
      "Epoch:306         Loss:24.374931069696718\n",
      "Epoch:307         Loss:24.294140454245944\n",
      "Epoch:308         Loss:24.21421579211304\n",
      "Epoch:309         Loss:24.13502184773862\n",
      "Epoch:310         Loss:24.055701700106876\n",
      "Epoch:311         Loss:23.97705626025829\n",
      "Epoch:312         Loss:23.899343132155607\n",
      "Epoch:313         Loss:23.8213547147944\n",
      "Epoch:314         Loss:23.744088525706783\n",
      "Epoch:315         Loss:23.667364088902975\n",
      "Epoch:316         Loss:23.59108772837057\n",
      "Epoch:317         Loss:23.51493938866273\n",
      "Epoch:318         Loss:23.43950588108158\n",
      "Epoch:319         Loss:23.365325206475717\n",
      "Epoch:320         Loss:23.29102199841356\n",
      "Epoch:321         Loss:23.217005550066904\n",
      "Epoch:322         Loss:23.14354640186529\n",
      "Epoch:323         Loss:23.070876469255555\n",
      "Epoch:324         Loss:22.998186692367344\n",
      "Epoch:325         Loss:22.92622921195246\n",
      "Epoch:326         Loss:22.85472669873632\n",
      "Epoch:327         Loss:22.783584209643813\n",
      "Epoch:328         Loss:22.712957002814914\n",
      "Epoch:329         Loss:22.642290514727193\n",
      "Epoch:330         Loss:22.572312709242038\n",
      "Epoch:331         Loss:22.502794692022064\n",
      "Epoch:332         Loss:22.433720050600346\n",
      "Epoch:333         Loss:22.36502105449835\n",
      "Epoch:334         Loss:22.296713176772208\n",
      "Epoch:335         Loss:22.22879960729411\n",
      "Epoch:336         Loss:22.161325915290142\n",
      "Epoch:337         Loss:22.094159777895406\n",
      "Epoch:338         Loss:22.02743323092715\n",
      "Epoch:339         Loss:21.961097585017036\n",
      "Epoch:340         Loss:21.89549626077056\n",
      "Epoch:341         Loss:21.83015881767646\n",
      "Epoch:342         Loss:21.76507387770316\n",
      "Epoch:343         Loss:21.70039433565691\n",
      "Epoch:344         Loss:21.63614852337467\n",
      "Epoch:345         Loss:21.572309115241424\n",
      "Epoch:346         Loss:21.508847889530205\n",
      "Epoch:347         Loss:21.445812961198843\n",
      "Epoch:348         Loss:21.382541630479974\n",
      "Epoch:349         Loss:21.32042241431056\n",
      "Epoch:350         Loss:21.258765461537983\n",
      "Epoch:351         Loss:21.197355227734448\n",
      "Epoch:352         Loss:21.13612227827059\n",
      "Epoch:353         Loss:21.07531970643579\n",
      "Epoch:354         Loss:21.014975491444805\n",
      "Epoch:355         Loss:20.954785653145702\n",
      "Epoch:356         Loss:20.895066587299578\n",
      "Epoch:357         Loss:20.835881253615746\n",
      "Epoch:358         Loss:20.77685957853539\n",
      "Epoch:359         Loss:20.718365014656293\n",
      "Epoch:360         Loss:20.660176499493566\n",
      "Epoch:361         Loss:20.60222094837312\n",
      "Epoch:362         Loss:20.544712497610718\n",
      "Epoch:363         Loss:20.487423305170577\n",
      "Epoch:364         Loss:20.430453529625915\n",
      "Epoch:365         Loss:20.3740115544503\n",
      "Epoch:366         Loss:20.31798640965438\n",
      "Epoch:367         Loss:20.26207165002582\n",
      "Epoch:368         Loss:20.206387675157643\n",
      "Epoch:369         Loss:20.151169554500143\n",
      "Epoch:370         Loss:20.09654401341282\n",
      "Epoch:371         Loss:20.041905892207197\n",
      "Epoch:372         Loss:19.98758608729047\n",
      "Epoch:373         Loss:19.933849766259943\n",
      "Epoch:374         Loss:19.88014509565273\n",
      "Epoch:375         Loss:19.82674935669754\n",
      "Epoch:376         Loss:19.773971767598\n",
      "Epoch:377         Loss:19.72121663237552\n",
      "Epoch:378         Loss:19.6687840617204\n",
      "Epoch:379         Loss:19.616899441655022\n",
      "Epoch:380         Loss:19.56502369258765\n",
      "Epoch:381         Loss:19.513404491371777\n",
      "Epoch:382         Loss:19.462517714127454\n",
      "Epoch:383         Loss:19.41149850390293\n",
      "Epoch:384         Loss:19.360877182662623\n",
      "Epoch:385         Loss:19.310483483499304\n",
      "Epoch:386         Loss:19.260778302241174\n",
      "Epoch:387         Loss:19.21092986644098\n",
      "Epoch:388         Loss:19.161368538656205\n",
      "Epoch:389         Loss:19.112338636574894\n",
      "Epoch:390         Loss:19.063238078279223\n",
      "Epoch:391         Loss:19.015197686176645\n",
      "Epoch:392         Loss:18.967597464118942\n",
      "Epoch:393         Loss:18.92019411676089\n",
      "Epoch:394         Loss:18.873078048115197\n",
      "Epoch:395         Loss:18.82631697884217\n",
      "Epoch:396         Loss:18.779659989435114\n",
      "Epoch:397         Loss:18.733343637569828\n",
      "Epoch:398         Loss:18.68718128422006\n",
      "Epoch:399         Loss:18.64138595435784\n",
      "Epoch:400         Loss:18.59575523400198\n",
      "Epoch:401         Loss:18.55047426677614\n",
      "Epoch:402         Loss:18.505356254857066\n",
      "Epoch:403         Loss:18.46058562804704\n",
      "Epoch:404         Loss:18.415993598035378\n",
      "Epoch:405         Loss:18.371765323135683\n",
      "Epoch:406         Loss:18.327530065543453\n",
      "Epoch:407         Loss:18.283850490517384\n",
      "Epoch:408         Loss:18.240547578631894\n",
      "Epoch:409         Loss:18.19714629652027\n",
      "Epoch:410         Loss:18.154084769714427\n",
      "Epoch:411         Loss:18.111209618655337\n",
      "Epoch:412         Loss:18.06882666869074\n",
      "Epoch:413         Loss:18.026410569106634\n",
      "Epoch:414         Loss:17.9843219559454\n",
      "Epoch:415         Loss:17.942476823856687\n",
      "Epoch:416         Loss:17.900761941363594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:417         Loss:17.85957716632973\n",
      "Epoch:418         Loss:17.81828404200489\n",
      "Epoch:419         Loss:17.777339286116707\n",
      "Epoch:420         Loss:17.736658474192186\n",
      "Epoch:421         Loss:17.696217568023496\n",
      "Epoch:422         Loss:17.655969473694125\n",
      "Epoch:423         Loss:17.615934416082446\n",
      "Epoch:424         Loss:17.576132257220745\n",
      "Epoch:425         Loss:17.536453589897807\n",
      "Epoch:426         Loss:17.497185438759082\n",
      "Epoch:427         Loss:17.458126628632513\n",
      "Epoch:428         Loss:17.41903441713444\n",
      "Epoch:429         Loss:17.380220237044107\n",
      "Epoch:430         Loss:17.341846010037667\n",
      "Epoch:431         Loss:17.30366715643588\n",
      "Epoch:432         Loss:17.265406205029883\n",
      "Epoch:433         Loss:17.227545321338624\n",
      "Epoch:434         Loss:17.190070523629938\n",
      "Epoch:435         Loss:17.152507488903638\n",
      "Epoch:436         Loss:17.115222144777036\n",
      "Epoch:437         Loss:17.0782343242016\n",
      "Epoch:438         Loss:17.041395544285862\n",
      "Epoch:439         Loss:17.004887293430414\n",
      "Epoch:440         Loss:16.968316067721474\n",
      "Epoch:441         Loss:16.93232422256095\n",
      "Epoch:442         Loss:16.896652139729223\n",
      "Epoch:443         Loss:16.86126325599869\n",
      "Epoch:444         Loss:16.826047614876078\n",
      "Epoch:445         Loss:16.790812002001935\n",
      "Epoch:446         Loss:16.75584574159215\n",
      "Epoch:447         Loss:16.72112084801326\n",
      "Epoch:448         Loss:16.686567349004363\n",
      "Epoch:449         Loss:16.652144018790107\n",
      "Epoch:450         Loss:16.617926425537703\n",
      "Epoch:451         Loss:16.58390781418928\n",
      "Epoch:452         Loss:16.550094969194873\n",
      "Epoch:453         Loss:16.516369416968917\n",
      "Epoch:454         Loss:16.48300718105529\n",
      "Epoch:455         Loss:16.449602354118962\n",
      "Epoch:456         Loss:16.416389494182333\n",
      "Epoch:457         Loss:16.38354973324081\n",
      "Epoch:458         Loss:16.350794324670854\n",
      "Epoch:459         Loss:16.31813197736423\n",
      "Epoch:460         Loss:16.285670109266334\n",
      "Epoch:461         Loss:16.253341964248424\n",
      "Epoch:462         Loss:16.22138551857488\n",
      "Epoch:463         Loss:16.18936992956152\n",
      "Epoch:464         Loss:16.157546392577178\n",
      "Epoch:465         Loss:16.12602404194386\n",
      "Epoch:466         Loss:16.094610140118768\n",
      "Epoch:467         Loss:16.063359754467864\n",
      "Epoch:468         Loss:16.03223027893618\n",
      "Epoch:469         Loss:16.00122085930443\n",
      "Epoch:470         Loss:15.970553087732737\n",
      "Epoch:471         Loss:15.939915860918173\n",
      "Epoch:472         Loss:15.909378847475853\n",
      "Epoch:473         Loss:15.879182554499337\n",
      "Epoch:474         Loss:15.84901409669253\n",
      "Epoch:475         Loss:15.819017803628025\n",
      "Epoch:476         Loss:15.789139106756238\n",
      "Epoch:477         Loss:15.759510114720882\n",
      "Epoch:478         Loss:15.729953236781675\n",
      "Epoch:479         Loss:15.700592751247338\n",
      "Epoch:480         Loss:15.671349786062244\n",
      "Epoch:481         Loss:15.642347614581933\n",
      "Epoch:482         Loss:15.613365346995906\n",
      "Epoch:483         Loss:15.584700425797969\n",
      "Epoch:484         Loss:15.55604191463675\n",
      "Epoch:485         Loss:15.527515199673035\n",
      "Epoch:486         Loss:15.499123214117917\n",
      "Epoch:487         Loss:15.471071830839671\n",
      "Epoch:488         Loss:15.443013051809872\n",
      "Epoch:489         Loss:15.415067255395282\n",
      "Epoch:490         Loss:15.387262717566761\n",
      "Epoch:491         Loss:15.359747564225616\n",
      "Epoch:492         Loss:15.332424081155938\n",
      "Epoch:493         Loss:15.305187605227724\n",
      "Epoch:494         Loss:15.277945000908703\n",
      "Epoch:495         Loss:15.250852429311035\n",
      "Epoch:496         Loss:15.223945718620234\n",
      "Epoch:497         Loss:15.197193664618295\n",
      "Epoch:498         Loss:15.170576645648248\n",
      "Epoch:499         Loss:15.144088614016962\n",
      "Epoch:500         Loss:15.117730660870421\n",
      "Epoch:501         Loss:15.091504602037347\n",
      "Epoch:502         Loss:15.065410721739974\n",
      "Epoch:503         Loss:15.03948110281625\n",
      "Epoch:504         Loss:15.013644858131359\n",
      "Epoch:505         Loss:14.987926092568115\n",
      "Epoch:506         Loss:14.962346497910563\n",
      "Epoch:507         Loss:14.936905570462905\n",
      "Epoch:508         Loss:14.911600151406995\n",
      "Epoch:509         Loss:14.886419224994803\n",
      "Epoch:510         Loss:14.861360998030422\n",
      "Epoch:511         Loss:14.836426042729267\n",
      "Epoch:512         Loss:14.811614662216801\n",
      "Epoch:513         Loss:14.786926364820998\n",
      "Epoch:514         Loss:14.762360247567505\n",
      "Epoch:515         Loss:14.737915378113303\n",
      "Epoch:516         Loss:14.713590925739192\n",
      "Epoch:517         Loss:14.68938613594908\n",
      "Epoch:518         Loss:14.665300277412852\n",
      "Epoch:519         Loss:14.641332615579602\n",
      "Epoch:520         Loss:14.617482410916422\n",
      "Epoch:521         Loss:14.593748925181252\n",
      "Epoch:522         Loss:14.57013142583868\n",
      "Epoch:523         Loss:14.546629187047413\n",
      "Epoch:524         Loss:14.523241489009921\n",
      "Epoch:525         Loss:14.499967617254349\n",
      "Epoch:526         Loss:14.476806862335009\n",
      "Epoch:527         Loss:14.45375851981614\n",
      "Epoch:528         Loss:14.430818333907867\n",
      "Epoch:529         Loss:14.407995555510457\n",
      "Epoch:530         Loss:14.38528434451846\n",
      "Epoch:531         Loss:14.36268230176767\n",
      "Epoch:532         Loss:14.340188759095446\n",
      "Epoch:533         Loss:14.317803376639741\n",
      "Epoch:534         Loss:14.29552564858762\n",
      "Epoch:535         Loss:14.27335490713066\n",
      "Epoch:536         Loss:14.251290441212257\n",
      "Epoch:537         Loss:14.22933156372419\n",
      "Epoch:538         Loss:14.207477618575444\n",
      "Epoch:539         Loss:14.185727965693085\n",
      "Epoch:540         Loss:14.16408196941042\n",
      "Epoch:541         Loss:14.142538995304221\n",
      "Epoch:542         Loss:14.12109841143333\n",
      "Epoch:543         Loss:14.099759589982332\n",
      "Epoch:544         Loss:14.078521907925278\n",
      "Epoch:545         Loss:14.057384746958986\n",
      "Epoch:546         Loss:14.036347493252338\n",
      "Epoch:547         Loss:14.015409537285347\n",
      "Epoch:548         Loss:13.994570273796649\n",
      "Epoch:549         Loss:13.973829101774449\n",
      "Epoch:550         Loss:13.953185199177675\n",
      "Epoch:551         Loss:13.932637883667892\n",
      "Epoch:552         Loss:13.91218695232241\n",
      "Epoch:553         Loss:13.891831840625809\n",
      "Epoch:554         Loss:13.871571916913787\n",
      "Epoch:555         Loss:13.851406570941162\n",
      "Epoch:556         Loss:13.831335224659083\n",
      "Epoch:557         Loss:13.81135731608821\n",
      "Epoch:558         Loss:13.791469947801417\n",
      "Epoch:559         Loss:13.771682682878355\n",
      "Epoch:560         Loss:13.751985146390496\n",
      "Epoch:561         Loss:13.73238418134707\n",
      "Epoch:562         Loss:13.712874454909768\n",
      "Epoch:563         Loss:13.693453796884288\n",
      "Epoch:564         Loss:13.674121983985248\n",
      "Epoch:565         Loss:13.654879030591369\n",
      "Epoch:566         Loss:13.635724631558636\n",
      "Epoch:567         Loss:13.61665652382588\n",
      "Epoch:568         Loss:13.597682147923479\n",
      "Epoch:569         Loss:13.578795240454\n",
      "Epoch:570         Loss:13.559993662897964\n",
      "Epoch:571         Loss:13.541277188097295\n",
      "Epoch:572         Loss:13.52264585706587\n",
      "Epoch:573         Loss:13.504099405288839\n",
      "Epoch:574         Loss:13.485637320220883\n",
      "Epoch:575         Loss:13.467259026283662\n",
      "Epoch:576         Loss:13.448963981637124\n",
      "Epoch:577         Loss:13.430751686070074\n",
      "Epoch:578         Loss:13.412621658434363\n",
      "Epoch:579         Loss:13.394573420110447\n",
      "Epoch:580         Loss:13.376606490902637\n",
      "Epoch:581         Loss:13.35872039108346\n",
      "Epoch:582         Loss:13.3409146438188\n",
      "Epoch:583         Loss:13.323188776103422\n",
      "Epoch:584         Loss:13.305542318651714\n",
      "Epoch:585         Loss:13.2879748055497\n",
      "Epoch:586         Loss:13.270485774050632\n",
      "Epoch:587         Loss:13.253074764528499\n",
      "Epoch:588         Loss:13.235741320490826\n",
      "Epoch:589         Loss:13.218484988584247\n",
      "Epoch:590         Loss:13.20129940634814\n",
      "Epoch:591         Loss:13.184200492608447\n",
      "Epoch:592         Loss:13.16717712359091\n",
      "Epoch:593         Loss:13.150228128817478\n",
      "Epoch:594         Loss:13.133353539413642\n",
      "Epoch:595         Loss:13.116723801150863\n",
      "Epoch:596         Loss:13.09975651283567\n",
      "Epoch:597         Loss:13.08330196470132\n",
      "Epoch:598         Loss:13.066522547947027\n",
      "Epoch:599         Loss:13.050229120013118\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# Momentum descent\n",
    "\n",
    "# updating the weights\n",
    "\n",
    "\n",
    "v = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss=0\n",
    "    for j in np.arange(0,x_train.shape[1],batch_size):       \n",
    "        dW = np.zeros(W[0].shape)\n",
    "        dM = np.zeros(W[1].shape)\n",
    "        dT = np.zeros(W[2].shape)\n",
    "        db1 = np.zeros(b[0].shape)\n",
    "        db2 = np.zeros(b[1].shape)\n",
    "        db3 = np.zeros(b[2].shape)\n",
    "        for k in range(0,batch_size):\n",
    "            # forward pass\n",
    "            x = x_train[:,j+k].reshape(-1,1)\n",
    "            y = y_train[:,j+k].reshape(-1,1)\n",
    "            out,y_pred=forward_path(noHiddenLayers,x,W,b,Actfnvect)\n",
    "            # backpropagation\n",
    "            dWtemp,dMtemp,dTtemp,db1temp,db2temp,db3temp=backprop(y_pred,y,out[1],W[-1],out[0],W[1],x)\n",
    "            \n",
    "            dW=dW+dWtemp\n",
    "            db1=db1+db1temp\n",
    "            \n",
    "            dM=dM+dMtemp\n",
    "            db2=db2+db2temp\n",
    "            \n",
    "            dT=dT+dTtemp\n",
    "            db3=db3+db3temp\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = loss + (-1.0*np.sum(y*np.log(y_pred)))\n",
    "            \n",
    "        # Updating the weights using MOMENTUM Approach\n",
    "        \n",
    "        dW=dW*(1.0/batch_size)\n",
    "        dM=dM*(1.0/batch_size)\n",
    "        dT=dT*(1.0/batch_size)\n",
    "        db1=db1*(1.0/batch_size)\n",
    "        db2=db2*(1.0/batch_size)\n",
    "        db3=db3*(1.0/batch_size)\n",
    "        \n",
    "        v[\"W[0]\"] = alpha*v[\"W[0]\"] - learning_rate*(dW)\n",
    "        v[\"W[1]\"] = alpha*v[\"W[1]\"] - learning_rate*(dM)\n",
    "        v[\"W[2]\"] = alpha*v[\"W[2]\"] - learning_rate*(dT)\n",
    "        v[\"b[0]\"] = alpha*v[\"b[0]\"] - learning_rate*(db1)\n",
    "        v[\"b[1]\"] = alpha*v[\"b[1]\"] - learning_rate*(db2)\n",
    "        v[\"b[2]\"] = alpha*v[\"b[2]\"] - learning_rate*(db3)\n",
    "        \n",
    "        W[0] = W[0] + v[\"W[0]\"]\n",
    "        W[1] = W[1] + v[\"W[1]\"]\n",
    "        W[2] = W[2] + v[\"W[2]\"]\n",
    "        b[0] = b[0] + v[\"b[0]\"]\n",
    "        b[1] = b[1] + v[\"b[1]\"]\n",
    "        b[2] = b[2] + v[\"b[2]\"]\n",
    "        \n",
    "    \n",
    "    #print the loss in each epoch\n",
    "    print('Epoch:'+str(i)+'         Loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,y_pred=forward_path(noHiddenLayers,x_test,W,b,Actfnvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(y):\n",
    "    return np.argmax(y)\n",
    "\n",
    "yvect=[]\n",
    "y_trurevect=[]\n",
    "\n",
    "for i in range(0,x_test.shape[1]):\n",
    "    yvect.append(predict(y_pred[:,i]))\n",
    "    y_trurevect.append(predict(y_test[:,i]))\n",
    "\n",
    "# find accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "#predicting test accuracy\n",
    "print(accuracy_score(y_trurevect, yvect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# to see the output vs true values\n",
    "\n",
    "print y_trurevect\n",
    "print yvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
