{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Note:run in python2######\n",
    "# same initialization of weights because same random seed used\n",
    "\n",
    "# OBSERVATIONS\n",
    "# The RMSProp modiﬁes AdaGrad to perform better in the nonconvex setting by \n",
    "# changing the gradient accumulation into an exponentially weighted moving average.\n",
    "\n",
    "# RMSProp uses an exponentially decaying average to discard history from the extreme past so that\n",
    "# it can converge rapidly after ﬁnding a convex bowl, as if it were an instance of the\n",
    "# AdaGrad algorithm initialized within that bowl\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#MLP\n",
    "  \n",
    "# initialisation of the weights he_normal\n",
    "def weights(noHiddenLayers,sizeOfLayers):\n",
    "\n",
    "    W=[]\n",
    "    b=[]\n",
    "\n",
    "    for i in range(0,noHiddenLayers+1):\n",
    "        \n",
    "        W.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+sizeOfLayers[i])),\n",
    "                                   (sizeOfLayers[i+1],sizeOfLayers[i])) )\n",
    "        b.append( np.random.normal(1e-4,np.sqrt(2*1.0/(sizeOfLayers[i+1]+1)),\n",
    "                                   (sizeOfLayers[i+1],1)) )\n",
    "\n",
    "    W=np.array(W)\n",
    "    b=np.array(b)\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "#mlp forward pass\n",
    "#layer\n",
    "def layer(w,x,b):\n",
    "    out = np.matmul(w,x)+b\n",
    "    return out\n",
    "\n",
    "def apply_activationMLP(Activation_function,inp):\n",
    "    \n",
    "    #activation functions\n",
    "    if Activation_function == 'relu':\n",
    "        return np.where(inp<0,0,inp)\n",
    "    elif Activation_function == \"tanh\":\n",
    "        return np.tanh(inp)\n",
    "    elif Activation_function == \"sigmoid\":\n",
    "        return 1.0/(1+np.exp(-1.0*inp))\n",
    "    elif Activation_function == \"softmax\":\n",
    "        return (1.0/(np.sum(np.exp(inp),axis=0)))*(np.exp(inp))\n",
    "\n",
    "#forward path\n",
    "def forward_path(noHiddenLayers,X,W,b,Actfnvect):\n",
    "\n",
    "    out=[]\n",
    "    \n",
    "    z=apply_activationMLP(Actfnvect[0],np.array(layer(W[0],X,b[0])))\n",
    "    out.append(np.array(z))\n",
    "\n",
    "    for i in range(1,noHiddenLayers):\n",
    "        z=apply_activationMLP(Actfnvect[i],np.array(layer(W[i],out[i-1],b[i])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    if noHiddenLayers > 0:\n",
    "        z=apply_activationMLP(Actfnvect[-1],np.array(layer(W[-1],out[-1],b[-1])))\n",
    "        out.append(np.array(z))\n",
    "\n",
    "    y_pred = out[-1]\n",
    "\n",
    "    return np.array(out),np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n"
     ]
    }
   ],
   "source": [
    "#only to import data\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# #import data\n",
    "iris_data = load_iris() # load the iris dataset\n",
    "\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "# Split the data for training and testing\n",
    "x_train , x_test, y_train , y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(3, 120)\n",
      "(4, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "# run MLP algorithm\n",
    "\n",
    "x_train = np.moveaxis(x_train,0,-1)\n",
    "y_train = np.moveaxis(y_train,0,-1)\n",
    "x_test = np.moveaxis(x_test,0,-1)\n",
    "y_test = np.moveaxis(y_test,0,-1)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of relu\n",
    "def der_relu(x):\n",
    "    return np.where(x == 0,0,1)\n",
    "\n",
    "# backpropation\n",
    "def backprop(y,y_true,z,T,u,M,x):\n",
    "    \n",
    "    dy = (y-y_true)\n",
    "    \n",
    "    # layer 3\n",
    "    dT  = np.matmul(dy,z.T)\n",
    "    db3 = np.sum(dy,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 2\n",
    "    s   = np.matmul(T.T,dy)\n",
    "    s   = s*der_relu(z)\n",
    "    dM  = np.matmul(s,u.T)\n",
    "    db2 = np.sum(s,axis=1).reshape(-1,1)\n",
    "    \n",
    "    # layer 1\n",
    "    sm  = np.matmul(M.T,s)\n",
    "    sm  = sm*der_relu(u)\n",
    "    dW  = np.matmul(sm,x.T)\n",
    "    db1 = np.sum(sm,axis=1).reshape(-1,1)\n",
    "    \n",
    "    return dW,dM,dT,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "################################\n",
    "# Training parameters\n",
    "num_classes = 3\n",
    "epochs = 160\n",
    "rho1 = 0.9\n",
    "rho2 = 0.99 \n",
    "batch_size = 10\n",
    "learning_rate = 1e-3\n",
    "delta=1e-8\n",
    "################################\n",
    "\n",
    "#MLP PARAMETERS\n",
    "noHiddenLayers=2\n",
    "\n",
    "#also includes the input vector dimension and output vector dimension\n",
    "sizeOfLayers=[x_train.shape[0],10,10,num_classes]\n",
    "\n",
    "sizeofOutput=num_classes\n",
    "\n",
    "Actfnvect = ['relu','relu','softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = weights(noHiddenLayers,sizeOfLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation\n",
    "\n",
    "#first moment\n",
    "s = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "\n",
    "#second moment\n",
    "r = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "\n",
    "#first moment Correct bias\n",
    "sHat = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "\n",
    "#second moment Correct bias\n",
    "rHat = {\"W[0]\" : np.zeros(W[0].shape) , \"W[1]\" : np.zeros(W[1].shape) ,\"W[2]\" : np.zeros(W[2].shape),\"b[0]\": np.zeros(b[0].shape)\n",
    "     ,\"b[1]\" :np.zeros(b[1].shape) , \"b[2]\" : np.zeros(b[2].shape)}\n",
    "#time\n",
    "t=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0         Loss:248.85349351697045\n",
      "Epoch:1         Loss:224.25902089445546\n",
      "Epoch:2         Loss:199.49114886564348\n",
      "Epoch:3         Loss:173.167903702095\n",
      "Epoch:4         Loss:149.84072004549512\n",
      "Epoch:5         Loss:128.92960991876973\n",
      "Epoch:6         Loss:110.58980303429249\n",
      "Epoch:7         Loss:94.0167578756742\n",
      "Epoch:8         Loss:79.14080038940143\n",
      "Epoch:9         Loss:68.36339538689423\n",
      "Epoch:10         Loss:61.38883039107031\n",
      "Epoch:11         Loss:56.66372176278357\n",
      "Epoch:12         Loss:53.25247088148282\n",
      "Epoch:13         Loss:50.69986310188742\n",
      "Epoch:14         Loss:48.55196963760715\n",
      "Epoch:15         Loss:46.728373328566576\n",
      "Epoch:16         Loss:45.11290011442958\n",
      "Epoch:17         Loss:43.70916023999247\n",
      "Epoch:18         Loss:42.412867863824616\n",
      "Epoch:19         Loss:41.222429173683174\n",
      "Epoch:20         Loss:40.099026270359296\n",
      "Epoch:21         Loss:39.04807196927103\n",
      "Epoch:22         Loss:38.01842704136115\n",
      "Epoch:23         Loss:37.01177251641953\n",
      "Epoch:24         Loss:35.935887739852184\n",
      "Epoch:25         Loss:34.64476026745518\n",
      "Epoch:26         Loss:33.26995476099263\n",
      "Epoch:27         Loss:31.937605068730456\n",
      "Epoch:28         Loss:30.683114139180624\n",
      "Epoch:29         Loss:29.489035866371285\n",
      "Epoch:30         Loss:28.295983225212765\n",
      "Epoch:31         Loss:27.09952523671827\n",
      "Epoch:32         Loss:25.996992245033464\n",
      "Epoch:33         Loss:24.988103708293387\n",
      "Epoch:34         Loss:24.05323129712766\n",
      "Epoch:35         Loss:23.186697790246523\n",
      "Epoch:36         Loss:22.36976931513414\n",
      "Epoch:37         Loss:21.596605336206053\n",
      "Epoch:38         Loss:20.870349533656317\n",
      "Epoch:39         Loss:20.18562271685735\n",
      "Epoch:40         Loss:19.53749642664738\n",
      "Epoch:41         Loss:18.930781130136214\n",
      "Epoch:42         Loss:18.359497438806095\n",
      "Epoch:43         Loss:17.81714168155807\n",
      "Epoch:44         Loss:17.310059038456554\n",
      "Epoch:45         Loss:16.828237240587562\n",
      "Epoch:46         Loss:16.37695117510939\n",
      "Epoch:47         Loss:15.950931074701119\n",
      "Epoch:48         Loss:15.548941662367222\n",
      "Epoch:49         Loss:15.169602088647936\n",
      "Epoch:50         Loss:14.813592346813904\n",
      "Epoch:51         Loss:14.477686097805785\n",
      "Epoch:52         Loss:14.161164682624385\n",
      "Epoch:53         Loss:13.862512628951466\n",
      "Epoch:54         Loss:13.581524264419173\n",
      "Epoch:55         Loss:13.316152198542966\n",
      "Epoch:56         Loss:13.065842693248799\n",
      "Epoch:57         Loss:12.82953561246542\n",
      "Epoch:58         Loss:12.605684063502508\n",
      "Epoch:59         Loss:12.394472871699177\n",
      "Epoch:60         Loss:12.194164808354333\n",
      "Epoch:61         Loss:12.004679931073964\n",
      "Epoch:62         Loss:11.82576393566357\n",
      "Epoch:63         Loss:11.65486215037283\n",
      "Epoch:64         Loss:11.494836471460644\n",
      "Epoch:65         Loss:11.340832802807418\n",
      "Epoch:66         Loss:11.195647811213954\n",
      "Epoch:67         Loss:11.05746203454242\n",
      "Epoch:68         Loss:10.925362747487272\n",
      "Epoch:69         Loss:10.800424828006582\n",
      "Epoch:70         Loss:10.680941947248515\n",
      "Epoch:71         Loss:10.566480214107075\n",
      "Epoch:72         Loss:10.457977842831427\n",
      "Epoch:73         Loss:10.354515401810314\n",
      "Epoch:74         Loss:10.255246734285333\n",
      "Epoch:75         Loss:10.160324476288041\n",
      "Epoch:76         Loss:10.070235629590583\n",
      "Epoch:77         Loss:9.983391287365464\n",
      "Epoch:78         Loss:9.899711951177228\n",
      "Epoch:79         Loss:9.820768801256744\n",
      "Epoch:80         Loss:9.74382836321654\n",
      "Epoch:81         Loss:9.670772324028583\n",
      "Epoch:82         Loss:9.600247968534873\n",
      "Epoch:83         Loss:9.532158277815643\n",
      "Epoch:84         Loss:9.467310666237735\n",
      "Epoch:85         Loss:9.40481851510725\n",
      "Epoch:86         Loss:9.344458388513925\n",
      "Epoch:87         Loss:9.286634993143306\n",
      "Epoch:88         Loss:9.231143172640706\n",
      "Epoch:89         Loss:9.176722254972095\n",
      "Epoch:90         Loss:9.125088172561755\n",
      "Epoch:91         Loss:9.074849076721396\n",
      "Epoch:92         Loss:9.026323961208922\n",
      "Epoch:93         Loss:8.979559906153195\n",
      "Epoch:94         Loss:8.93428601795263\n",
      "Epoch:95         Loss:8.89050846384045\n",
      "Epoch:96         Loss:8.849355190072782\n",
      "Epoch:97         Loss:8.806498332894279\n",
      "Epoch:98         Loss:8.768101509387803\n",
      "Epoch:99         Loss:8.72894451503317\n",
      "Epoch:100         Loss:8.691738587166368\n",
      "Epoch:101         Loss:8.656884562174342\n",
      "Epoch:102         Loss:8.619977853368711\n",
      "Epoch:103         Loss:8.587199626363656\n",
      "Epoch:104         Loss:8.554567455493416\n",
      "Epoch:105         Loss:8.520884325737496\n",
      "Epoch:106         Loss:8.491094964974923\n",
      "Epoch:107         Loss:8.46007259828407\n",
      "Epoch:108         Loss:8.431693482439695\n",
      "Epoch:109         Loss:8.401704841914372\n",
      "Epoch:110         Loss:8.374867844127207\n",
      "Epoch:111         Loss:8.348034626015467\n",
      "Epoch:112         Loss:8.320220063109492\n",
      "Epoch:113         Loss:8.295886472540948\n",
      "Epoch:114         Loss:8.270055388878646\n",
      "Epoch:115         Loss:8.246578242141709\n",
      "Epoch:116         Loss:8.221516805400125\n",
      "Epoch:117         Loss:8.199200478965011\n",
      "Epoch:118         Loss:8.176143556477378\n",
      "Epoch:119         Loss:8.154055218591663\n",
      "Epoch:120         Loss:8.13277002680557\n",
      "Epoch:121         Loss:8.11166088517251\n",
      "Epoch:122         Loss:8.091961005834362\n",
      "Epoch:123         Loss:8.07063991524276\n",
      "Epoch:124         Loss:8.052009276997765\n",
      "Epoch:125         Loss:8.032350294925704\n",
      "Epoch:126         Loss:8.013702987754316\n",
      "Epoch:127         Loss:7.995597365497318\n",
      "Epoch:128         Loss:7.977671141888744\n",
      "Epoch:129         Loss:7.960189475123722\n",
      "Epoch:130         Loss:7.943113464536543\n",
      "Epoch:131         Loss:7.926374586939931\n",
      "Epoch:132         Loss:7.909999250607508\n",
      "Epoch:133         Loss:7.894698371643808\n",
      "Epoch:134         Loss:7.877817958963143\n",
      "Epoch:135         Loss:7.863292071119307\n",
      "Epoch:136         Loss:7.847739173939873\n",
      "Epoch:137         Loss:7.832999746781211\n",
      "Epoch:138         Loss:7.818699373062\n",
      "Epoch:139         Loss:7.804472946660805\n",
      "Epoch:140         Loss:7.790582451601784\n",
      "Epoch:141         Loss:7.777665674348445\n",
      "Epoch:142         Loss:7.763216756114964\n",
      "Epoch:143         Loss:7.750912717097268\n",
      "Epoch:144         Loss:7.737633691574618\n",
      "Epoch:145         Loss:7.724936849686194\n",
      "Epoch:146         Loss:7.712769577751706\n",
      "Epoch:147         Loss:7.700574218860951\n",
      "Epoch:148         Loss:7.688597358203168\n",
      "Epoch:149         Loss:7.677051731431355\n",
      "Epoch:150         Loss:7.665527237876457\n",
      "Epoch:151         Loss:7.654226222462556\n",
      "Epoch:152         Loss:7.643165609258889\n",
      "Epoch:153         Loss:7.6322712249762885\n",
      "Epoch:154         Loss:7.621562244716964\n",
      "Epoch:155         Loss:7.611043566702513\n",
      "Epoch:156         Loss:7.600699433567571\n",
      "Epoch:157         Loss:7.590527614490701\n",
      "Epoch:158         Loss:7.5806058238030225\n",
      "Epoch:159         Loss:7.570637418626937\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# ADAM descent\n",
    "\n",
    "# updating the weights\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss=0\n",
    "    for j in np.arange(0,x_train.shape[1],batch_size):       \n",
    "        dW = np.zeros(W[0].shape)\n",
    "        dM = np.zeros(W[1].shape)\n",
    "        dT = np.zeros(W[2].shape)\n",
    "        db1 = np.zeros(b[0].shape)\n",
    "        db2 = np.zeros(b[1].shape)\n",
    "        db3 = np.zeros(b[2].shape)\n",
    "        \n",
    "        for k in range(0,batch_size):\n",
    "            # forward pass\n",
    "            x = x_train[:,j+k].reshape(-1,1)\n",
    "            y = y_train[:,j+k].reshape(-1,1)\n",
    "            out,y_pred=forward_path(noHiddenLayers,x,W,b,Actfnvect)\n",
    "            # backpropagation\n",
    "            dWtemp,dMtemp,dTtemp,db1temp,db2temp,db3temp=backprop(y_pred,y,out[1],W[-1],out[0],W[1],x)\n",
    "            \n",
    "            dW=dW+dWtemp\n",
    "            db1=db1+db1temp\n",
    "            \n",
    "            dM=dM+dMtemp\n",
    "            db2=db2+db2temp\n",
    "            \n",
    "            dT=dT+dTtemp\n",
    "            db3=db3+db3temp\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = loss + (-1.0*np.sum(y*np.log(y_pred)))\n",
    "            \n",
    "        # Updating the weights using ADAM Approach\n",
    "        \n",
    "        #Normalising the weights as in deep learning text book\n",
    "        dW=dW*(1.0/batch_size)\n",
    "        dM=dM*(1.0/batch_size)\n",
    "        dT=dT*(1.0/batch_size)\n",
    "        db1=db1*(1.0/batch_size)\n",
    "        db2=db2*(1.0/batch_size)\n",
    "        db3=db3*(1.0/batch_size)\n",
    "        \n",
    "        #time update\n",
    "        t=t+1\n",
    "        \n",
    "        #Update biased first moment estimate\n",
    "        s[\"W[0]\"] = rho1*s[\"W[0]\"] + ((1.0-rho1)*(dW))\n",
    "        s[\"W[1]\"] = rho1*s[\"W[1]\"] + ((1.0-rho1)*(dM))\n",
    "        s[\"W[2]\"] = rho1*s[\"W[2]\"] + ((1.0-rho1)*(dT))\n",
    "        s[\"b[0]\"] = rho1*s[\"b[0]\"] + ((1.0-rho1)*(db1))\n",
    "        s[\"b[1]\"] = rho1*s[\"b[1]\"] + ((1.0-rho1)*(db2))\n",
    "        s[\"b[2]\"] = rho1*s[\"b[2]\"] + ((1.0-rho1)*(db3))\n",
    "        \n",
    "        #Update biased second moment estimate\n",
    "        r[\"W[0]\"] = rho2*r[\"W[0]\"] + ((1.0-rho2)*(dW*dW))\n",
    "        r[\"W[1]\"] = rho2*r[\"W[1]\"] + ((1.0-rho2)*(dM*dM))\n",
    "        r[\"W[2]\"] = rho2*r[\"W[2]\"] + ((1.0-rho2)*(dT*dT))\n",
    "        r[\"b[0]\"] = rho2*r[\"b[0]\"] + ((1.0-rho2)*(db1*db1))\n",
    "        r[\"b[1]\"] = rho2*r[\"b[1]\"] + ((1.0-rho2)*(db2*db2))\n",
    "        r[\"b[2]\"] = rho2*r[\"b[2]\"] + ((1.0-rho2)*(db3*db3))\n",
    "        \n",
    "        #Correct bias in ﬁrst moment\n",
    "        sHat[\"W[0]\"] = (1.0/(1-(rho1**t)))*s[\"W[0]\"] \n",
    "        sHat[\"W[1]\"] = (1.0/(1-(rho1**t)))*s[\"W[1]\"] \n",
    "        sHat[\"W[2]\"] = (1.0/(1-(rho1**t)))*s[\"W[2]\"] \n",
    "        sHat[\"b[0]\"] = (1.0/(1-(rho1**t)))*s[\"b[0]\"]\n",
    "        sHat[\"b[1]\"] = (1.0/(1-(rho1**t)))*s[\"b[1]\"]\n",
    "        sHat[\"b[2]\"] = (1.0/(1-(rho1**t)))*s[\"b[2]\"]\n",
    "        \n",
    "        #Correct bias in second moment\n",
    "        rHat[\"W[0]\"] = (1.0/(1-(rho2**t)))*r[\"W[0]\"] \n",
    "        rHat[\"W[1]\"] = (1.0/(1-(rho2**t)))*r[\"W[1]\"] \n",
    "        rHat[\"W[2]\"] = (1.0/(1-(rho2**t)))*r[\"W[2]\"] \n",
    "        rHat[\"b[0]\"] = (1.0/(1-(rho2**t)))*r[\"b[0]\"]\n",
    "        rHat[\"b[1]\"] = (1.0/(1-(rho2**t)))*r[\"b[1]\"]\n",
    "        rHat[\"b[2]\"] = (1.0/(1-(rho2**t)))*r[\"b[2]\"]\n",
    "        \n",
    "        #Apply update\n",
    "        W[0] = W[0] + (((-1.0*learning_rate)*sHat[\"W[0]\"])/(np.sqrt(rHat[\"W[0]\"])+delta))\n",
    "        W[1] = W[1] + (((-1.0*learning_rate)*sHat[\"W[1]\"])/(np.sqrt(rHat[\"W[1]\"])+delta))\n",
    "        W[2] = W[2] + (((-1.0*learning_rate)*sHat[\"W[2]\"])/(np.sqrt(rHat[\"W[2]\"])+delta))\n",
    "        b[0] = b[0] + (((-1.0*learning_rate)*sHat[\"b[0]\"])/(np.sqrt(rHat[\"b[0]\"])+delta))\n",
    "        b[1] = b[1] + (((-1.0*learning_rate)*sHat[\"b[1]\"])/(np.sqrt(rHat[\"b[1]\"])+delta))\n",
    "        b[2] = b[2] + (((-1.0*learning_rate)*sHat[\"b[2]\"])/(np.sqrt(rHat[\"b[2]\"])+delta))\n",
    "    \n",
    "    #print the loss in each epoch\n",
    "    print('Epoch:'+str(i)+'         Loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,y_pred=forward_path(noHiddenLayers,x_test,W,b,Actfnvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(y):\n",
    "    return np.argmax(y)\n",
    "\n",
    "yvect=[]\n",
    "y_trurevect=[]\n",
    "\n",
    "for i in range(0,x_test.shape[1]):\n",
    "    yvect.append(predict(y_pred[:,i]))\n",
    "    y_trurevect.append(predict(y_test[:,i]))\n",
    "\n",
    "# find accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "#predicting test accuracy\n",
    "print(accuracy_score(y_trurevect, yvect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# to see the output vs true values\n",
    "\n",
    "print y_trurevect\n",
    "print yvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
